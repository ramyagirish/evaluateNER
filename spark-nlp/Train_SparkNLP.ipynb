{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import sparknlp\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from sparknlp.training import CoNLL\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# to use GPU \n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts our regular CONLL files to SparkNLP's CONLL format\n",
    "def convert_format(inputpath, outputpath):\n",
    "    # create the training file\n",
    "    with open(inputpath) as fp:\n",
    "        text = fp.readlines()\n",
    "    text = \"\".join(text[1:]).split(\"\\n\\n\") \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], \n",
    "                      columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    \n",
    "    # creating the training data\n",
    "    conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "    for t in range(len(text)):    \n",
    "        df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "        tokens = df.Token.tolist()\n",
    "        pos_labels = df.Pos.tolist()\n",
    "        entity_labels = df.Entity_label.tolist()\n",
    "        for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "            conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "        conll_lines += \"\\n\"\n",
    "        \n",
    "    with open(outputpath,\"w\") as fp:\n",
    "        for line in conll_lines:\n",
    "            fp.write(line)\n",
    "    \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio-splits/train/sample.train')\n",
    "dev_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio-splits/dev/sample.train')\n",
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio-splits/test/sample.train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertEmbeddings.pretrained('bert_base_cased', 'en').setInputCols([\"sentence\",'token']).setOutputCol(\"bert\").setCaseSensitive(True)#.setMaxSentenceLength(512)\n",
    "# transforming the training data into embeddings and saving it as parquet files\n",
    "readyTrainingData = bert.transform(training_data)\n",
    "readyTrainingData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_train\")\n",
    "# transforming the development data into embeddings and saving it as parquet files\n",
    "readyDevData = bert.transform(dev_data)\n",
    "\n",
    "readyDevData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_dev\")\n",
    "\n",
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")\n",
    "\n",
    "readyTrainingData = spark.read.parquet(\"/tmp/conll2003/bert_train\")\n",
    "readyDevData = spark.read.parquet(\"/tmp/conll2003/bert_dev\")\n",
    "readyTestData = spark.read.parquet(\"/tmp/conll2003/bert_test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize NER tagger\n",
    "nerTagger = NerDLApproach()\\\n",
    ".setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
    ".setLabelColumn(\"label\")\\\n",
    ".setOutputCol(\"ner\")\\\n",
    ".setMaxEpochs(10)\\\n",
    ".setBatchSize(4)\\\n",
    ".setEnableMemoryOptimizer(True)\\\n",
    ".setRandomSeed(0)\\\n",
    ".setVerbose(1)\\\n",
    ".setValidationSplit(0.2)\\\n",
    ".setEvaluationLogExtended(True)\\\n",
    ".setEnableOutputLogs(True)\\\n",
    ".setIncludeConfidence(True)\\\n",
    ".setTestDataset(\"/tmp/conll2003/bert_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "%time myNerModel = nerTagger.fit(readyTrainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(readyTestData):\n",
    "    results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()    \n",
    "    #test_data.show()\n",
    "    \n",
    "    # to find exceptions where no. of labels does not match no. of ners detected\n",
    "    count = 0\n",
    "    indices = []\n",
    "    for i,row in enumerate(results):\n",
    "        if len(row['label']) != len(row['ner']):\n",
    "            count += 1\n",
    "            indices.append(i)\n",
    "\n",
    "    print(count)\n",
    "    print(indices)\n",
    "\n",
    "    exclusion_list = [results[t] for t in indices]\n",
    "    results = [results[i] for i in range(len(results)) if i not in indices]\n",
    "    \n",
    "    tokens = []\n",
    "    labels = []\n",
    "    ners = []\n",
    "\n",
    "    for row in results:\n",
    "        tokens.append([t['result'] for t in row['token']])\n",
    "        labels.append([t['result'] for t in row['label']])\n",
    "        ners.append([t['result'] for t in row['ner']])\n",
    "\n",
    "    from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "    #print(accuracy_score(labels,ners))\n",
    "    #print(f1_score(labels,ners))\n",
    "\n",
    "    print(classification_report(labels,ners, zero_division=1,digits=6))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
