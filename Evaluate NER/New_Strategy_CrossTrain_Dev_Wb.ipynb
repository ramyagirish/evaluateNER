{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1edef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  3.1.0\n",
      "Apache Spark version:  3.1.2\n"
     ]
    }
   ],
   "source": [
    "# call relevant packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "import sparknlp\n",
    "\n",
    "# to use GPU \n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb98346",
   "metadata": {},
   "source": [
    "## Creating a composite CoNLL file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4910c848",
   "metadata": {},
   "source": [
    "### Create training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "024205af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training filJe with all the news corpus\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/onto.bn.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85935111",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769deee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This\\tDT\\t(TOP(S(NP*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nThe\\tDT\\t(NP(NP*\\tB-ORG\\nWorld\\tNNP\\t*)\\tI-ORG\\n,\\t,\\t*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nco-production\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tB-ORG\\nBBC\\tNNP\\t*\\tI-ORG\\nWorld\\tNNP\\t*\\tI-ORG\\nService\\tNNP\\t*)\\tI-ORG\\n,\\t,\\t*\\tO\\nPRI\\tNNP\\t(NP*)\\tB-ORG\\n,\\t,\\t*\\tO\\nand\\tCC\\t*\\tO\\nWGBH\\tNNP\\t(NP(NP*)\\tB-ORG\\nin\\tIN\\t(PP*\\tO\\nBoston\\tNNP\\t(NP*))))))))\\tB-GPE\\n.\\t.\\t*))\\tO',\n",
       " 'I\\tPRP\\t(TOP(S(NP*)\\tO\\nam\\tVBP\\t(VP*\\tO\\nLisa\\tNNP\\t(NP*\\tB-PERSON\\nMullins\\tNNP\\t*))\\tI-PERSON\\n.\\t.\\t*))\\tO',\n",
       " \"A\\tDT\\t(TOP(S(NP*\\tO\\nplan\\tNN\\t*)\\tO\\nwas\\tVBD\\t(VP*\\tO\\nannounced\\tVBN\\t(VP*\\tO\\ntoday\\tNN\\t(NP*)\\tB-DATE\\nto\\tTO\\t(S(VP*\\tO\\nraise\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nRussian\\tJJ\\t*\\tB-NORP\\nnuclear\\tJJ\\t*\\tO\\nsubmarine\\tNN\\t*)\\tO\\n`\\t''\\t(NP*\\tO\\nKursk\\tNNP\\t*\\tB-PRODUCT\\n'\\t''\\t*))\\tO\\n,\\t,\\t*\\tO\\nnext\\tJJ\\t(NP(NP*\\tB-DATE\\nsummer\\tNN\\t*)\\tI-DATE\\n,\\t,\\t*\\tO\\nalmost\\tRB\\t(SBAR(NP(QP*\\tB-DATE\\na\\tDT\\t*)\\tI-DATE\\nyear\\tNN\\t*)\\tI-DATE\\nafter\\tIN\\t*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\nhit\\tVBD\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nbottom\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tB-LOC\\nBarents\\tNNP\\t*\\tI-LOC\\nSea\\tNNP\\t*)))\\tI-LOC\\nfollowing\\tVBG\\t(PP*\\tO\\na\\tDT\\t(NP*\\tO\\nmysterious\\tJJ\\t*\\tO\\naccident\\tNN\\t*)))))))))))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'The\\tDT\\t(TOP(S(NP(NP*\\tB-ORG\\nKursk\\tNNP\\t*\\tI-ORG\\nFoundation\\tNNP\\t*)\\tI-ORG\\n,\\t,\\t*\\tO\\nan\\tDT\\t(NP(NP*\\tO\\ninternational\\tJJ\\t*\\tO\\nconsortium\\tNN\\t*)\\tO\\nled\\tVBN\\t(VP*\\tO\\nby\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\ngovernments\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nRussia\\tNNP\\t(NP(NP*)\\tB-GPE\\nand\\tCC\\t*\\tO\\nThe\\tDT\\t(NP*\\tB-GPE\\nNetherlands\\tNNP\\t*))))))))\\tI-GPE\\nsays\\tVBZ\\t(VP*\\tO\\nit\\tPRP\\t(SBAR(S(NP*)\\tO\\ncan\\tMD\\t(VP*\\tO\\ndo\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\njob\\tNN\\t*)\\tO\\nsafely\\tRB\\t(ADVP*)\\tO\\nif\\tIN\\t(SBAR*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\ncan\\tMD\\t(VP*\\tO\\nsecure\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nfunding\\tNN\\t*))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " \"The\\tDT\\t(TOP(S(NP(NP*\\tO\\nBBC\\tNNP\\t*\\tB-ORG\\n's\\tPOS\\t*)\\tO\\nJames\\tNNP\\t*\\tB-PERSON\\nRogers\\tNNP\\t*)\\tI-PERSON\\nwas\\tVBD\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nBrussels\\tNNP\\t(NP*))\\tB-GPE\\nfor\\tIN\\t(PP*\\tO\\ntoday\\tNN\\t(NP(NP*\\tB-DATE\\n's\\tPOS\\t*)\\tO\\nannouncement\\tNN\\t*)))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'He\\tPRP\\t(TOP(S(NP*)\\tO\\nsays\\tVBZ\\t(VP*\\tO\\nthere\\tEX\\t(SBAR(S(NP*)\\tO\\nare\\tVBP\\t(VP*\\tO\\ntwo\\tCD\\t(NP(NP*\\tB-CARDINAL\\nmain\\tJJ\\t*\\tO\\nreasons\\tNNS\\t*)\\tO\\nfor\\tIN\\t(PP*\\tO\\nraising\\tVBG\\t(S(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nKursk\\tNNP\\t*)))))))))\\tB-PRODUCT\\n.\\t.\\t*))\\tO',\n",
       " 'Obviously\\tRB\\t(TOP(S(ADVP*)\\tO\\nthe\\tDT\\t(NP*\\tO\\nfirst\\tJJ\\t*\\tB-ORDINAL\\none\\tNN\\t*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nto\\tTO\\t(S(VP*\\tO\\nrecover\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nbodies\\tNNS\\t*)\\tO\\nthat\\tWDT\\t(SBAR(WHNP*)\\tO\\nremain\\tVBP\\t(S(VP*))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'There\\tEX\\t(TOP(S(NP*)\\tO\\nare\\tVBP\\t(VP*\\tO\\nstill\\tRB\\t(ADVP*)\\tO\\nthe\\tDT\\t(NP(NP(NP*\\tO\\nbodies\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nmore\\tJJR\\t(NP(QP*\\tB-CARDINAL\\nthan\\tIN\\t*\\tI-CARDINAL\\n100\\tCD\\t*)\\tI-CARDINAL\\nsailors\\tNNS\\t*)))\\tO\\ntrapped\\tVBN\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nvessel\\tNN\\t*))\\tO\\nsince\\tIN\\t(SBAR*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\nsank\\tVBD\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tB-LOC\\nBarents\\tNNP\\t*\\tI-LOC\\nSea\\tNNP\\t*))\\tI-LOC\\nin\\tIN\\t(PP*\\tO\\nAugust\\tNNP\\t(NP(NP*)\\tB-DATE\\nof\\tIN\\t(PP*\\tI-DATE\\nlast\\tJJ\\t(NP*\\tI-DATE\\nyear\\tNN\\t*))))))))))\\tI-DATE\\n.\\t.\\t*))\\tO',\n",
       " \"Secondly\\tRB\\t(TOP(S(ADVP*)\\tB-ORDINAL\\n,\\t,\\t*\\tO\\nthere\\tEX\\t(S(NP*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\na\\tDT\\t(NP*\\tO\\nlonger\\tJJR\\t(NML*\\tO\\n-\\tHYPH\\t*\\tO\\nterm\\tNN\\t*)\\tO\\nthreat\\tNN\\t*)))\\tO\\nand\\tCC\\t*\\tO\\nthat\\tDT\\t(S(NP*)\\tO\\n's\\tVBZ\\t(VP*\\tO\\nan\\tDT\\t(NP*\\tO\\nenvironmental\\tJJ\\t*\\tO\\nthreat\\tNN\\t*)))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'The\\tDT\\t(TOP(S(NP*\\tB-ORG\\nKursk\\tNNP\\t*\\tI-ORG\\nFoundation\\tNNP\\t*)\\tI-ORG\\nsays\\tVBZ\\t(VP*\\tO\\nthat\\tIN\\t(SBAR(SBAR*\\tO\\nthe\\tDT\\t(S(NP(NP*\\tO\\nnuclear\\tJJ\\t*\\tO\\nreactors\\tNNS\\t*)\\tO\\n,\\t,\\t*\\tO\\nthere\\tEX\\t(PRN(S(NP*)\\tO\\nare\\tVBP\\t(VP*\\tO\\ntwo\\tCD\\t(NP(NP*)\\tB-CARDINAL\\non\\tIN\\t(PP*\\tO\\nboard\\tNN\\t(NP*)))))))\\tO\\n,\\t,\\t*\\tO\\nare\\tVBP\\t(VP*\\tO\\nsafe\\tJJ\\t(ADJP*\\tO\\nfor\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\ntime\\tNN\\t*\\tO\\nbeing\\tNN\\t*))))))\\tO\\nbut\\tCC\\t*\\tO\\nthat\\tIN\\t(SBAR*\\tO\\nthe\\tDT\\t(S(NP(NP*\\tO\\ncorrosive\\tJJ\\t*\\tO\\neffects\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nseawater\\tNN\\t(NP*)))\\tO\\ncould\\tMD\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\ntime\\tNN\\t(NP*))\\tO\\nlead\\tVB\\t(VP*\\tO\\nto\\tIN\\t(PP*\\tO\\nradioactive\\tJJ\\t(NP(NP(NP*\\tO\\nleaks\\tNNS\\t*)\\tO\\ninto\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nocean\\tNN\\t*)))\\tO\\n,\\t,\\t*\\tO\\nwhich\\tWDT\\t(SBAR(WHNP*)\\tO\\nobviously\\tRB\\t(S(ADVP*)\\tO\\ncould\\tMD\\t(VP*\\tO\\nhave\\tVB\\t(VP*\\tO\\ndevastating\\tJJ\\t(NP(NP*\\tO\\nconsequences\\tNNS\\t*)\\tO\\non\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nfishery\\tNN\\t*)\\tO\\nand\\tCC\\t*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nenvironment\\tNN\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\ngeneral\\tJJ\\t(ADJP*))))))))))))))))))\\tO\\n.\\t.\\t*))\\tO']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f341e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a5589da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41459c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/onto.mz.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84dedece",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f31fbf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/onto.nw.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5134538",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41e0b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa11d3b4",
   "metadata": {},
   "source": [
    "### Create dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "488f800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training filJe with all the news corpus\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/development/onto.wb.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc5fc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3f11a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abd\\tNNP\\t(TOP(NP(NP*\\tB-PERSON\\nal\\tNNP\\t*\\tI-PERSON\\n-\\tHYPH\\t*\\tI-PERSON\\nBari\\tNNP\\t*\\tI-PERSON\\nAtwan\\tNNP\\t*)\\tI-PERSON\\n:\\t:\\t*\\tO\\n-LRB-\\t-LRB-\\t(NP*\\tO\\nPresident\\tNNP\\t*\\tO\\nSaddam\\tNNP\\t*\\tB-PERSON\\n-RRB-\\t-RRB-\\t*)\\tO\\nGreat\\tJJ\\t(ADJP*\\tO\\nin\\tIN\\t(PP*\\tO\\nHis\\tPRP$\\t(NP*\\tO\\nLife\\tNN\\t*)))\\tO\\n..\\tNFP\\t*\\tO\\nGreat\\tJJ\\t(ADJP*\\tO\\nin\\tIN\\t(PP*\\tO\\nHis\\tPRP$\\t(NP*\\tO\\nMartyrdom\\tNN\\t*)))))\\tO',\n",
       " 'Asad\\tNNP\\t(TOP(FRAG(NP*)))\\tB-PERSON',\n",
       " '1/02/2007\\tNN\\t(TOP(NP*))\\tO',\n",
       " 'This\\tDT\\t(TOP(S(NP(NP*\\tO\\nunprecedented\\tJJ\\t*\\tO\\nArab\\tJJ\\t(ADJP*\\tB-NORP\\nand\\tCC\\t*\\tO\\ninternational\\tJJ\\t*)\\tO\\ninterest\\tNN\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP(NP*\\tO\\nexecution\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nIraqi\\tJJ\\t(NP(NML*\\tB-NORP\\nPresident\\tNNP\\t*)\\tO\\nSaddam\\tNNP\\t*\\tB-PERSON\\nHussein\\tNNP\\t*)))\\tI-PERSON\\nand\\tCC\\t*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nbarbaric\\tJJ\\t*\\tO\\nway\\tNN\\t*)\\tO\\nin\\tIN\\t(SBAR(WHPP*\\tO\\nwhich\\tWDT\\t(WHNP*))\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\nwas\\tVBD\\t(VP*\\tO\\ncarried\\tVBN\\t(VP*\\tO\\nout\\tRP\\t(PRT*)))))))))\\tO\\n,\\t,\\t*\\tO\\nconfirms\\tVBZ\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nimportance\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nman\\tNN\\t*)\\tO\\n,\\t,\\t*\\tO\\nhis\\tPRP$\\t(NP*\\tO\\nprominent\\tJJ\\t*\\tO\\nstatus\\tNN\\t*)\\tO\\n,\\t,\\t*\\tO\\nand\\tCC\\t*\\tO\\nhis\\tPRP$\\t(NP(NP*\\tO\\ndistinguished\\tJJ\\t*\\tO\\nrole\\tNN\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nhistory\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nArab\\tJJ\\t*\\tB-NORP\\nregion\\tNN\\t*)))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'The\\tDT\\t(TOP(S(NP(NP*\\tO\\nAmerican\\tJJ\\t*\\tB-NORP\\nadministration\\tNN\\t*)\\tO\\n,\\t,\\t*\\tO\\nwho\\tWP\\t(SBAR(WHNP*)\\tO\\nplanned\\tVBD\\t(S(VP*\\tO\\ncarefully\\tRB\\t(ADVP*)\\tO\\nfor\\tIN\\t(PP*\\tO\\nthis\\tDT\\t(NP*\\tO\\nevent\\tNN\\t*))\\tO\\nthrough\\tIN\\t(PP*\\tO\\nexperts\\tNNS\\t(NP(NP(NP*)\\tO\\nin\\tIN\\t(PP*\\tO\\nmedia\\tNNS\\t(NP(NP*)\\tO\\nand\\tCC\\t*\\tO\\npublic\\tJJ\\t(NP*\\tO\\nrelations\\tNNS\\t*))))\\tO\\n,\\t,\\t*\\tO\\nand\\tCC\\t*\\tO\\nits\\tPRP$\\t(NP(NP*\\tO\\ntools\\tNNS\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\nIraq\\tNNP\\t(NP*)))\\tB-GPE\\nand\\tCC\\t*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\ninclinations\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\npublic\\tJJ\\t(NP*\\tO\\nopinion\\tNN\\t*)))))))))\\tO\\ndid\\tVBD\\t(VP*\\tO\\nnot\\tRB\\t*\\tO\\nat\\tRB\\t(ADVP*\\tO\\nall\\tRB\\t*)\\tO\\nexpect\\tVB\\t(VP*\\tO\\nsuch\\tPDT\\t(NP(NP*\\tO\\nan\\tDT\\t*\\tO\\nopposite\\tJJ\\t*\\tO\\nresult\\tNN\\t*)\\tO\\n,\\t,\\t*\\tO\\nwhich\\tWDT\\t(SBAR(WHNP*)\\tO\\nalong\\tIN\\t(S(PP*\\tO\\nwith\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\ndestructive\\tJJ\\t*\\tO\\nconsequences\\tNNS\\t*)\\tO\\nthat\\tWDT\\t(SBAR(WHNP*)\\tO\\nmay\\tMD\\t(S(VP*\\tO\\nresult\\tVB\\t(VP*\\tO\\nfrom\\tIN\\t(PP*\\tO\\nit\\tPRP\\t(NP*))\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tB-DATE\\ncoming\\tVBG\\t*\\tI-DATE\\ndays\\tNNS\\t*)))))))))\\tI-DATE\\nhave\\tVBP\\t(VP*\\tO\\ncaught\\tVBN\\t(VP*\\tO\\nthem\\tPRP\\t(NP*)\\tO\\nby\\tIN\\t(PP*\\tO\\nsurprise\\tNN\\t(NP*)))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'Saddam\\tNNP\\t(TOP(S(NP*\\tB-PERSON\\nHussein\\tNNP\\t*)\\tI-PERSON\\nproceeded\\tVBD\\t(VP*\\tO\\nto\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\ngallows\\tNNS\\t*))\\tO\\nlike\\tIN\\t(PP*\\tO\\na\\tDT\\t(NP*\\tO\\nproud\\tJJ\\t*\\tO\\nmountain\\tNN\\t*))\\tO\\n,\\t,\\t*\\tO\\nwith\\tIN\\t(SBAR*\\tO\\nhead\\tNN\\t(S(NP*)\\tO\\nraised\\tVBN\\t(VP*)))\\tO\\n,\\t,\\t*\\tO\\nstanding\\tVBG\\t(S(VP*\\tO\\nupright\\tRB\\t(ADVP*)))\\tO\\n,\\t,\\t*\\tO\\nlooking\\tVBG\\t(S(VP*\\tO\\ndown\\tRB\\t(ADVP*\\tO\\nat\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\ninsignificant\\tJJ\\t(NML(NML*\\tO\\nthings\\tNNS\\t*)\\tO\\nand\\tCC\\t*\\tO\\ninsignificant\\tJJ\\t(NML*\\tO\\npeople\\tNNS\\t*)))))))\\tO\\n,\\t,\\t*\\tO\\nbelieving\\tVBG\\t(S(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nhis\\tPRP$\\t(NP(NP*\\tO\\ncreed\\tNN\\t*)\\tO\\nand\\tCC\\t*\\tO\\nhis\\tPRP$\\t(NP*\\tO\\nArabism\\tNNP\\t*)))))\\tB-NORP\\n,\\t,\\t*\\tO\\nrepeating\\tVBG\\t(S(VP*\\tO\\nslogans\\tNNS\\t(NP(NP*)\\tO\\nof\\tIN\\t(PP*\\tO\\npride\\tNN\\t(NP*\\tO\\nand\\tCC\\t*\\tO\\ndignity\\tNN\\t*)))\\tO\\n,\\t,\\t*\\tO\\nafter\\tIN\\t(SBAR*\\tO\\nhe\\tPRP\\t(S(NP*)\\tO\\nconfirmed\\tVBD\\t(VP*\\tO\\nhis\\tPRP$\\t(NP*\\tO\\nfaith\\tNN\\t*)))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'He\\tPRP\\t(TOP(S(NP*)\\tO\\nconfirmed\\tVBD\\t(VP*\\tO\\nit\\tPRP\\t(NP*)\\tO\\nby\\tIN\\t(PP*\\tO\\nrepeating\\tVBG\\t(S(VP*\\tO\\nverses\\tNNS\\t(NP(NP(NP*)\\tO\\nfrom\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nnoble\\tJJ\\t*\\tO\\nKoran\\tNNP\\t*)))\\tB-WORK_OF_ART\\nand\\tCC\\t*\\tO\\nthe\\tDT\\t(NP*\\tO\\ntwo\\tCD\\t*\\tB-CARDINAL\\ntestimonies\\tNNS\\t*))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'He\\tPRP\\t(TOP(S(NP*)\\tO\\nwill\\tMD\\t(VP*\\tO\\nbe\\tVB\\t(VP*\\tO\\nenvied\\tVBN\\t(VP*\\tO\\nby\\tIN\\t(PP*\\tO\\nall\\tDT\\t(NP(NP*\\tO\\nArab\\tJJ\\t*\\tB-PERSON\\nleaders\\tNNS\\t*)\\tO\\n,\\t,\\t*\\tO\\nboth\\tCC\\t(ADJP*\\tO\\nliving\\tJJ\\t*\\tO\\nand\\tCC\\t*\\tO\\ndead\\tJJ\\t*)))\\tO\\n,\\t,\\t*\\tO\\nfor\\tIN\\t(PP*\\tO\\nthis\\tDT\\t(NP(NP*\\tO\\nhonorable\\tJJ\\t*\\tO\\nmartyrdom\\tNN\\t*)\\tO\\n,\\t,\\t*\\tO\\nand\\tCC\\t*\\tO\\nthis\\tDT\\t(NP(NP*\\tO\\nenormous\\tJJ\\t*\\tO\\nlove\\tNN\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nmidst\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nhundreds\\tNNS\\t(NP(NP*)\\tB-CARDINAL\\nof\\tIN\\t(PP*\\tI-CARDINAL\\nmillions\\tNNS\\t(NP*))\\tI-CARDINAL\\nof\\tIN\\t(PP*\\tO\\nArabs\\tNNPS\\t(NP*\\tB-NORP\\nand\\tCC\\t*\\tO\\nMuslims\\tNNPS\\t*))\\tB-NORP\\nall\\tRB\\t(PP(ADVP*)\\tO\\nover\\tIN\\t*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\ndifferent\\tJJ\\t*\\tO\\nparts\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nworld\\tNN\\t*))))))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'Saddam\\tNNP\\t(TOP(S(NP*\\tB-PERSON\\nHussein\\tNNP\\t*)\\tI-PERSON\\nwas\\tVBD\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP(NP*\\tO\\nonly\\tJJ\\t*\\tO\\nleader\\tNN\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nmodern\\tJJ\\t*\\tO\\nhistory\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthis\\tDT\\t(NP*\\tO\\nnation\\tNN\\t*)))))\\tO\\nwho\\tWP\\t(SBAR(WHNP*)\\tO\\nwent\\tVBD\\t(S(VP*\\tO\\nto\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nguillotine\\tNN\\t*))\\tO\\nbecause\\tIN\\t(SBAR*\\tO\\nhe\\tPRP\\t(S(NP*)\\tO\\nwas\\tVBD\\t(VP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\npatriot\\tNN\\t*)\\tO\\nwho\\tWP\\t(SBAR(WHNP*)\\tO\\nrefused\\tVBD\\t(S(VP(VP*\\tO\\noccupation\\tNN\\t(NP(NP*)\\tO\\nand\\tCC\\t*\\tO\\nsurrender\\tNN\\t(NP(NP*)\\tO\\nto\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\ninvasion\\tNN\\t*)))))\\tO\\n,\\t,\\t*\\tO\\nand\\tCC\\t*\\tO\\nchose\\tVBD\\t(VP*\\tO\\nresistance\\tNN\\t(NP*))))))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'The\\tDT\\t(TOP(S(NP(NP*\\tO\\nonly\\tJJ\\t*\\tO\\nones\\tNNS\\t*)\\tO\\nwho\\tWP\\t(SBAR(WHNP*)\\tO\\npreceded\\tVBD\\t(S(VP*\\tO\\nhim\\tPRP\\t(NP*)\\tO\\nin\\tIN\\t(PP*\\tO\\nthis\\tDT\\t(NP*\\tO\\nhonor\\tNN\\t*))))))\\tO\\nwere\\tVBD\\t(VP*\\tO\\nmen\\tNNS\\t(NP(NP*)\\tO\\nlike\\tIN\\t(PP*\\tO\\nUmar\\tNNP\\t(NP(NP(NP*\\tB-PERSON\\nal\\tNNP\\t*\\tI-PERSON\\n-\\tHYPH\\t*\\tI-PERSON\\nMukhtar\\tNNP\\t*)\\tI-PERSON\\nand\\tCC\\t*\\tO\\nYusuf\\tNNP\\t(NP*\\tB-PERSON\\nal\\tNNP\\t*\\tI-PERSON\\n-\\tHYPH\\t*\\tI-PERSON\\nAzma\\tNNP\\t*))\\tI-PERSON\\n,\\t,\\t*\\tO\\nby\\tIN\\t(SBAR(WHPP*\\tO\\nwhose\\tWP$\\t(WHNP*\\tO\\nstruggle\\tNN\\t*))\\tO\\nthe\\tDT\\t(S(NP(NP*\\tO\\nnation\\tNN\\t*)\\tO\\nand\\tCC\\t*\\tO\\nits\\tPRP$\\t(NP*\\tO\\nhistory\\tNN\\t*))\\tO\\nwere\\tVBD\\t(VP*\\tO\\nhonored\\tVBN\\t(VP*))))))))\\tO\\n,\\t,\\t*\\tO\\nand\\tCC\\t(X*\\tO\\nkept\\tVBD\\t(VP*\\tO\\nit\\tPRP\\t(NP*)\\tO\\nfor\\tIN\\t(PP*\\tO\\nthem\\tPRP\\t(NP*))\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nrecords\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\npride\\tNN\\t(NP*\\tO\\nand\\tCC\\t*\\tO\\ndignity\\tNN\\t*))))))\\tO\\n.\\t.\\t*))\\tO']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5e15110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7901892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d997328",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/development/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc40ca6",
   "metadata": {},
   "source": [
    "### Create test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b203802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training filJe with all the news corpus\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.bn.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "556a0e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c848103b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Iraqi\\tJJ\\t(TOP(S(NP(NML*\\tB-NORP\\nleader\\tNN\\t*)\\tO\\nSaddam\\tNNP\\t*\\tB-PERSON\\nHussein\\tNNP\\t*)\\tI-PERSON\\nhas\\tVBZ\\t(VP*\\tO\\ngiven\\tVBN\\t(VP*\\tO\\na\\tDT\\t(NP*\\tO\\ndefiant\\tJJ\\t*\\tO\\nspeech\\tNN\\t*)\\tO\\nto\\tTO\\t(S(VP*\\tO\\nmark\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\ntenth\\tJJ\\t*\\tB-ORDINAL\\nanniversary\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tB-EVENT\\nGulf\\tNNP\\t*\\tI-EVENT\\nWar\\tNNP\\t*))))))))\\tI-EVENT\\n.\\t.\\t*))\\tO',\n",
       " 'He\\tPRP\\t(TOP(S(NP*)\\tO\\nsays\\tVBZ\\t(VP*\\tO\\nIraq\\tNNP\\t(SBAR(S(NP*)\\tB-GPE\\nhas\\tVBZ\\t(VP*\\tO\\ntriumphed\\tVBN\\t(VP*\\tO\\nover\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nevil\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nWest\\tNNP\\t*)))))))))\\tB-LOC\\n.\\t.\\t*))\\tO',\n",
       " 'Barbara\\tNNP\\t(TOP(S(NP*\\tB-PERSON\\nPlett\\tNNP\\t*)\\tI-PERSON\\nreports\\tVBZ\\t(VP*\\tO\\nfrom\\tIN\\t(PP*\\tO\\nBaghdad\\tNNP\\t(NP*)))\\tB-GPE\\n.\\t.\\t*))\\tO',\n",
       " 'Saddam\\tNNP\\t(TOP(S(NP*\\tB-PERSON\\nHussein\\tNNP\\t*)\\tI-PERSON\\naddressed\\tVBD\\t(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nnation\\tNN\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nspeech\\tNN\\t*)\\tO\\nfilled\\tVBN\\t(VP*\\tO\\nwith\\tIN\\t(PP*\\tO\\nrhetoric\\tNN\\t(NP(NP*)\\tO\\nand\\tCC\\t*\\tO\\na\\tDT\\t(NP(NP*\\tO\\ndeclaration\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nvictory\\tNN\\t(NP*)))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " \"``\\t``\\t(TOP(S*\\tO\\nIraq\\tNNP\\t(S(NP*)\\tB-GPE\\nhas\\tVBZ\\t(VP*\\tO\\ntriumphed\\tVBN\\t(VP*\\tO\\nover\\tIN\\t(PP*\\tO\\nits\\tPRP$\\t(NP*\\tO\\nenemies\\tNNS\\t*)))))\\tO\\n,\\t,\\t*\\tO\\n''\\t''\\t*\\tO\\nhe\\tPRP\\t(PRN(S(NP*)\\tO\\nsaid\\tVBD\\t(VP*)))\\tO\\n,\\t,\\t*\\tO\\n``\\t``\\t*\\tO\\nand\\tCC\\t*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\nwill\\tMD\\t(VP*\\tO\\ntriumph\\tVB\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nremaining\\tVBG\\t*\\tO\\nrounds\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nbattle\\tNN\\t*)))))))\\tO\\n.\\t.\\t*\\tO\\n''\\t''\\t*))\\tO\",\n",
       " 'The\\tDT\\t(TOP(S(NP*\\tO\\nIraqi\\tJJ\\t*\\tB-NORP\\nleader\\tNN\\t*)\\tO\\nsaid\\tVBD\\t(VP*\\tO\\nthe\\tDT\\t(SBAR(S(NP*\\tB-EVENT\\nGulf\\tNNP\\t*\\tI-EVENT\\nWar\\tNNP\\t*)\\tI-EVENT\\nwas\\tVBD\\t(VP*\\tO\\na\\tDT\\t(NP(NP(NP*\\tO\\nconfrontation\\tNN\\t*)\\tO\\nbetween\\tIN\\t(PP*\\tO\\ngood\\tNN\\t(NP*\\tO\\nand\\tCC\\t*\\tO\\nevil\\tNN\\t*)))\\tO\\nthat\\tWDT\\t(SBAR(WHNP*)\\tO\\ncontinues\\tVBZ\\t(S(VP*\\tO\\nto\\tIN\\t(PP*\\tO\\nthis\\tDT\\t(NP*\\tO\\nday\\tNN\\t*))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'He\\tPRP\\t(TOP(S(NP*)\\tO\\nwas\\tVBD\\t(VP*\\tO\\nreferring\\tVBG\\t(VP*\\tO\\nto\\tIN\\t(PP*\\tO\\nhis\\tPRP$\\t(NP(NP*\\tO\\nstruggle\\tNN\\t*)\\tO\\nagainst\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nUS\\tNNP\\t*)\\tB-GPE\\n,\\t,\\t*\\tO\\nwhich\\tWDT\\t(SBAR(WHNP*)\\tO\\nstrongly\\tRB\\t(S(ADVP*)\\tO\\nsupports\\tVBZ\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\neconomic\\tJJ\\t*\\tO\\nembargo\\tNN\\t*)\\tO\\nmeant\\tVBN\\t(VP*\\tO\\nto\\tTO\\t(S(VP*\\tO\\nforce\\tVB\\t(VP*\\tO\\nhim\\tPRP\\t(NP*)\\tO\\nto\\tTO\\t(S(VP*\\tO\\ndisarm\\tVB\\t(VP*)))))))))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'After\\tIN\\t(TOP(S(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nspeech\\tNN\\t*))\\tO\\ndemonstrators\\tNNS\\t(NP*)\\tO\\ngathered\\tVBD\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\npublic\\tJJ\\t*\\tO\\nshow\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nsupport\\tNN\\t(NP(NP*)\\tO\\nfor\\tIN\\t(PP*\\tO\\ntheir\\tPRP$\\t(NP*\\tO\\nPresident\\tNNP\\t*)))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'A\\tDT\\t(TOP(S(NP(NP*\\tO\\nsense\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nnationalism\\tNN\\t(NP*)))\\tO\\nhas\\tVBZ\\t(VP*\\tO\\ngrown\\tVBN\\t(VP*\\tO\\nstronger\\tJJR\\t(ADJP*)\\tO\\nhere\\tRB\\t(ADVP*)\\tO\\nas\\tIN\\t(SBAR*\\tO\\nmany\\tJJ\\t(S(NP*\\tO\\npeople\\tNNS\\t*)\\tO\\nrally\\tVBP\\t(VP*\\tO\\nagainst\\tIN\\t(PP*\\tO\\nwhat\\tWP\\t(SBAR(WHNP*)\\tO\\nthey\\tPRP\\t(S(NP*)\\tO\\nsay\\tVBP\\t(VP*\\tO\\nis\\tVBZ\\t(SBAR(S(VP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nsiege\\tNN\\t*)\\tO\\non\\tIN\\t(PP*\\tO\\ntheir\\tPRP$\\t(NP*\\tO\\ncountry\\tNN\\t*)))))))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'Barbara\\tNNP\\t(TOP(FRAG(NP*\\tB-PERSON\\nPlett\\tNNP\\t*)\\tI-PERSON\\n,\\t,\\t*\\tO\\nBBC\\tNNP\\t(NP*\\tB-ORG\\nNews\\tNNP\\t*)\\tI-ORG\\n,\\t,\\t*\\tO\\nBaghdad\\tNNP\\t(NP*)\\tB-GPE\\n.\\t.\\t*))\\tO']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d115bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09dadc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "586b9172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.mz.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30c753da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33682301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.nw.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "711d912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ebb90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da1bf3a",
   "metadata": {},
   "source": [
    "### import data in CONLL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3174f407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|This is The World...|[{document, 0, 88...|[{document, 0, 88...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "| I am Lisa Mullins .|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|A plan was announ...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 0, A,...|[{pos, 0, 0, DT, ...|[{named_entity, 0...|\n",
      "|The Kursk Foundat...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The BBC 's James ...|[{document, 0, 66...|[{document, 0, 66...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|He says there are...|[{document, 0, 57...|[{document, 0, 57...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|Obviously the fir...|[{document, 0, 61...|[{document, 0, 61...|[{token, 0, 8, Ob...|[{pos, 0, 8, RB, ...|[{named_entity, 0...|\n",
      "|There are still t...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 4, Th...|[{pos, 0, 4, EX, ...|[{named_entity, 0...|\n",
      "|Secondly , there ...|[{document, 0, 79...|[{document, 0, 79...|[{token, 0, 7, Se...|[{pos, 0, 7, RB, ...|[{named_entity, 0...|\n",
      "|The Kursk Foundat...|[{document, 0, 30...|[{document, 0, 30...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|That 's the dange...|[{document, 0, 44...|[{document, 0, 44...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|I would think the...|[{document, 0, 56...|[{document, 0, 56...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|I mean this is a ...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|     That 's right .|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|The size of these...|[{document, 0, 51...|[{document, 0, 51...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|In fact one of th...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|Now , it 's lying...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 2, No...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|It is a huge subm...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|There are any num...|[{document, 0, 19...|[{document, 0, 19...|[{token, 0, 4, Th...|[{pos, 0, 4, EX, ...|[{named_entity, 0...|\n",
      "|Second problem , ...|[{document, 0, 77...|[{document, 0, 77...|[{token, 0, 5, Se...|[{pos, 0, 5, JJ, ...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing the training set\n",
    "from sparknlp.training import CoNLL\n",
    "\n",
    "training_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/train/sample.train')\n",
    "training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80fc3b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Abd al - Bari Atw...|[{document, 0, 93...|[{document, 0, 93...|[{token, 0, 2, Ab...|[{pos, 0, 2, NNP,...|[{named_entity, 0...|\n",
      "|                Asad|[{document, 0, 3,...|[{document, 0, 3,...|[{token, 0, 3, As...|[{pos, 0, 3, NNP,...|[{named_entity, 0...|\n",
      "|           1/02/2007|[{document, 0, 8,...|[{document, 0, 8,...|[{token, 0, 8, 1/...|[{pos, 0, 8, NN, ...|[{named_entity, 0...|\n",
      "|This unprecedente...|[{document, 0, 27...|[{document, 0, 27...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|The American admi...|[{document, 0, 34...|[{document, 0, 34...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Saddam Hussein pr...|[{document, 0, 28...|[{document, 0, 28...|[{token, 0, 5, Sa...|[{pos, 0, 5, NNP,...|[{named_entity, 0...|\n",
      "|He confirmed it b...|[{document, 0, 81...|[{document, 0, 81...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|He will be envied...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|Saddam Hussein wa...|[{document, 0, 20...|[{document, 0, 20...|[{token, 0, 5, Sa...|[{pos, 0, 5, NNP,...|[{named_entity, 0...|\n",
      "|The only ones who...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|They hastily exec...|[{document, 0, 44...|[{document, 0, 44...|[{token, 0, 3, Th...|[{pos, 0, 3, PRP,...|[{named_entity, 0...|\n",
      "|He embarrassed th...|[{document, 0, 84...|[{document, 0, 84...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|They were terrifi...|[{document, 0, 57...|[{document, 0, 57...|[{token, 0, 3, Th...|[{pos, 0, 3, PRP,...|[{named_entity, 0...|\n",
      "|They were exposed...|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 3, Th...|[{pos, 0, 3, PRP,...|[{named_entity, 0...|\n",
      "|So they decided t...|[{document, 0, 77...|[{document, 0, 77...|[{token, 0, 1, So...|[{pos, 0, 1, CC, ...|[{named_entity, 0...|\n",
      "|Thanks to the tec...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 5, Th...|[{pos, 0, 5, NNS,...|[{named_entity, 0...|\n",
      "|This tape exposed...|[{document, 0, 17...|[{document, 0, 17...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|It presented us w...|[{document, 0, 85...|[{document, 0, 85...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|He was right when...|[{document, 0, 27...|[{document, 0, 27...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|Is it manliness t...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 1, Is...|[{pos, 0, 1, VBZ,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/development/sample.train')\n",
    "dev_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7e4b286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Iraqi leader Sadd...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 4, Ir...|[{pos, 0, 4, JJ, ...|[{named_entity, 0...|\n",
      "|He says Iraq has ...|[{document, 0, 53...|[{document, 0, 53...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|Barbara Plett rep...|[{document, 0, 35...|[{document, 0, 35...|[{token, 0, 6, Ba...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
      "|Saddam Hussein ad...|[{document, 0, 98...|[{document, 0, 98...|[{token, 0, 5, Sa...|[{pos, 0, 5, NNP,...|[{named_entity, 0...|\n",
      "|`` Iraq has trium...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 1, ``...|[{pos, 0, 1, ``, ...|[{named_entity, 0...|\n",
      "|The Iraqi leader ...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|He was referring ...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|After the speech ...|[{document, 0, 88...|[{document, 0, 88...|[{token, 0, 4, Af...|[{pos, 0, 4, IN, ...|[{named_entity, 0...|\n",
      "|A sense of nation...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 0, A,...|[{pos, 0, 0, DT, ...|[{named_entity, 0...|\n",
      "|Barbara Plett , B...|[{document, 0, 35...|[{document, 0, 35...|[{token, 0, 6, Ba...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
      "| This is The World .|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|    I am Tony Kahn .|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|Gao Xingjian arri...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 2, Ga...|[{pos, 0, 2, NNP,...|[{named_entity, 0...|\n",
      "|Today Gao , who n...|[{document, 0, 17...|[{document, 0, 17...|[{token, 0, 4, To...|[{pos, 0, 4, NN, ...|[{named_entity, 0...|\n",
      "|Gao 's 1989 novel...|[{document, 0, 84...|[{document, 0, 84...|[{token, 0, 2, Ga...|[{pos, 0, 2, NNP,...|[{named_entity, 0...|\n",
      "|If you want to kn...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 1, If...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|His politics are ...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, Hi...|[{pos, 0, 2, PRP$...|[{named_entity, 0...|\n",
      "|This novel ` soul...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|It 's a large nov...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|It is , if you li...|[{document, 0, 62...|[{document, 0, 62...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e54ee7",
   "metadata": {},
   "source": [
    "### Training the model - 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5203e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base_cased download started this may take some time.\n",
      "Approximate size to download 389.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# use bert embeddings\n",
    "bert = BertEmbeddings.pretrained('bert_base_cased', 'en').setInputCols([\"sentence\",'token']).setOutputCol(\"bert\").setCaseSensitive(True)#.setMaxSentenceLength(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d0c34ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the training data into embeddings and saving it as parquet files\n",
    "readyTrainingData = bert.transform(training_data)\n",
    "\n",
    "readyTrainingData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab96caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "readyTrainingData = spark.read.parquet(\"/tmp/conll2003/bert_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c4f2096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the development data into embeddings and saving it as parquet files\n",
    "readyDevData = bert.transform(dev_data)\n",
    "\n",
    "readyDevData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0c284d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "readyDevData = spark.read.parquet(\"/tmp/conll2003/bert_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d64dac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa34583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "readyTestData = spark.read.parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0897336",
   "metadata": {},
   "source": [
    "### Dev set - news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7bd2f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize NER tagger\n",
    "nerTagger = NerDLApproach()\\\n",
    ".setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
    ".setLabelColumn(\"label\")\\\n",
    ".setOutputCol(\"ner\")\\\n",
    ".setMaxEpochs(10)\\\n",
    ".setBatchSize(4)\\\n",
    ".setEnableMemoryOptimizer(True)\\\n",
    ".setRandomSeed(0)\\\n",
    ".setVerbose(1)\\\n",
    ".setValidationSplit(0.2)\\\n",
    ".setEvaluationLogExtended(True)\\\n",
    ".setEnableOutputLogs(True)\\\n",
    ".setIncludeConfidence(True)\\\n",
    ".setTestDataset(\"/tmp/conll2003/bert_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68c9418e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 803 ms, sys: 533 ms, total: 1.34 s\n",
      "Wall time: 8h 57min 44s\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "%time myNerModel = nerTagger.fit(readyTrainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327a58c4",
   "metadata": {},
   "source": [
    "### Testing on News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "822a465e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.17 s, sys: 363 ms, total: 2.53 s\n",
      "Wall time: 21 s\n"
     ]
    }
   ],
   "source": [
    "# infer from the trained model\n",
    "%time results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c24e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c50bb9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[88, 392]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5da5e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a198acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "275e30d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9766470856051437\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "445fbe5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8922542204568022\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cdac3a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.85      0.85      0.85       616\n",
      "        DATE       0.83      0.89      0.86      1252\n",
      "       EVENT       0.73      0.65      0.69        37\n",
      "         FAC       0.70      0.47      0.56        66\n",
      "         GPE       0.97      0.94      0.95      1664\n",
      "    LANGUAGE       0.71      0.50      0.59        10\n",
      "         LAW       0.77      0.56      0.65        36\n",
      "         LOC       0.84      0.75      0.79       144\n",
      "       MONEY       0.89      0.90      0.90       279\n",
      "        NORP       0.96      0.94      0.95       579\n",
      "     ORDINAL       0.80      0.91      0.85       118\n",
      "         ORG       0.87      0.90      0.88      1494\n",
      "     PERCENT       0.92      0.92      0.92       293\n",
      "      PERSON       0.94      0.95      0.95      1099\n",
      "     PRODUCT       0.64      0.55      0.59        71\n",
      "    QUANTITY       0.70      0.78      0.74        59\n",
      "        TIME       0.57      0.64      0.60       106\n",
      " WORK_OF_ART       0.84      0.65      0.73       111\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      8034\n",
      "   macro avg       0.81      0.76      0.78      8034\n",
      "weighted avg       0.89      0.89      0.89      8034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9202cdb8",
   "metadata": {},
   "source": [
    "### Testing on News Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e8a88d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.bc.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae1562fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b6103b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15469afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|-- basically , it...|[{document, 0, 78...|[{document, 0, 78...|[{token, 0, 1, --...|[{pos, 0, 1, :, {...|[{named_entity, 0...|\n",
      "|To express its de...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 1, To...|[{pos, 0, 1, TO, ...|[{named_entity, 0...|\n",
      "|It takes time to ...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|Dear viewers , th...|[{document, 0, 52...|[{document, 0, 52...|[{token, 0, 3, De...|[{pos, 0, 3, NNP,...|[{named_entity, 0...|\n",
      "|     This is Xu Li .|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|Thank you everyon...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 4, Th...|[{pos, 0, 4, VBP,...|[{named_entity, 0...|\n",
      "|Coming up is the ...|[{document, 0, 59...|[{document, 0, 59...|[{token, 0, 5, Co...|[{pos, 0, 5, VBG,...|[{named_entity, 0...|\n",
      "|Good-bye , dear v...|[{document, 0, 24...|[{document, 0, 24...|[{token, 0, 7, Go...|[{pos, 0, 7, UH, ...|[{named_entity, 0...|\n",
      "|Hello , dear view...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 4, He...|[{pos, 0, 4, UH, ...|[{named_entity, 0...|\n",
      "|Welcome to Focus ...|[{document, 0, 23...|[{document, 0, 23...|[{token, 0, 6, We...|[{pos, 0, 6, VBP,...|[{named_entity, 0...|\n",
      "|Today , let 's tu...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 4, To...|[{pos, 0, 4, NN, ...|[{named_entity, 0...|\n",
      "|Before dawn on Ja...|[{document, 0, 19...|[{document, 0, 19...|[{token, 0, 5, Be...|[{pos, 0, 5, IN, ...|[{named_entity, 0...|\n",
      "|Relevant departme...|[{document, 0, 94...|[{document, 0, 94...|[{token, 0, 7, Re...|[{pos, 0, 7, JJ, ...|[{named_entity, 0...|\n",
      "|The traffic admin...|[{document, 0, 94...|[{document, 0, 94...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Well , how did th...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 3, We...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|After the holiday...|[{document, 0, 72...|[{document, 0, 72...|[{token, 0, 4, Af...|[{pos, 0, 4, IN, ...|[{named_entity, 0...|\n",
      "|In addition , wha...|[{document, 0, 19...|[{document, 0, 19...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|Well , we have in...|[{document, 0, 93...|[{document, 0, 93...|[{token, 0, 3, We...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|One of the two ho...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 2, On...|[{pos, 0, 2, CD, ...|[{named_entity, 0...|\n",
      "|             Hello .|[{document, 0, 6,...|[{document, 0, 6,...|[{token, 0, 4, He...|[{pos, 0, 4, UH, ...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "71c67d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cf52b8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.28 s, sys: 388 ms, total: 1.67 s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "# infer from the trained model\n",
    "%time results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "985cb059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af4130a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "25428c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fca4070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "23eb70dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9754063038660429\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "27452eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7887240356083086\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f39a1c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.71      0.74      0.73       182\n",
      "        DATE       0.73      0.79      0.76       200\n",
      "       EVENT       0.50      0.14      0.22        14\n",
      "         FAC       0.93      0.83      0.88        48\n",
      "         GPE       0.91      0.91      0.91       353\n",
      "         LAW       1.00      0.00      0.00         3\n",
      "         LOC       1.00      0.38      0.56        26\n",
      "       MONEY       1.00      1.00      1.00         3\n",
      "        NORP       0.62      0.63      0.62       138\n",
      "     ORDINAL       0.88      0.86      0.87        50\n",
      "         ORG       0.73      0.82      0.77       153\n",
      "     PERCENT       0.93      0.93      0.93        14\n",
      "      PERSON       0.93      0.91      0.92       382\n",
      "    QUANTITY       0.21      0.15      0.17        40\n",
      "        TIME       0.52      0.56      0.54        63\n",
      " WORK_OF_ART       0.30      0.11      0.16        28\n",
      "\n",
      "   micro avg       0.79      0.78      0.79      1697\n",
      "   macro avg       0.74      0.61      0.63      1697\n",
      "weighted avg       0.79      0.78      0.78      1697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929acf71",
   "metadata": {},
   "source": [
    "### Testing on WebLogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0597d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.wb.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bbd25fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6524551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6c942177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|The success of al...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The Source of the...|[{document, 0, 23...|[{document, 0, 23...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The al - Jazeera ...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|In this film the ...|[{document, 0, 73...|[{document, 0, 73...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|The Hebrew channe...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|This is a unique ...|[{document, 0, 45...|[{document, 0, 45...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|There are many qu...|[{document, 0, 20...|[{document, 0, 20...|[{token, 0, 4, Th...|[{pos, 0, 4, EX, ...|[{named_entity, 0...|\n",
      "|I leave you with ...|[{document, 0, 72...|[{document, 0, 72...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|the link to the f...|[{document, 0, 22...|[{document, 0, 22...|[{token, 0, 2, th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|http://z08.zuploa...|[{document, 0, 52...|[{document, 0, 52...|[{token, 0, 52, h...|[{pos, 0, 52, ADD...|[{named_entity, 0...|\n",
      "|The Islam Diary :...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Speaking at the s...|[{document, 0, 44...|[{document, 0, 44...|[{token, 0, 7, Sp...|[{pos, 0, 7, VBG,...|[{named_entity, 0...|\n",
      "|He also told the ...|[{document, 0, 34...|[{document, 0, 34...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|The newspaper quo...|[{document, 0, 33...|[{document, 0, 33...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The newspaper des...|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|He speaks in the ...|[{document, 0, 93...|[{document, 0, 93...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|He was quoted as ...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|It pointed out th...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|This means that t...|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|\" Fadil \" was quo...|[{document, 0, 22...|[{document, 0, 22...|[{token, 0, 0, \",...|[{pos, 0, 0, ``, ...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3e3adc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bee7150b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 424 ms, sys: 346 ms, total: 771 ms\n",
      "Wall time: 34.1 s\n"
     ]
    }
   ],
   "source": [
    "# infer from the trained model\n",
    "%time results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8cb952ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ff92a64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "572ca05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "485a5704",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1e8cdd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9631037212984956\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "96bea8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7555749890686488\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6f84964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.83      0.74      0.78        85\n",
      "        DATE       0.46      0.62      0.53        74\n",
      "       EVENT       0.43      0.25      0.32        12\n",
      "         FAC       0.60      0.17      0.26        18\n",
      "         GPE       0.93      0.94      0.93       173\n",
      "    LANGUAGE       0.25      0.25      0.25         4\n",
      "         LAW       1.00      1.00      1.00         1\n",
      "         LOC       1.00      0.56      0.71         9\n",
      "       MONEY       0.66      0.76      0.70        25\n",
      "        NORP       0.92      0.92      0.92       107\n",
      "     ORDINAL       0.80      0.67      0.73        18\n",
      "         ORG       0.55      0.85      0.67       117\n",
      "     PERCENT       0.58      0.58      0.58        33\n",
      "      PERSON       0.85      0.76      0.81       407\n",
      "     PRODUCT       0.00      0.00      0.00         1\n",
      "    QUANTITY       0.50      0.33      0.40         6\n",
      "        TIME       0.46      0.65      0.54        20\n",
      " WORK_OF_ART       0.43      0.22      0.29        27\n",
      "\n",
      "   micro avg       0.75      0.76      0.76      1137\n",
      "   macro avg       0.62      0.57      0.58      1137\n",
      "weighted avg       0.77      0.76      0.76      1137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629decfc",
   "metadata": {},
   "source": [
    "### Testing on Telephonic Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4f401797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.tc.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6b826c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f7b8fe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "74968763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|But %um , guessed...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 2, Bu...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|              What ?|[{document, 0, 5,...|[{document, 0, 5,...|[{token, 0, 3, Wh...|[{pos, 0, 3, WP, ...|[{named_entity, 0...|\n",
      "|         %um The %um|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, %u...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|             Again ?|[{document, 0, 6,...|[{document, 0, 6,...|[{token, 0, 4, Ag...|[{pos, 0, 4, RB, ...|[{named_entity, 0...|\n",
      "|%um 118.91_120.82...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 2, %u...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|       two weeks ago|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 2, tw...|[{pos, 0, 2, CD, ...|[{named_entity, 0...|\n",
      "|I know Sue is goi...|[{document, 0, 36...|[{document, 0, 36...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|         %eh Right .|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, %e...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|             Right .|[{document, 0, 6,...|[{document, 0, 6,...|[{token, 0, 4, Ri...|[{pos, 0, 4, UH, ...|[{named_entity, 0...|\n",
      "|Did she see her m...|[{document, 0, 20...|[{document, 0, 20...|[{token, 0, 2, Di...|[{pos, 0, 2, VBD,...|[{named_entity, 0...|\n",
      "|       Yes she did .|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 2, Ye...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|               And ?|[{document, 0, 4,...|[{document, 0, 4,...|[{token, 0, 2, An...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|       Yes she did .|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 2, Ye...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|               And ?|[{document, 0, 4,...|[{document, 0, 4,...|[{token, 0, 2, An...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|%um , %um , which...|[{document, 0, 73...|[{document, 0, 73...|[{token, 0, 2, %u...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|I spoke to her mo...|[{document, 0, 29...|[{document, 0, 29...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "| %um and I asked her|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 2, %u...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|       I said listen|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|I said %um please...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|things have come ...|[{document, 0, 78...|[{document, 0, 78...|[{token, 0, 5, th...|[{pos, 0, 5, NNS,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cf3efcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eabb88c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 346 ms, sys: 325 ms, total: 671 ms\n",
      "Wall time: 30.2 s\n"
     ]
    }
   ],
   "source": [
    "# infer from the trained model\n",
    "%time results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "75b3e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a64ec236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ccb527cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "788c5188",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0d231fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9677478134110787\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c2cdc728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6472019464720195\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0475c395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.50      0.44      0.47        52\n",
      "        DATE       0.55      0.78      0.64        74\n",
      "         FAC       1.00      0.00      0.00         3\n",
      "         GPE       0.90      0.88      0.89        50\n",
      "    LANGUAGE       1.00      0.75      0.86         8\n",
      "         LOC       0.00      1.00      0.00         0\n",
      "       MONEY       1.00      0.71      0.83         7\n",
      "        NORP       0.81      1.00      0.89        17\n",
      "     ORDINAL       0.75      1.00      0.86         9\n",
      "         ORG       0.28      0.37      0.32        27\n",
      "     PERCENT       0.07      0.50      0.13         6\n",
      "      PERSON       0.85      0.85      0.85       100\n",
      "     PRODUCT       0.00      0.00      0.00         4\n",
      "    QUANTITY       0.00      1.00      0.00         0\n",
      "        TIME       0.43      0.26      0.32        23\n",
      " WORK_OF_ART       0.00      1.00      0.00         0\n",
      "\n",
      "   micro avg       0.60      0.70      0.65       380\n",
      "   macro avg       0.51      0.66      0.44       380\n",
      "weighted avg       0.67      0.70      0.67       380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a27f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
