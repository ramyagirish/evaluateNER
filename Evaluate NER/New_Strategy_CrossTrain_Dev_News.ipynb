{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1edef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  3.1.0\n",
      "Apache Spark version:  3.1.2\n"
     ]
    }
   ],
   "source": [
    "# call relevant packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "import sparknlp\n",
    "\n",
    "# to use GPU \n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb98346",
   "metadata": {},
   "source": [
    "## Creating a composite CoNLL file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4910c848",
   "metadata": {},
   "source": [
    "### Create training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "024205af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training filJe with all the news corpus\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/onto.bn.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85935111",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769deee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This\\tDT\\t(TOP(S(NP*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nThe\\tDT\\t(NP(NP*\\tB-ORG\\nWorld\\tNNP\\t*)\\tI-ORG\\n,\\t,\\t*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nco-production\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tB-ORG\\nBBC\\tNNP\\t*\\tI-ORG\\nWorld\\tNNP\\t*\\tI-ORG\\nService\\tNNP\\t*)\\tI-ORG\\n,\\t,\\t*\\tO\\nPRI\\tNNP\\t(NP*)\\tB-ORG\\n,\\t,\\t*\\tO\\nand\\tCC\\t*\\tO\\nWGBH\\tNNP\\t(NP(NP*)\\tB-ORG\\nin\\tIN\\t(PP*\\tO\\nBoston\\tNNP\\t(NP*))))))))\\tB-GPE\\n.\\t.\\t*))\\tO',\n",
       " 'I\\tPRP\\t(TOP(S(NP*)\\tO\\nam\\tVBP\\t(VP*\\tO\\nLisa\\tNNP\\t(NP*\\tB-PERSON\\nMullins\\tNNP\\t*))\\tI-PERSON\\n.\\t.\\t*))\\tO',\n",
       " \"A\\tDT\\t(TOP(S(NP*\\tO\\nplan\\tNN\\t*)\\tO\\nwas\\tVBD\\t(VP*\\tO\\nannounced\\tVBN\\t(VP*\\tO\\ntoday\\tNN\\t(NP*)\\tB-DATE\\nto\\tTO\\t(S(VP*\\tO\\nraise\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nRussian\\tJJ\\t*\\tB-NORP\\nnuclear\\tJJ\\t*\\tO\\nsubmarine\\tNN\\t*)\\tO\\n`\\t''\\t(NP*\\tO\\nKursk\\tNNP\\t*\\tB-PRODUCT\\n'\\t''\\t*))\\tO\\n,\\t,\\t*\\tO\\nnext\\tJJ\\t(NP(NP*\\tB-DATE\\nsummer\\tNN\\t*)\\tI-DATE\\n,\\t,\\t*\\tO\\nalmost\\tRB\\t(SBAR(NP(QP*\\tB-DATE\\na\\tDT\\t*)\\tI-DATE\\nyear\\tNN\\t*)\\tI-DATE\\nafter\\tIN\\t*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\nhit\\tVBD\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nbottom\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tB-LOC\\nBarents\\tNNP\\t*\\tI-LOC\\nSea\\tNNP\\t*)))\\tI-LOC\\nfollowing\\tVBG\\t(PP*\\tO\\na\\tDT\\t(NP*\\tO\\nmysterious\\tJJ\\t*\\tO\\naccident\\tNN\\t*)))))))))))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'The\\tDT\\t(TOP(S(NP(NP*\\tB-ORG\\nKursk\\tNNP\\t*\\tI-ORG\\nFoundation\\tNNP\\t*)\\tI-ORG\\n,\\t,\\t*\\tO\\nan\\tDT\\t(NP(NP*\\tO\\ninternational\\tJJ\\t*\\tO\\nconsortium\\tNN\\t*)\\tO\\nled\\tVBN\\t(VP*\\tO\\nby\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\ngovernments\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nRussia\\tNNP\\t(NP(NP*)\\tB-GPE\\nand\\tCC\\t*\\tO\\nThe\\tDT\\t(NP*\\tB-GPE\\nNetherlands\\tNNP\\t*))))))))\\tI-GPE\\nsays\\tVBZ\\t(VP*\\tO\\nit\\tPRP\\t(SBAR(S(NP*)\\tO\\ncan\\tMD\\t(VP*\\tO\\ndo\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\njob\\tNN\\t*)\\tO\\nsafely\\tRB\\t(ADVP*)\\tO\\nif\\tIN\\t(SBAR*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\ncan\\tMD\\t(VP*\\tO\\nsecure\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nfunding\\tNN\\t*))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " \"The\\tDT\\t(TOP(S(NP(NP*\\tO\\nBBC\\tNNP\\t*\\tB-ORG\\n's\\tPOS\\t*)\\tO\\nJames\\tNNP\\t*\\tB-PERSON\\nRogers\\tNNP\\t*)\\tI-PERSON\\nwas\\tVBD\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nBrussels\\tNNP\\t(NP*))\\tB-GPE\\nfor\\tIN\\t(PP*\\tO\\ntoday\\tNN\\t(NP(NP*\\tB-DATE\\n's\\tPOS\\t*)\\tO\\nannouncement\\tNN\\t*)))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'He\\tPRP\\t(TOP(S(NP*)\\tO\\nsays\\tVBZ\\t(VP*\\tO\\nthere\\tEX\\t(SBAR(S(NP*)\\tO\\nare\\tVBP\\t(VP*\\tO\\ntwo\\tCD\\t(NP(NP*\\tB-CARDINAL\\nmain\\tJJ\\t*\\tO\\nreasons\\tNNS\\t*)\\tO\\nfor\\tIN\\t(PP*\\tO\\nraising\\tVBG\\t(S(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nKursk\\tNNP\\t*)))))))))\\tB-PRODUCT\\n.\\t.\\t*))\\tO',\n",
       " 'Obviously\\tRB\\t(TOP(S(ADVP*)\\tO\\nthe\\tDT\\t(NP*\\tO\\nfirst\\tJJ\\t*\\tB-ORDINAL\\none\\tNN\\t*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nto\\tTO\\t(S(VP*\\tO\\nrecover\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nbodies\\tNNS\\t*)\\tO\\nthat\\tWDT\\t(SBAR(WHNP*)\\tO\\nremain\\tVBP\\t(S(VP*))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'There\\tEX\\t(TOP(S(NP*)\\tO\\nare\\tVBP\\t(VP*\\tO\\nstill\\tRB\\t(ADVP*)\\tO\\nthe\\tDT\\t(NP(NP(NP*\\tO\\nbodies\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nmore\\tJJR\\t(NP(QP*\\tB-CARDINAL\\nthan\\tIN\\t*\\tI-CARDINAL\\n100\\tCD\\t*)\\tI-CARDINAL\\nsailors\\tNNS\\t*)))\\tO\\ntrapped\\tVBN\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nvessel\\tNN\\t*))\\tO\\nsince\\tIN\\t(SBAR*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\nsank\\tVBD\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tB-LOC\\nBarents\\tNNP\\t*\\tI-LOC\\nSea\\tNNP\\t*))\\tI-LOC\\nin\\tIN\\t(PP*\\tO\\nAugust\\tNNP\\t(NP(NP*)\\tB-DATE\\nof\\tIN\\t(PP*\\tI-DATE\\nlast\\tJJ\\t(NP*\\tI-DATE\\nyear\\tNN\\t*))))))))))\\tI-DATE\\n.\\t.\\t*))\\tO',\n",
       " \"Secondly\\tRB\\t(TOP(S(ADVP*)\\tB-ORDINAL\\n,\\t,\\t*\\tO\\nthere\\tEX\\t(S(NP*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\na\\tDT\\t(NP*\\tO\\nlonger\\tJJR\\t(NML*\\tO\\n-\\tHYPH\\t*\\tO\\nterm\\tNN\\t*)\\tO\\nthreat\\tNN\\t*)))\\tO\\nand\\tCC\\t*\\tO\\nthat\\tDT\\t(S(NP*)\\tO\\n's\\tVBZ\\t(VP*\\tO\\nan\\tDT\\t(NP*\\tO\\nenvironmental\\tJJ\\t*\\tO\\nthreat\\tNN\\t*)))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'The\\tDT\\t(TOP(S(NP*\\tB-ORG\\nKursk\\tNNP\\t*\\tI-ORG\\nFoundation\\tNNP\\t*)\\tI-ORG\\nsays\\tVBZ\\t(VP*\\tO\\nthat\\tIN\\t(SBAR(SBAR*\\tO\\nthe\\tDT\\t(S(NP(NP*\\tO\\nnuclear\\tJJ\\t*\\tO\\nreactors\\tNNS\\t*)\\tO\\n,\\t,\\t*\\tO\\nthere\\tEX\\t(PRN(S(NP*)\\tO\\nare\\tVBP\\t(VP*\\tO\\ntwo\\tCD\\t(NP(NP*)\\tB-CARDINAL\\non\\tIN\\t(PP*\\tO\\nboard\\tNN\\t(NP*)))))))\\tO\\n,\\t,\\t*\\tO\\nare\\tVBP\\t(VP*\\tO\\nsafe\\tJJ\\t(ADJP*\\tO\\nfor\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\ntime\\tNN\\t*\\tO\\nbeing\\tNN\\t*))))))\\tO\\nbut\\tCC\\t*\\tO\\nthat\\tIN\\t(SBAR*\\tO\\nthe\\tDT\\t(S(NP(NP*\\tO\\ncorrosive\\tJJ\\t*\\tO\\neffects\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nseawater\\tNN\\t(NP*)))\\tO\\ncould\\tMD\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\ntime\\tNN\\t(NP*))\\tO\\nlead\\tVB\\t(VP*\\tO\\nto\\tIN\\t(PP*\\tO\\nradioactive\\tJJ\\t(NP(NP(NP*\\tO\\nleaks\\tNNS\\t*)\\tO\\ninto\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nocean\\tNN\\t*)))\\tO\\n,\\t,\\t*\\tO\\nwhich\\tWDT\\t(SBAR(WHNP*)\\tO\\nobviously\\tRB\\t(S(ADVP*)\\tO\\ncould\\tMD\\t(VP*\\tO\\nhave\\tVB\\t(VP*\\tO\\ndevastating\\tJJ\\t(NP(NP*\\tO\\nconsequences\\tNNS\\t*)\\tO\\non\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nfishery\\tNN\\t*)\\tO\\nand\\tCC\\t*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nenvironment\\tNN\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\ngeneral\\tJJ\\t(ADJP*))))))))))))))))))\\tO\\n.\\t.\\t*))\\tO']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f341e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a5589da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41459c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/onto.mz.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84dedece",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f31fbf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/onto.nw.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5134538",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41e0b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa11d3b4",
   "metadata": {},
   "source": [
    "### Create dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "488f800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training filJe with all the news corpus\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/development/onto.bn.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc5fc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3f11a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The\\tDT\\t(TOP(S(NP(NP*\\tO\\nIsraeli\\tJJ\\t*\\tB-NORP\\nPrime\\tNNP\\t*\\tO\\nMinister\\tNNP\\t*)\\tO\\nEhud\\tNNP\\t(NP*\\tB-PERSON\\nBarak\\tNNP\\t*))\\tI-PERSON\\nis\\tVBZ\\t(VP*\\tO\\ndue\\tJJ\\t(ADJP*\\tO\\nto\\tTO\\t(S(VP*\\tO\\nmeet\\tVB\\t(VP*\\tO\\nwith\\tIN\\t(PP*\\tO\\nEgypt\\tNNP\\t(NP(NP*\\tB-GPE\\n's\\tPOS\\t*)\\tO\\nPresident\\tNNP\\t*\\tO\\nMubarak\\tNNP\\t*))\\tB-PERSON\\nin\\tIN\\t(PP*\\tO\\nCairo\\tNNP\\t(NP*))\\tB-GPE\\non\\tIN\\t(PP*\\tO\\nThursday\\tNNP\\t(NP*)))))))\\tB-DATE\\n.\\t.\\t*))\\tO\",\n",
       " \"The\\tDT\\t(TOP(S(NP*\\tO\\nencounter\\tNN\\t*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nbeing\\tVBG\\t(VP*\\tO\\nseen\\tVBN\\t(VP*\\tO\\nas\\tIN\\t(PP*\\tO\\nan\\tDT\\t(NP(NP*\\tO\\neffort\\tNN\\t*)\\tO\\nby\\tIN\\t(PP*\\tO\\nBarak\\tNNP\\t(NP*))\\tB-PERSON\\nto\\tTO\\t(S(VP*\\tO\\ndrum\\tVB\\t(VP*\\tO\\nup\\tRP\\t(PRT*)\\tO\\nregional\\tJJ\\t(NP(NP*\\tO\\nsupport\\tNN\\t*)\\tO\\nfor\\tIN\\t(PP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nMiddle\\tNNP\\t(NML*\\tB-LOC\\nEast\\tNNP\\t*)\\tI-LOC\\npeace\\tNN\\t*\\tO\\nagreement\\tNN\\t*)\\tO\\nbased\\tVBN\\t(VP*\\tO\\non\\tIN\\t(PP*\\tO\\nPresident\\tNNP\\t(NP(NP*\\tO\\nClinton\\tNNP\\t*\\tB-PERSON\\n's\\tPOS\\t*)\\tO\\nproposals\\tNNS\\t*))))))))))))))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'They\\tPRP\\t(TOP(S(S(NP*)\\tO\\nrequire\\tVBP\\t(VP(VP*\\tO\\nIsrael\\tNNP\\t(NP*)\\tB-GPE\\nto\\tTO\\t(S(VP*\\tO\\nrelinquish\\tVB\\t(VP*\\tO\\nsovereignty\\tNN\\t(NP(NP*)\\tO\\nover\\tIN\\t(PP*\\tO\\nmost\\tJJS\\t(NP(NP*)\\tO\\nof\\tIN\\t(PP*\\tO\\nArab\\tJJ\\t(NP*\\tB-NORP\\nEast\\tNNP\\t*\\tB-GPE\\nJerusalem\\tNNP\\t*)))))))))\\tI-GPE\\nand\\tCC\\t*\\tO\\nthe\\tDT\\t(VP(NP*\\tO\\nPalestinians\\tNNPS\\t*)\\tB-NORP\\nto\\tTO\\t(S(VP*\\tO\\nscale\\tVB\\t(VP*\\tO\\nback\\tRP\\t(PRT*)\\tO\\ntheir\\tPRP$\\t(NP(NP*\\tO\\ndemand\\tNN\\t*)\\tO\\nfor\\tIN\\t(PP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nreturn\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nrefugees\\tNNS\\t(NP*)))))))))))\\tO\\n,\\t,\\t*\\tO\\nbut\\tCC\\t*\\tO\\nPalestinian\\tJJ\\t(S(NP(NML*\\tB-NORP\\nchief\\tNN\\t*\\tO\\nnegotiator\\tNN\\t*)\\tO\\nSayeb\\tNNP\\t*\\tB-PERSON\\nErekat\\tNNP\\t*)\\tI-PERSON\\nsays\\tVBZ\\t(VP*\\tO\\nmore\\tJJR\\t(SBAR(S(NP*\\tO\\ndetails\\tNNS\\t*)\\tO\\nare\\tVBP\\t(VP*\\tO\\nrequired\\tVBN\\t(VP*\\tO\\nbefore\\tIN\\t(SBAR*\\tO\\nboth\\tDT\\t(S(NP*\\tO\\nsides\\tNNS\\t*)\\tO\\ncan\\tMD\\t(VP*\\tO\\nstrike\\tVB\\t(VP*\\tO\\na\\tDT\\t(NP*\\tO\\ndeal\\tNN\\t*)))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'We\\tPRP\\t(TOP(S(NP*)\\tO\\nhave\\tVBP\\t(VP*\\tO\\nso\\tRB\\t(NP(NP(NP(ADJP*\\tO\\nmany\\tJJ\\t*)\\tO\\nquestions\\tNNS\\t*)\\tO\\nthat\\tWDT\\t(SBAR(WHNP*)\\tO\\nwe\\tPRP\\t(S(NP*)\\tO\\nhave\\tVBP\\t(VP*))))\\tO\\n...\\t:\\t*\\tO\\nso\\tRB\\t(NP(NP(ADJP*\\tO\\nmany\\tJJ\\t*)\\tO\\npoints\\tNNS\\t*)\\tO\\nthat\\tWDT\\t(SBAR(WHNP*)\\tO\\nneed\\tVBP\\t(S(VP*\\tO\\nto\\tTO\\t(S(VP*\\tO\\nbe\\tVB\\t(VP*\\tO\\nclarified\\tVBN\\t(VP*))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'The\\tDT\\t(TOP(S(NP*\\tO\\nmost\\tRBS\\t(ADJP*\\tO\\nimportant\\tJJ\\t*)\\tO\\nthing\\tNN\\t*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nclarity\\tNN\\t*))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'The\\tDT\\t(TOP(S(NP*\\tO\\nmost\\tRBS\\t(ADJP*\\tO\\nimportant\\tJJ\\t*)\\tO\\nthing\\tNN\\t*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nthat\\tIN\\t(SBAR*\\tO\\nwe\\tPRP\\t(S(NP*)\\tO\\nfocus\\tVBP\\t(VP*\\tO\\non\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\ntravails\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\npeace\\tNN\\t*\\tO\\nprocess\\tNN\\t*))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'Israeli\\tJJ\\t(TOP(S(NP*\\tB-NORP\\nsettlers\\tNNS\\t*)\\tO\\nare\\tVBP\\t(VP*\\tO\\nworried\\tJJ\\t(ADJP*\\tO\\nthat\\tIN\\t(SBAR*\\tO\\nmost\\tJJS\\t(S(NP(NP*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nsettlements\\tNNS\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tB-GPE\\nWest\\tNNP\\t*\\tI-GPE\\nBank\\tNNP\\t*)))))\\tI-GPE\\nwill\\tMD\\t(VP*\\tO\\nbe\\tVB\\t(VP*\\tO\\ndismantled\\tVBN\\t(VP*\\tO\\nif\\tIN\\t(SBAR*\\tO\\nan\\tDT\\t(S(NP*\\tO\\nagreement\\tNN\\t*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nreached\\tVBN\\t(VP*)))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " \"The\\tDT\\t(TOP(S(NP(NP(NP*\\tO\\nspokeswoman\\tNN\\t*)\\tO\\nfor\\tIN\\t(PP*\\tO\\nthis\\tDT\\t(NP(NP*\\tB-ORG\\nSettlers\\tNNPS\\t*\\tI-ORG\\n'\\tPOS\\t*)\\tI-ORG\\nCouncil\\tNNP\\t*)))\\tI-ORG\\nYahudi\\tNNP\\t(NP*\\tB-PERSON\\nTayar\\tNNP\\t*))\\tI-PERSON\\nsays\\tVBZ\\t(VP*\\tO\\nthis\\tDT\\t(SBAR(S(NP*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nunacceptable\\tJJ\\t(ADJP*)))))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'We\\tPRP\\t(TOP(S(NP*)\\tO\\nhave\\tVBP\\t(VP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nsituation\\tNN\\t*)\\tO\\nwhere\\tWRB\\t(SBAR(WHADVP*)\\tO\\ninstead\\tRB\\t(S(PP*\\tO\\nof\\tIN\\t*\\tO\\nprotecting\\tVBG\\t(S(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\ninterests\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nJewish\\tJJ\\t*\\tB-NORP\\npeople\\tNNS\\t*))))))\\tO\\n,\\t,\\t*\\tO\\nthe\\tDT\\t(NP*\\tO\\nIsraeli\\tJJ\\t*\\tB-NORP\\nPrime\\tNNP\\t*\\tO\\nMinister\\tNNP\\t*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nwilling\\tJJ\\t(ADJP*\\tO\\nto\\tTO\\t(S(VP*\\tO\\nforgo\\tVB\\t(VP*\\tO\\non\\tIN\\t(PP*\\tO\\nall\\tDT\\t(NP(NP(NP*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nvital\\tJJ\\t(ADJP*\\tO\\nand\\tCC\\t*\\tO\\nimportant\\tJJ\\t*)\\tO\\nareas\\tNNS\\t*)))\\tO\\nand\\tCC\\t*\\tO\\nnot\\tRB\\t(NP(CONJP*\\tO\\nonly\\tRB\\t*)\\tO\\nour\\tPRP$\\t(NP*\\tO\\nhistory\\tNN\\t*)\\tO\\n,\\t,\\t*\\tO\\nbut\\tCC\\t*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nsecurity\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nfuture\\tNN\\t*))\\tO\\nfor\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nJewish\\tJJ\\t*\\tB-NORP\\npeople\\tNNS\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\nIsrael\\tNNP\\t(NP*)))))))))))))))))\\tB-GPE\\n.\\t.\\t*))\\tO',\n",
       " 'More\\tJJR\\t(TOP(S(NP(QP*\\tB-CARDINAL\\nthan\\tIN\\t*\\tI-CARDINAL\\n300\\tCD\\t*)\\tI-CARDINAL\\npeople\\tNNS\\t*)\\tO\\nhave\\tVBP\\t(VP*\\tO\\nbeen\\tVBN\\t(VP*\\tO\\nkilled\\tVBN\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nWest\\tNNP\\t(NML(NML*\\tB-GPE\\nBank\\tNNP\\t*)\\tI-GPE\\nand\\tCC\\t*\\tO\\nGaza\\tNNP\\t(NML*\\tB-GPE\\nStrip\\tNNP\\t*))))\\tI-GPE\\nsince\\tIN\\t(PP*\\tO\\nSeptember\\tNNP\\t(NP*)))))\\tB-DATE\\n.\\t.\\t*))\\tO']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5e15110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7901892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3cd1913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/development/onto.mz.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4371637",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66a49195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/development/onto.nw.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7d7c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d997328",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/development/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc40ca6",
   "metadata": {},
   "source": [
    "### Create test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b203802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training filJe with all the news corpus\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.bn.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "556a0e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c848103b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Iraqi\\tJJ\\t(TOP(S(NP(NML*\\tB-NORP\\nleader\\tNN\\t*)\\tO\\nSaddam\\tNNP\\t*\\tB-PERSON\\nHussein\\tNNP\\t*)\\tI-PERSON\\nhas\\tVBZ\\t(VP*\\tO\\ngiven\\tVBN\\t(VP*\\tO\\na\\tDT\\t(NP*\\tO\\ndefiant\\tJJ\\t*\\tO\\nspeech\\tNN\\t*)\\tO\\nto\\tTO\\t(S(VP*\\tO\\nmark\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\ntenth\\tJJ\\t*\\tB-ORDINAL\\nanniversary\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tB-EVENT\\nGulf\\tNNP\\t*\\tI-EVENT\\nWar\\tNNP\\t*))))))))\\tI-EVENT\\n.\\t.\\t*))\\tO',\n",
       " 'He\\tPRP\\t(TOP(S(NP*)\\tO\\nsays\\tVBZ\\t(VP*\\tO\\nIraq\\tNNP\\t(SBAR(S(NP*)\\tB-GPE\\nhas\\tVBZ\\t(VP*\\tO\\ntriumphed\\tVBN\\t(VP*\\tO\\nover\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nevil\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nWest\\tNNP\\t*)))))))))\\tB-LOC\\n.\\t.\\t*))\\tO',\n",
       " 'Barbara\\tNNP\\t(TOP(S(NP*\\tB-PERSON\\nPlett\\tNNP\\t*)\\tI-PERSON\\nreports\\tVBZ\\t(VP*\\tO\\nfrom\\tIN\\t(PP*\\tO\\nBaghdad\\tNNP\\t(NP*)))\\tB-GPE\\n.\\t.\\t*))\\tO',\n",
       " 'Saddam\\tNNP\\t(TOP(S(NP*\\tB-PERSON\\nHussein\\tNNP\\t*)\\tI-PERSON\\naddressed\\tVBD\\t(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nnation\\tNN\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nspeech\\tNN\\t*)\\tO\\nfilled\\tVBN\\t(VP*\\tO\\nwith\\tIN\\t(PP*\\tO\\nrhetoric\\tNN\\t(NP(NP*)\\tO\\nand\\tCC\\t*\\tO\\na\\tDT\\t(NP(NP*\\tO\\ndeclaration\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nvictory\\tNN\\t(NP*)))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " \"``\\t``\\t(TOP(S*\\tO\\nIraq\\tNNP\\t(S(NP*)\\tB-GPE\\nhas\\tVBZ\\t(VP*\\tO\\ntriumphed\\tVBN\\t(VP*\\tO\\nover\\tIN\\t(PP*\\tO\\nits\\tPRP$\\t(NP*\\tO\\nenemies\\tNNS\\t*)))))\\tO\\n,\\t,\\t*\\tO\\n''\\t''\\t*\\tO\\nhe\\tPRP\\t(PRN(S(NP*)\\tO\\nsaid\\tVBD\\t(VP*)))\\tO\\n,\\t,\\t*\\tO\\n``\\t``\\t*\\tO\\nand\\tCC\\t*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\nwill\\tMD\\t(VP*\\tO\\ntriumph\\tVB\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nremaining\\tVBG\\t*\\tO\\nrounds\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nbattle\\tNN\\t*)))))))\\tO\\n.\\t.\\t*\\tO\\n''\\t''\\t*))\\tO\",\n",
       " 'The\\tDT\\t(TOP(S(NP*\\tO\\nIraqi\\tJJ\\t*\\tB-NORP\\nleader\\tNN\\t*)\\tO\\nsaid\\tVBD\\t(VP*\\tO\\nthe\\tDT\\t(SBAR(S(NP*\\tB-EVENT\\nGulf\\tNNP\\t*\\tI-EVENT\\nWar\\tNNP\\t*)\\tI-EVENT\\nwas\\tVBD\\t(VP*\\tO\\na\\tDT\\t(NP(NP(NP*\\tO\\nconfrontation\\tNN\\t*)\\tO\\nbetween\\tIN\\t(PP*\\tO\\ngood\\tNN\\t(NP*\\tO\\nand\\tCC\\t*\\tO\\nevil\\tNN\\t*)))\\tO\\nthat\\tWDT\\t(SBAR(WHNP*)\\tO\\ncontinues\\tVBZ\\t(S(VP*\\tO\\nto\\tIN\\t(PP*\\tO\\nthis\\tDT\\t(NP*\\tO\\nday\\tNN\\t*))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'He\\tPRP\\t(TOP(S(NP*)\\tO\\nwas\\tVBD\\t(VP*\\tO\\nreferring\\tVBG\\t(VP*\\tO\\nto\\tIN\\t(PP*\\tO\\nhis\\tPRP$\\t(NP(NP*\\tO\\nstruggle\\tNN\\t*)\\tO\\nagainst\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nUS\\tNNP\\t*)\\tB-GPE\\n,\\t,\\t*\\tO\\nwhich\\tWDT\\t(SBAR(WHNP*)\\tO\\nstrongly\\tRB\\t(S(ADVP*)\\tO\\nsupports\\tVBZ\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\neconomic\\tJJ\\t*\\tO\\nembargo\\tNN\\t*)\\tO\\nmeant\\tVBN\\t(VP*\\tO\\nto\\tTO\\t(S(VP*\\tO\\nforce\\tVB\\t(VP*\\tO\\nhim\\tPRP\\t(NP*)\\tO\\nto\\tTO\\t(S(VP*\\tO\\ndisarm\\tVB\\t(VP*)))))))))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'After\\tIN\\t(TOP(S(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nspeech\\tNN\\t*))\\tO\\ndemonstrators\\tNNS\\t(NP*)\\tO\\ngathered\\tVBD\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\npublic\\tJJ\\t*\\tO\\nshow\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nsupport\\tNN\\t(NP(NP*)\\tO\\nfor\\tIN\\t(PP*\\tO\\ntheir\\tPRP$\\t(NP*\\tO\\nPresident\\tNNP\\t*)))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'A\\tDT\\t(TOP(S(NP(NP*\\tO\\nsense\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nnationalism\\tNN\\t(NP*)))\\tO\\nhas\\tVBZ\\t(VP*\\tO\\ngrown\\tVBN\\t(VP*\\tO\\nstronger\\tJJR\\t(ADJP*)\\tO\\nhere\\tRB\\t(ADVP*)\\tO\\nas\\tIN\\t(SBAR*\\tO\\nmany\\tJJ\\t(S(NP*\\tO\\npeople\\tNNS\\t*)\\tO\\nrally\\tVBP\\t(VP*\\tO\\nagainst\\tIN\\t(PP*\\tO\\nwhat\\tWP\\t(SBAR(WHNP*)\\tO\\nthey\\tPRP\\t(S(NP*)\\tO\\nsay\\tVBP\\t(VP*\\tO\\nis\\tVBZ\\t(SBAR(S(VP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nsiege\\tNN\\t*)\\tO\\non\\tIN\\t(PP*\\tO\\ntheir\\tPRP$\\t(NP*\\tO\\ncountry\\tNN\\t*)))))))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'Barbara\\tNNP\\t(TOP(FRAG(NP*\\tB-PERSON\\nPlett\\tNNP\\t*)\\tI-PERSON\\n,\\t,\\t*\\tO\\nBBC\\tNNP\\t(NP*\\tB-ORG\\nNews\\tNNP\\t*)\\tI-ORG\\n,\\t,\\t*\\tO\\nBaghdad\\tNNP\\t(NP*)\\tB-GPE\\n.\\t.\\t*))\\tO']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d115bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09dadc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "586b9172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.mz.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30c753da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33682301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.nw.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "711d912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ebb90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da1bf3a",
   "metadata": {},
   "source": [
    "### import data in CONLL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3174f407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|This is The World...|[{document, 0, 88...|[{document, 0, 88...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "| I am Lisa Mullins .|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|A plan was announ...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 0, A,...|[{pos, 0, 0, DT, ...|[{named_entity, 0...|\n",
      "|The Kursk Foundat...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The BBC 's James ...|[{document, 0, 66...|[{document, 0, 66...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|He says there are...|[{document, 0, 57...|[{document, 0, 57...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|Obviously the fir...|[{document, 0, 61...|[{document, 0, 61...|[{token, 0, 8, Ob...|[{pos, 0, 8, RB, ...|[{named_entity, 0...|\n",
      "|There are still t...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 4, Th...|[{pos, 0, 4, EX, ...|[{named_entity, 0...|\n",
      "|Secondly , there ...|[{document, 0, 79...|[{document, 0, 79...|[{token, 0, 7, Se...|[{pos, 0, 7, RB, ...|[{named_entity, 0...|\n",
      "|The Kursk Foundat...|[{document, 0, 30...|[{document, 0, 30...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|That 's the dange...|[{document, 0, 44...|[{document, 0, 44...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|I would think the...|[{document, 0, 56...|[{document, 0, 56...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|I mean this is a ...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|     That 's right .|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|The size of these...|[{document, 0, 51...|[{document, 0, 51...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|In fact one of th...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|Now , it 's lying...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 2, No...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|It is a huge subm...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|There are any num...|[{document, 0, 19...|[{document, 0, 19...|[{token, 0, 4, Th...|[{pos, 0, 4, EX, ...|[{named_entity, 0...|\n",
      "|Second problem , ...|[{document, 0, 77...|[{document, 0, 77...|[{token, 0, 5, Se...|[{pos, 0, 5, JJ, ...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing the training set\n",
    "from sparknlp.training import CoNLL\n",
    "\n",
    "training_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/train/sample.train')\n",
    "training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80fc3b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|The Israeli Prime...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The encounter is ...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|They require Isra...|[{document, 0, 26...|[{document, 0, 26...|[{token, 0, 3, Th...|[{pos, 0, 3, PRP,...|[{named_entity, 0...|\n",
      "|We have so many q...|[{document, 0, 84...|[{document, 0, 84...|[{token, 0, 1, We...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|The most importan...|[{document, 0, 40...|[{document, 0, 40...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The most importan...|[{document, 0, 79...|[{document, 0, 79...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Israeli settlers ...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 6, Is...|[{pos, 0, 6, JJ, ...|[{named_entity, 0...|\n",
      "|The spokeswoman f...|[{document, 0, 83...|[{document, 0, 83...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|We have a situati...|[{document, 0, 26...|[{document, 0, 26...|[{token, 0, 1, We...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|More than 300 peo...|[{document, 0, 86...|[{document, 0, 86...|[{token, 0, 3, Mo...|[{pos, 0, 3, JJR,...|[{named_entity, 0...|\n",
      "|The head of the P...|[{document, 0, 86...|[{document, 0, 86...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Masked gunmen ope...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 5, Ma...|[{pos, 0, 5, JJ, ...|[{named_entity, 0...|\n",
      "|You 're listening...|[{document, 0, 30...|[{document, 0, 30...|[{token, 0, 2, Yo...|[{pos, 0, 2, PRP,...|[{named_entity, 0...|\n",
      "|This is the World...|[{document, 0, 36...|[{document, 0, 36...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|Well we ca n't te...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 3, We...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|First up we have ...|[{document, 0, 50...|[{document, 0, 50...|[{token, 0, 4, Fi...|[{pos, 0, 4, RB, ...|[{named_entity, 0...|\n",
      "|We are looking fo...|[{document, 0, 72...|[{document, 0, 72...|[{token, 0, 1, We...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|The area is mostl...|[{document, 0, 90...|[{document, 0, 90...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|It also includes ...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|The answer is Abr...|[{document, 0, 22...|[{document, 0, 22...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/development/sample.train')\n",
    "dev_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7e4b286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Iraqi leader Sadd...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 4, Ir...|[{pos, 0, 4, JJ, ...|[{named_entity, 0...|\n",
      "|He says Iraq has ...|[{document, 0, 53...|[{document, 0, 53...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|Barbara Plett rep...|[{document, 0, 35...|[{document, 0, 35...|[{token, 0, 6, Ba...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
      "|Saddam Hussein ad...|[{document, 0, 98...|[{document, 0, 98...|[{token, 0, 5, Sa...|[{pos, 0, 5, NNP,...|[{named_entity, 0...|\n",
      "|`` Iraq has trium...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 1, ``...|[{pos, 0, 1, ``, ...|[{named_entity, 0...|\n",
      "|The Iraqi leader ...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|He was referring ...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|After the speech ...|[{document, 0, 88...|[{document, 0, 88...|[{token, 0, 4, Af...|[{pos, 0, 4, IN, ...|[{named_entity, 0...|\n",
      "|A sense of nation...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 0, A,...|[{pos, 0, 0, DT, ...|[{named_entity, 0...|\n",
      "|Barbara Plett , B...|[{document, 0, 35...|[{document, 0, 35...|[{token, 0, 6, Ba...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
      "| This is The World .|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|    I am Tony Kahn .|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|Gao Xingjian arri...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 2, Ga...|[{pos, 0, 2, NNP,...|[{named_entity, 0...|\n",
      "|Today Gao , who n...|[{document, 0, 17...|[{document, 0, 17...|[{token, 0, 4, To...|[{pos, 0, 4, NN, ...|[{named_entity, 0...|\n",
      "|Gao 's 1989 novel...|[{document, 0, 84...|[{document, 0, 84...|[{token, 0, 2, Ga...|[{pos, 0, 2, NNP,...|[{named_entity, 0...|\n",
      "|If you want to kn...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 1, If...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|His politics are ...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, Hi...|[{pos, 0, 2, PRP$...|[{named_entity, 0...|\n",
      "|This novel ` soul...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|It 's a large nov...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|It is , if you li...|[{document, 0, 62...|[{document, 0, 62...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e54ee7",
   "metadata": {},
   "source": [
    "### Training the model - 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5203e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base_cased download started this may take some time.\n",
      "Approximate size to download 389.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# use bert embeddings\n",
    "bert = BertEmbeddings.pretrained('bert_base_cased', 'en').setInputCols([\"sentence\",'token']).setOutputCol(\"bert\").setCaseSensitive(True)#.setMaxSentenceLength(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d0c34ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the training data into embeddings and saving it as parquet files\n",
    "readyTrainingData = bert.transform(training_data)\n",
    "\n",
    "readyTrainingData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab96caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "readyTrainingData = spark.read.parquet(\"/tmp/conll2003/bert_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c4f2096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the development data into embeddings and saving it as parquet files\n",
    "readyDevData = bert.transform(dev_data)\n",
    "\n",
    "readyDevData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0c284d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "readyDevData = spark.read.parquet(\"/tmp/conll2003/bert_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d64dac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa34583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "readyTestData = spark.read.parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0897336",
   "metadata": {},
   "source": [
    "### Dev set - news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7bd2f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize NER tagger\n",
    "nerTagger = NerDLApproach()\\\n",
    ".setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
    ".setLabelColumn(\"label\")\\\n",
    ".setOutputCol(\"ner\")\\\n",
    ".setMaxEpochs(10)\\\n",
    ".setBatchSize(4)\\\n",
    ".setEnableMemoryOptimizer(True)\\\n",
    ".setRandomSeed(0)\\\n",
    ".setVerbose(1)\\\n",
    ".setValidationSplit(0.2)\\\n",
    ".setEvaluationLogExtended(True)\\\n",
    ".setEnableOutputLogs(True)\\\n",
    ".setIncludeConfidence(True)\\\n",
    ".setTestDataset(\"/tmp/conll2003/bert_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68c9418e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 636 ms, sys: 369 ms, total: 1.01 s\n",
      "Wall time: 2h 3min 54s\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "%time myNerModel = nerTagger.fit(readyTrainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327a58c4",
   "metadata": {},
   "source": [
    "### Testing on News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "822a465e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.35 s, sys: 458 ms, total: 2.8 s\n",
      "Wall time: 59.9 s\n"
     ]
    }
   ],
   "source": [
    "# infer from the trained model\n",
    "%time results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8c24e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c50bb9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[146, 334]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5da5e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4a198acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "275e30d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9772023141927532\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "445fbe5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.894753105494098\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cdac3a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.85      0.88      0.87       616\n",
      "        DATE       0.84      0.90      0.87      1252\n",
      "       EVENT       0.63      0.51      0.57        37\n",
      "         FAC       0.64      0.45      0.53        66\n",
      "         GPE       0.96      0.95      0.95      1664\n",
      "    LANGUAGE       0.83      0.50      0.62        10\n",
      "         LAW       0.69      0.56      0.62        36\n",
      "         LOC       0.89      0.75      0.82       144\n",
      "       MONEY       0.88      0.89      0.89       279\n",
      "        NORP       0.96      0.95      0.95       579\n",
      "     ORDINAL       0.81      0.94      0.87       118\n",
      "         ORG       0.87      0.91      0.89      1494\n",
      "     PERCENT       0.91      0.90      0.90       293\n",
      "      PERSON       0.94      0.95      0.95      1099\n",
      "     PRODUCT       0.75      0.73      0.74        71\n",
      "    QUANTITY       0.67      0.68      0.67        59\n",
      "        TIME       0.63      0.59      0.61       106\n",
      " WORK_OF_ART       0.71      0.67      0.69       111\n",
      "\n",
      "   micro avg       0.89      0.90      0.89      8034\n",
      "   macro avg       0.80      0.76      0.78      8034\n",
      "weighted avg       0.89      0.90      0.89      8034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9202cdb8",
   "metadata": {},
   "source": [
    "### Testing on News Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e8a88d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.bc.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ae1562fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b6103b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "15469afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|-- basically , it...|[{document, 0, 78...|[{document, 0, 78...|[{token, 0, 1, --...|[{pos, 0, 1, :, {...|[{named_entity, 0...|\n",
      "|To express its de...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 1, To...|[{pos, 0, 1, TO, ...|[{named_entity, 0...|\n",
      "|It takes time to ...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|Dear viewers , th...|[{document, 0, 52...|[{document, 0, 52...|[{token, 0, 3, De...|[{pos, 0, 3, NNP,...|[{named_entity, 0...|\n",
      "|     This is Xu Li .|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|Thank you everyon...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 4, Th...|[{pos, 0, 4, VBP,...|[{named_entity, 0...|\n",
      "|Coming up is the ...|[{document, 0, 59...|[{document, 0, 59...|[{token, 0, 5, Co...|[{pos, 0, 5, VBG,...|[{named_entity, 0...|\n",
      "|Good-bye , dear v...|[{document, 0, 24...|[{document, 0, 24...|[{token, 0, 7, Go...|[{pos, 0, 7, UH, ...|[{named_entity, 0...|\n",
      "|Hello , dear view...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 4, He...|[{pos, 0, 4, UH, ...|[{named_entity, 0...|\n",
      "|Welcome to Focus ...|[{document, 0, 23...|[{document, 0, 23...|[{token, 0, 6, We...|[{pos, 0, 6, VBP,...|[{named_entity, 0...|\n",
      "|Today , let 's tu...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 4, To...|[{pos, 0, 4, NN, ...|[{named_entity, 0...|\n",
      "|Before dawn on Ja...|[{document, 0, 19...|[{document, 0, 19...|[{token, 0, 5, Be...|[{pos, 0, 5, IN, ...|[{named_entity, 0...|\n",
      "|Relevant departme...|[{document, 0, 94...|[{document, 0, 94...|[{token, 0, 7, Re...|[{pos, 0, 7, JJ, ...|[{named_entity, 0...|\n",
      "|The traffic admin...|[{document, 0, 94...|[{document, 0, 94...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Well , how did th...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 3, We...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|After the holiday...|[{document, 0, 72...|[{document, 0, 72...|[{token, 0, 4, Af...|[{pos, 0, 4, IN, ...|[{named_entity, 0...|\n",
      "|In addition , wha...|[{document, 0, 19...|[{document, 0, 19...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|Well , we have in...|[{document, 0, 93...|[{document, 0, 93...|[{token, 0, 3, We...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|One of the two ho...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 2, On...|[{pos, 0, 2, CD, ...|[{named_entity, 0...|\n",
      "|             Hello .|[{document, 0, 6,...|[{document, 0, 6,...|[{token, 0, 4, He...|[{pos, 0, 4, UH, ...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "71c67d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf52b8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.61 s, sys: 1.16 s, total: 2.76 s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "# infer from the trained model\n",
    "%time results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "985cb059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "af4130a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "25428c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fca4070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "23eb70dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9764836247229747\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "27452eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7878430215402774\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f39a1c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.69      0.71      0.70       182\n",
      "        DATE       0.70      0.76      0.73       200\n",
      "       EVENT       0.50      0.21      0.30        14\n",
      "         FAC       0.95      0.85      0.90        48\n",
      "         GPE       0.90      0.92      0.91       353\n",
      "         LAW       1.00      0.00      0.00         3\n",
      "         LOC       0.71      0.38      0.50        26\n",
      "       MONEY       0.50      1.00      0.67         3\n",
      "        NORP       0.63      0.65      0.64       138\n",
      "     ORDINAL       0.88      0.86      0.87        50\n",
      "         ORG       0.74      0.86      0.80       153\n",
      "     PERCENT       1.00      1.00      1.00        14\n",
      "      PERSON       0.92      0.90      0.91       382\n",
      "    QUANTITY       0.26      0.20      0.23        40\n",
      "        TIME       0.68      0.57      0.62        63\n",
      " WORK_OF_ART       0.30      0.21      0.25        28\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      1697\n",
      "   macro avg       0.71      0.63      0.63      1697\n",
      "weighted avg       0.79      0.79      0.78      1697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929acf71",
   "metadata": {},
   "source": [
    "### Testing on WebLogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0597d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.wb.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bbd25fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6524551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6c942177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|The success of al...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The Source of the...|[{document, 0, 23...|[{document, 0, 23...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The al - Jazeera ...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|In this film the ...|[{document, 0, 73...|[{document, 0, 73...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|The Hebrew channe...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|This is a unique ...|[{document, 0, 45...|[{document, 0, 45...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|There are many qu...|[{document, 0, 20...|[{document, 0, 20...|[{token, 0, 4, Th...|[{pos, 0, 4, EX, ...|[{named_entity, 0...|\n",
      "|I leave you with ...|[{document, 0, 72...|[{document, 0, 72...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|the link to the f...|[{document, 0, 22...|[{document, 0, 22...|[{token, 0, 2, th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|http://z08.zuploa...|[{document, 0, 52...|[{document, 0, 52...|[{token, 0, 52, h...|[{pos, 0, 52, ADD...|[{named_entity, 0...|\n",
      "|The Islam Diary :...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Speaking at the s...|[{document, 0, 44...|[{document, 0, 44...|[{token, 0, 7, Sp...|[{pos, 0, 7, VBG,...|[{named_entity, 0...|\n",
      "|He also told the ...|[{document, 0, 34...|[{document, 0, 34...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|The newspaper quo...|[{document, 0, 33...|[{document, 0, 33...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The newspaper des...|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|He speaks in the ...|[{document, 0, 93...|[{document, 0, 93...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|He was quoted as ...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|It pointed out th...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|This means that t...|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|\" Fadil \" was quo...|[{document, 0, 22...|[{document, 0, 22...|[{token, 0, 0, \",...|[{pos, 0, 0, ``, ...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3e3adc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bee7150b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 392 ms, sys: 145 ms, total: 538 ms\n",
      "Wall time: 32.6 s\n"
     ]
    }
   ],
   "source": [
    "# infer from the trained model\n",
    "%time results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8cb952ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ff92a64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "572ca05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "485a5704",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1e8cdd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9634732119292689\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "96bea8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7562879444926279\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6f84964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.84      0.79      0.81        85\n",
      "        DATE       0.52      0.68      0.59        74\n",
      "       EVENT       0.62      0.42      0.50        12\n",
      "         FAC       0.57      0.22      0.32        18\n",
      "         GPE       0.90      0.94      0.92       173\n",
      "    LANGUAGE       1.00      0.25      0.40         4\n",
      "         LAW       0.50      1.00      0.67         1\n",
      "         LOC       0.67      0.44      0.53         9\n",
      "       MONEY       0.81      0.84      0.82        25\n",
      "        NORP       0.90      0.86      0.88       107\n",
      "     ORDINAL       0.81      0.72      0.76        18\n",
      "         ORG       0.55      0.83      0.66       117\n",
      "     PERCENT       0.61      0.61      0.61        33\n",
      "      PERSON       0.86      0.76      0.81       407\n",
      "     PRODUCT       0.00      0.00      0.00         1\n",
      "    QUANTITY       0.33      0.33      0.33         6\n",
      "        TIME       0.52      0.60      0.56        20\n",
      " WORK_OF_ART       0.26      0.37      0.31        27\n",
      "\n",
      "   micro avg       0.75      0.77      0.76      1137\n",
      "   macro avg       0.63      0.59      0.58      1137\n",
      "weighted avg       0.77      0.77      0.76      1137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629decfc",
   "metadata": {},
   "source": [
    "### Testing on Telephonic Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4f401797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.tc.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6b826c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f7b8fe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "74968763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|But %um , guessed...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 2, Bu...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|              What ?|[{document, 0, 5,...|[{document, 0, 5,...|[{token, 0, 3, Wh...|[{pos, 0, 3, WP, ...|[{named_entity, 0...|\n",
      "|         %um The %um|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, %u...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|             Again ?|[{document, 0, 6,...|[{document, 0, 6,...|[{token, 0, 4, Ag...|[{pos, 0, 4, RB, ...|[{named_entity, 0...|\n",
      "|%um 118.91_120.82...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 2, %u...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|       two weeks ago|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 2, tw...|[{pos, 0, 2, CD, ...|[{named_entity, 0...|\n",
      "|I know Sue is goi...|[{document, 0, 36...|[{document, 0, 36...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|         %eh Right .|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, %e...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|             Right .|[{document, 0, 6,...|[{document, 0, 6,...|[{token, 0, 4, Ri...|[{pos, 0, 4, UH, ...|[{named_entity, 0...|\n",
      "|Did she see her m...|[{document, 0, 20...|[{document, 0, 20...|[{token, 0, 2, Di...|[{pos, 0, 2, VBD,...|[{named_entity, 0...|\n",
      "|       Yes she did .|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 2, Ye...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|               And ?|[{document, 0, 4,...|[{document, 0, 4,...|[{token, 0, 2, An...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|       Yes she did .|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 2, Ye...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|               And ?|[{document, 0, 4,...|[{document, 0, 4,...|[{token, 0, 2, An...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|%um , %um , which...|[{document, 0, 73...|[{document, 0, 73...|[{token, 0, 2, %u...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|I spoke to her mo...|[{document, 0, 29...|[{document, 0, 29...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "| %um and I asked her|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 2, %u...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|       I said listen|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|I said %um please...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|things have come ...|[{document, 0, 78...|[{document, 0, 78...|[{token, 0, 5, th...|[{pos, 0, 5, NNS,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cf3efcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eabb88c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 346 ms, sys: 98 ms, total: 444 ms\n",
      "Wall time: 31.2 s\n"
     ]
    }
   ],
   "source": [
    "# infer from the trained model\n",
    "%time results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "75b3e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a64ec236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ccb527cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "788c5188",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0d231fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9641946064139941\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c2cdc728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.630841121495327\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0475c395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.53      0.46      0.49        52\n",
      "        DATE       0.67      0.73      0.70        74\n",
      "         FAC       0.00      0.00      0.00         3\n",
      "         GPE       0.78      0.92      0.84        50\n",
      "    LANGUAGE       1.00      0.50      0.67         8\n",
      "         LAW       0.00      1.00      0.00         0\n",
      "       MONEY       1.00      0.71      0.83         7\n",
      "        NORP       0.81      1.00      0.89        17\n",
      "     ORDINAL       0.80      0.89      0.84         9\n",
      "         ORG       0.27      0.52      0.36        27\n",
      "     PERCENT       0.05      0.50      0.09         6\n",
      "      PERSON       0.74      0.86      0.80       100\n",
      "     PRODUCT       0.50      0.25      0.33         4\n",
      "    QUANTITY       0.00      1.00      0.00         0\n",
      "        TIME       0.62      0.35      0.44        23\n",
      " WORK_OF_ART       0.00      1.00      0.00         0\n",
      "\n",
      "   micro avg       0.57      0.71      0.63       380\n",
      "   macro avg       0.49      0.67      0.46       380\n",
      "weighted avg       0.66      0.71      0.67       380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a27f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
