{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1edef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  3.1.0\n",
      "Apache Spark version:  3.1.2\n"
     ]
    }
   ],
   "source": [
    "# call relevant packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "import sparknlp\n",
    "\n",
    "# to use GPU \n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb98346",
   "metadata": {},
   "source": [
    "## Creating a composite CoNLL file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4910c848",
   "metadata": {},
   "source": [
    "### Create training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "024205af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training filJe with all the news corpus\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/onto.bn.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85935111",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769deee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This\\tDT\\t(TOP(S(NP*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nThe\\tDT\\t(NP(NP*\\tB-ORG\\nWorld\\tNNP\\t*)\\tI-ORG\\n,\\t,\\t*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nco-production\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tB-ORG\\nBBC\\tNNP\\t*\\tI-ORG\\nWorld\\tNNP\\t*\\tI-ORG\\nService\\tNNP\\t*)\\tI-ORG\\n,\\t,\\t*\\tO\\nPRI\\tNNP\\t(NP*)\\tB-ORG\\n,\\t,\\t*\\tO\\nand\\tCC\\t*\\tO\\nWGBH\\tNNP\\t(NP(NP*)\\tB-ORG\\nin\\tIN\\t(PP*\\tO\\nBoston\\tNNP\\t(NP*))))))))\\tB-GPE\\n.\\t.\\t*))\\tO',\n",
       " 'I\\tPRP\\t(TOP(S(NP*)\\tO\\nam\\tVBP\\t(VP*\\tO\\nLisa\\tNNP\\t(NP*\\tB-PERSON\\nMullins\\tNNP\\t*))\\tI-PERSON\\n.\\t.\\t*))\\tO',\n",
       " \"A\\tDT\\t(TOP(S(NP*\\tO\\nplan\\tNN\\t*)\\tO\\nwas\\tVBD\\t(VP*\\tO\\nannounced\\tVBN\\t(VP*\\tO\\ntoday\\tNN\\t(NP*)\\tB-DATE\\nto\\tTO\\t(S(VP*\\tO\\nraise\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nRussian\\tJJ\\t*\\tB-NORP\\nnuclear\\tJJ\\t*\\tO\\nsubmarine\\tNN\\t*)\\tO\\n`\\t''\\t(NP*\\tO\\nKursk\\tNNP\\t*\\tB-PRODUCT\\n'\\t''\\t*))\\tO\\n,\\t,\\t*\\tO\\nnext\\tJJ\\t(NP(NP*\\tB-DATE\\nsummer\\tNN\\t*)\\tI-DATE\\n,\\t,\\t*\\tO\\nalmost\\tRB\\t(SBAR(NP(QP*\\tB-DATE\\na\\tDT\\t*)\\tI-DATE\\nyear\\tNN\\t*)\\tI-DATE\\nafter\\tIN\\t*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\nhit\\tVBD\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nbottom\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tB-LOC\\nBarents\\tNNP\\t*\\tI-LOC\\nSea\\tNNP\\t*)))\\tI-LOC\\nfollowing\\tVBG\\t(PP*\\tO\\na\\tDT\\t(NP*\\tO\\nmysterious\\tJJ\\t*\\tO\\naccident\\tNN\\t*)))))))))))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'The\\tDT\\t(TOP(S(NP(NP*\\tB-ORG\\nKursk\\tNNP\\t*\\tI-ORG\\nFoundation\\tNNP\\t*)\\tI-ORG\\n,\\t,\\t*\\tO\\nan\\tDT\\t(NP(NP*\\tO\\ninternational\\tJJ\\t*\\tO\\nconsortium\\tNN\\t*)\\tO\\nled\\tVBN\\t(VP*\\tO\\nby\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\ngovernments\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nRussia\\tNNP\\t(NP(NP*)\\tB-GPE\\nand\\tCC\\t*\\tO\\nThe\\tDT\\t(NP*\\tB-GPE\\nNetherlands\\tNNP\\t*))))))))\\tI-GPE\\nsays\\tVBZ\\t(VP*\\tO\\nit\\tPRP\\t(SBAR(S(NP*)\\tO\\ncan\\tMD\\t(VP*\\tO\\ndo\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\njob\\tNN\\t*)\\tO\\nsafely\\tRB\\t(ADVP*)\\tO\\nif\\tIN\\t(SBAR*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\ncan\\tMD\\t(VP*\\tO\\nsecure\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nfunding\\tNN\\t*))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " \"The\\tDT\\t(TOP(S(NP(NP*\\tO\\nBBC\\tNNP\\t*\\tB-ORG\\n's\\tPOS\\t*)\\tO\\nJames\\tNNP\\t*\\tB-PERSON\\nRogers\\tNNP\\t*)\\tI-PERSON\\nwas\\tVBD\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nBrussels\\tNNP\\t(NP*))\\tB-GPE\\nfor\\tIN\\t(PP*\\tO\\ntoday\\tNN\\t(NP(NP*\\tB-DATE\\n's\\tPOS\\t*)\\tO\\nannouncement\\tNN\\t*)))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'He\\tPRP\\t(TOP(S(NP*)\\tO\\nsays\\tVBZ\\t(VP*\\tO\\nthere\\tEX\\t(SBAR(S(NP*)\\tO\\nare\\tVBP\\t(VP*\\tO\\ntwo\\tCD\\t(NP(NP*\\tB-CARDINAL\\nmain\\tJJ\\t*\\tO\\nreasons\\tNNS\\t*)\\tO\\nfor\\tIN\\t(PP*\\tO\\nraising\\tVBG\\t(S(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nKursk\\tNNP\\t*)))))))))\\tB-PRODUCT\\n.\\t.\\t*))\\tO',\n",
       " 'Obviously\\tRB\\t(TOP(S(ADVP*)\\tO\\nthe\\tDT\\t(NP*\\tO\\nfirst\\tJJ\\t*\\tB-ORDINAL\\none\\tNN\\t*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nto\\tTO\\t(S(VP*\\tO\\nrecover\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nbodies\\tNNS\\t*)\\tO\\nthat\\tWDT\\t(SBAR(WHNP*)\\tO\\nremain\\tVBP\\t(S(VP*))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'There\\tEX\\t(TOP(S(NP*)\\tO\\nare\\tVBP\\t(VP*\\tO\\nstill\\tRB\\t(ADVP*)\\tO\\nthe\\tDT\\t(NP(NP(NP*\\tO\\nbodies\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nmore\\tJJR\\t(NP(QP*\\tB-CARDINAL\\nthan\\tIN\\t*\\tI-CARDINAL\\n100\\tCD\\t*)\\tI-CARDINAL\\nsailors\\tNNS\\t*)))\\tO\\ntrapped\\tVBN\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nvessel\\tNN\\t*))\\tO\\nsince\\tIN\\t(SBAR*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\nsank\\tVBD\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tB-LOC\\nBarents\\tNNP\\t*\\tI-LOC\\nSea\\tNNP\\t*))\\tI-LOC\\nin\\tIN\\t(PP*\\tO\\nAugust\\tNNP\\t(NP(NP*)\\tB-DATE\\nof\\tIN\\t(PP*\\tI-DATE\\nlast\\tJJ\\t(NP*\\tI-DATE\\nyear\\tNN\\t*))))))))))\\tI-DATE\\n.\\t.\\t*))\\tO',\n",
       " \"Secondly\\tRB\\t(TOP(S(ADVP*)\\tB-ORDINAL\\n,\\t,\\t*\\tO\\nthere\\tEX\\t(S(NP*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\na\\tDT\\t(NP*\\tO\\nlonger\\tJJR\\t(NML*\\tO\\n-\\tHYPH\\t*\\tO\\nterm\\tNN\\t*)\\tO\\nthreat\\tNN\\t*)))\\tO\\nand\\tCC\\t*\\tO\\nthat\\tDT\\t(S(NP*)\\tO\\n's\\tVBZ\\t(VP*\\tO\\nan\\tDT\\t(NP*\\tO\\nenvironmental\\tJJ\\t*\\tO\\nthreat\\tNN\\t*)))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'The\\tDT\\t(TOP(S(NP*\\tB-ORG\\nKursk\\tNNP\\t*\\tI-ORG\\nFoundation\\tNNP\\t*)\\tI-ORG\\nsays\\tVBZ\\t(VP*\\tO\\nthat\\tIN\\t(SBAR(SBAR*\\tO\\nthe\\tDT\\t(S(NP(NP*\\tO\\nnuclear\\tJJ\\t*\\tO\\nreactors\\tNNS\\t*)\\tO\\n,\\t,\\t*\\tO\\nthere\\tEX\\t(PRN(S(NP*)\\tO\\nare\\tVBP\\t(VP*\\tO\\ntwo\\tCD\\t(NP(NP*)\\tB-CARDINAL\\non\\tIN\\t(PP*\\tO\\nboard\\tNN\\t(NP*)))))))\\tO\\n,\\t,\\t*\\tO\\nare\\tVBP\\t(VP*\\tO\\nsafe\\tJJ\\t(ADJP*\\tO\\nfor\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\ntime\\tNN\\t*\\tO\\nbeing\\tNN\\t*))))))\\tO\\nbut\\tCC\\t*\\tO\\nthat\\tIN\\t(SBAR*\\tO\\nthe\\tDT\\t(S(NP(NP*\\tO\\ncorrosive\\tJJ\\t*\\tO\\neffects\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nseawater\\tNN\\t(NP*)))\\tO\\ncould\\tMD\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\ntime\\tNN\\t(NP*))\\tO\\nlead\\tVB\\t(VP*\\tO\\nto\\tIN\\t(PP*\\tO\\nradioactive\\tJJ\\t(NP(NP(NP*\\tO\\nleaks\\tNNS\\t*)\\tO\\ninto\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nocean\\tNN\\t*)))\\tO\\n,\\t,\\t*\\tO\\nwhich\\tWDT\\t(SBAR(WHNP*)\\tO\\nobviously\\tRB\\t(S(ADVP*)\\tO\\ncould\\tMD\\t(VP*\\tO\\nhave\\tVB\\t(VP*\\tO\\ndevastating\\tJJ\\t(NP(NP*\\tO\\nconsequences\\tNNS\\t*)\\tO\\non\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nfishery\\tNN\\t*)\\tO\\nand\\tCC\\t*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nenvironment\\tNN\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\ngeneral\\tJJ\\t(ADJP*))))))))))))))))))\\tO\\n.\\t.\\t*))\\tO']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f341e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a5589da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41459c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/onto.mz.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84dedece",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f31fbf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/onto.nw.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5134538",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "053567a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding tc files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/onto.tc.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84579d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac2b9a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding wb files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/onto.wb.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d711f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41e0b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa11d3b4",
   "metadata": {},
   "source": [
    "### Create dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "488f800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training filJe with all the news corpus\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/development/onto.bc.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc5fc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3f11a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In\\tIN\\t(TOP(S(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tB-DATE\\nsummer\\tNN\\t*)\\tI-DATE\\nof\\tIN\\t(PP*\\tI-DATE\\n2005\\tCD\\t(NP*))))\\tI-DATE\\n,\\t,\\t*\\tO\\na\\tDT\\t(NP(NP*\\tO\\npicture\\tNN\\t*)\\tO\\nthat\\tWDT\\t(SBAR(WHNP*)\\tO\\npeople\\tNNS\\t(S(NP*)\\tO\\nhave\\tVBP\\t(VP*\\tO\\nlong\\tRB\\t(ADVP*)\\tO\\nbeen\\tVBN\\t(VP*\\tO\\nlooking\\tVBG\\t(VP*\\tO\\nforward\\tRB\\t(ADVP*\\tO\\nto\\tIN\\t(PP*))))))))\\tO\\nstarted\\tVBD\\t(VP*\\tO\\nemerging\\tVBG\\t(S(VP*\\tO\\nwith\\tIN\\t(PP*\\tO\\nfrequency\\tNN\\t(NP*))\\tO\\nin\\tIN\\t(PP*\\tO\\nvarious\\tJJ\\t(NP*\\tO\\nmajor\\tJJ\\t*\\tO\\nHong\\tNNP\\t(NML*\\tB-GPE\\nKong\\tNNP\\t*)\\tI-GPE\\nmedia\\tNNS\\t*)))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'With\\tIN\\t(TOP(S(PP*\\tO\\ntheir\\tPRP$\\t(NP*\\tO\\nunique\\tJJ\\t*\\tO\\ncharm\\tNN\\t*))\\tO\\n,\\t,\\t*\\tO\\nthese\\tDT\\t(NP*\\tO\\nwell\\tRB\\t(ADJP*\\tO\\n-\\tHYPH\\t*\\tO\\nknown\\tVBN\\t*)\\tO\\ncartoon\\tNN\\t*\\tO\\nimages\\tNNS\\t*)\\tO\\nonce\\tRB\\t(ADVP*\\tO\\nagain\\tRB\\t*)\\tO\\ncaused\\tVBD\\t(VP*\\tO\\nHong\\tNNP\\t(S(NP*\\tB-GPE\\nKong\\tNNP\\t*)\\tI-GPE\\nto\\tTO\\t(VP*\\tO\\nbe\\tVB\\t(VP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nfocus\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nworldwide\\tJJ\\t(NP*\\tO\\nattention\\tNN\\t*)))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " \"The\\tDT\\t(TOP(S(NP(NP*\\tO\\nworld\\tNN\\t*\\tO\\n's\\tPOS\\t*)\\tO\\nfifth\\tJJ\\t*\\tB-ORDINAL\\nDisney\\tNNP\\t*\\tB-ORG\\npark\\tNN\\t*)\\tO\\nwill\\tMD\\t(VP*\\tO\\nsoon\\tRB\\t(ADVP*)\\tO\\nopen\\tVB\\t(VP*\\tO\\nto\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\npublic\\tNN\\t*))\\tO\\nhere\\tRB\\t(ADVP*)))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'The\\tDT\\t(TOP(S(NP(NP*\\tO\\nmost\\tRBS\\t(ADJP*\\tO\\nimportant\\tJJ\\t*)\\tO\\nthing\\tNN\\t*)\\tO\\nabout\\tIN\\t(PP*\\tO\\nDisney\\tNNP\\t(NP*)))\\tB-ORG\\nis\\tVBZ\\t(VP*\\tO\\nthat\\tIN\\t(SBAR*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\na\\tDT\\t(NP*\\tO\\nglobal\\tJJ\\t*\\tO\\nbrand\\tNN\\t*)))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'Well\\tUH\\t(TOP(S(INTJ*)\\tO\\n,\\t,\\t*\\tO\\nfor\\tIN\\t(PP*\\tO\\nseveral\\tJJ\\t(NP*\\tB-DATE\\nyears\\tNNS\\t*))\\tI-DATE\\n,\\t,\\t*\\tO\\nalthough\\tIN\\t(SBAR*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\nwas\\tVBD\\t(VP*\\tO\\nstill\\tRB\\t(ADVP*)\\tO\\nunder\\tIN\\t(UCP(PP*\\tO\\nconstruction\\tNN\\t(NP*))\\tO\\nand\\tCC\\t*\\tO\\n,\\t,\\t*\\tO\\ner\\tUH\\t(INTJ*)\\tO\\n,\\t,\\t*\\tO\\nnot\\tRB\\t(ADJP(ADVP*\\tO\\nyet\\tRB\\t*)\\tO\\nopen\\tJJ\\t*)))))\\tO\\n,\\t,\\t*\\tO\\nit\\tPRP\\t(NP(NP*))\\tO\\ncan\\tMD\\t(VP*\\tO\\nbe\\tVB\\t(VP*\\tO\\nsaid\\tVBN\\t(VP*\\tO\\nthat\\tIN\\t(SBAR*\\tO\\nmany\\tJJ\\t(S(NP*\\tO\\npeople\\tNNS\\t*)\\tO\\nhave\\tVBP\\t(VP*\\tO\\nviewed\\tVBN\\t(VP*\\tO\\nHong\\tNNP\\t(NP*\\tB-GPE\\nKong\\tNNP\\t*)\\tI-GPE\\nwith\\tIN\\t(PP*\\tO\\nnew\\tJJ\\t(NP*\\tO\\nrespect\\tNN\\t*)))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'Then\\tRB\\t(TOP(S(ADVP*)\\tO\\nwelcome\\tVBP\\t(VP*\\tO\\nto\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nofficial\\tJJ\\t*\\tO\\nwriting\\tNN\\t*\\tO\\nceremony\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nHong\\tNNP\\t(NP(NML*\\tB-FAC\\nKong\\tNNP\\t*)\\tI-FAC\\nDisneyland\\tNNP\\t*)))))\\tI-FAC\\n.\\t.\\t*))\\tO',\n",
       " 'The\\tDT\\t(TOP(S(NP(NP*\\tO\\nconstruction\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nHong\\tNNP\\t(NP(NML*\\tB-FAC\\nKong\\tNNP\\t*)\\tI-FAC\\nDisneyland\\tNNP\\t*)))\\tI-FAC\\nbegan\\tVBD\\t(VP*\\tO\\ntwo\\tCD\\t(ADVP(NP*\\tB-DATE\\nyears\\tNNS\\t*)\\tI-DATE\\nago\\tRB\\t*)\\tI-DATE\\n,\\t,\\t*\\tO\\nin\\tIN\\t(PP*\\tO\\n2003\\tCD\\t(NP*)))\\tB-DATE\\n.\\t.\\t*))\\tO',\n",
       " 'In\\tIN\\t(TOP(S(PP*\\tO\\nJanuary\\tNNP\\t(NP(NP*)\\tB-DATE\\nof\\tIN\\t(PP*\\tI-DATE\\nthat\\tDT\\t(NP*\\tI-DATE\\nyear\\tNN\\t*))))\\tI-DATE\\n,\\t,\\t*\\tO\\nthe\\tDT\\t(NP*\\tO\\nHong\\tNNP\\t(NML*\\tB-GPE\\nKong\\tNNP\\t*)\\tI-GPE\\ngovernment\\tNN\\t*)\\tO\\nturned\\tVBD\\t(VP*\\tO\\nover\\tRP\\t(PRT*)\\tO\\nto\\tIN\\t(PP*\\tO\\nDisney\\tNNP\\t(NP*\\tB-ORG\\nCorporation\\tNNP\\t*))\\tI-ORG\\n200\\tCD\\t(NP(NP(NP*\\tB-QUANTITY\\nhectares\\tNNS\\t*)\\tI-QUANTITY\\nof\\tIN\\t(PP*\\tO\\nland\\tNN\\t(NP*))\\tO\\nat\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nfoot\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nLantau\\tNNP\\t(NP*\\tB-LOC\\nIsland\\tNNP\\t*)))))\\tI-LOC\\nthat\\tWDT\\t(SBAR(WHNP*)\\tO\\nwas\\tVBD\\t(S(VP*\\tO\\nobtained\\tVBN\\t(VP*\\tO\\nfollowing\\tVBG\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nlargest\\tJJS\\t*\\tO\\nland\\tNN\\t(NML*\\tO\\nreclamation\\tNN\\t*)\\tO\\nproject\\tNN\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\nrecent\\tJJ\\t(NP*\\tB-DATE\\nyears\\tNNS\\t*))))))))))\\tI-DATE\\n.\\t.\\t*))\\tO',\n",
       " 'One\\tCD\\t(TOP(NP*\\tB-CARDINAL\\n.\\t.\\t*))\\tO',\n",
       " 'Since\\tIN\\t(TOP(S(PP*\\tO\\nthen\\tRB\\t(ADVP*))\\tO\\n,\\t,\\t*\\tO\\nthis\\tDT\\t(NP*\\tO\\narea\\tNN\\t*)\\tO\\nhas\\tVBZ\\t(VP*\\tO\\nbecome\\tVBN\\t(VP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nprohibited\\tVBN\\t*\\tO\\nzone\\tNN\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\nHong\\tNNP\\t(NP*\\tB-GPE\\nKong\\tNNP\\t*)))))\\tI-GPE\\n.\\t.\\t*))\\tO']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5e15110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7901892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d997328",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/development/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc40ca6",
   "metadata": {},
   "source": [
    "### Create test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b203802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training filJe with all the news corpus\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.bn.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "556a0e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c848103b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Iraqi\\tJJ\\t(TOP(S(NP(NML*\\tB-NORP\\nleader\\tNN\\t*)\\tO\\nSaddam\\tNNP\\t*\\tB-PERSON\\nHussein\\tNNP\\t*)\\tI-PERSON\\nhas\\tVBZ\\t(VP*\\tO\\ngiven\\tVBN\\t(VP*\\tO\\na\\tDT\\t(NP*\\tO\\ndefiant\\tJJ\\t*\\tO\\nspeech\\tNN\\t*)\\tO\\nto\\tTO\\t(S(VP*\\tO\\nmark\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\ntenth\\tJJ\\t*\\tB-ORDINAL\\nanniversary\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tB-EVENT\\nGulf\\tNNP\\t*\\tI-EVENT\\nWar\\tNNP\\t*))))))))\\tI-EVENT\\n.\\t.\\t*))\\tO',\n",
       " 'He\\tPRP\\t(TOP(S(NP*)\\tO\\nsays\\tVBZ\\t(VP*\\tO\\nIraq\\tNNP\\t(SBAR(S(NP*)\\tB-GPE\\nhas\\tVBZ\\t(VP*\\tO\\ntriumphed\\tVBN\\t(VP*\\tO\\nover\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nevil\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nWest\\tNNP\\t*)))))))))\\tB-LOC\\n.\\t.\\t*))\\tO',\n",
       " 'Barbara\\tNNP\\t(TOP(S(NP*\\tB-PERSON\\nPlett\\tNNP\\t*)\\tI-PERSON\\nreports\\tVBZ\\t(VP*\\tO\\nfrom\\tIN\\t(PP*\\tO\\nBaghdad\\tNNP\\t(NP*)))\\tB-GPE\\n.\\t.\\t*))\\tO',\n",
       " 'Saddam\\tNNP\\t(TOP(S(NP*\\tB-PERSON\\nHussein\\tNNP\\t*)\\tI-PERSON\\naddressed\\tVBD\\t(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nnation\\tNN\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nspeech\\tNN\\t*)\\tO\\nfilled\\tVBN\\t(VP*\\tO\\nwith\\tIN\\t(PP*\\tO\\nrhetoric\\tNN\\t(NP(NP*)\\tO\\nand\\tCC\\t*\\tO\\na\\tDT\\t(NP(NP*\\tO\\ndeclaration\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nvictory\\tNN\\t(NP*)))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " \"``\\t``\\t(TOP(S*\\tO\\nIraq\\tNNP\\t(S(NP*)\\tB-GPE\\nhas\\tVBZ\\t(VP*\\tO\\ntriumphed\\tVBN\\t(VP*\\tO\\nover\\tIN\\t(PP*\\tO\\nits\\tPRP$\\t(NP*\\tO\\nenemies\\tNNS\\t*)))))\\tO\\n,\\t,\\t*\\tO\\n''\\t''\\t*\\tO\\nhe\\tPRP\\t(PRN(S(NP*)\\tO\\nsaid\\tVBD\\t(VP*)))\\tO\\n,\\t,\\t*\\tO\\n``\\t``\\t*\\tO\\nand\\tCC\\t*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\nwill\\tMD\\t(VP*\\tO\\ntriumph\\tVB\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nremaining\\tVBG\\t*\\tO\\nrounds\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nbattle\\tNN\\t*)))))))\\tO\\n.\\t.\\t*\\tO\\n''\\t''\\t*))\\tO\",\n",
       " 'The\\tDT\\t(TOP(S(NP*\\tO\\nIraqi\\tJJ\\t*\\tB-NORP\\nleader\\tNN\\t*)\\tO\\nsaid\\tVBD\\t(VP*\\tO\\nthe\\tDT\\t(SBAR(S(NP*\\tB-EVENT\\nGulf\\tNNP\\t*\\tI-EVENT\\nWar\\tNNP\\t*)\\tI-EVENT\\nwas\\tVBD\\t(VP*\\tO\\na\\tDT\\t(NP(NP(NP*\\tO\\nconfrontation\\tNN\\t*)\\tO\\nbetween\\tIN\\t(PP*\\tO\\ngood\\tNN\\t(NP*\\tO\\nand\\tCC\\t*\\tO\\nevil\\tNN\\t*)))\\tO\\nthat\\tWDT\\t(SBAR(WHNP*)\\tO\\ncontinues\\tVBZ\\t(S(VP*\\tO\\nto\\tIN\\t(PP*\\tO\\nthis\\tDT\\t(NP*\\tO\\nday\\tNN\\t*))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'He\\tPRP\\t(TOP(S(NP*)\\tO\\nwas\\tVBD\\t(VP*\\tO\\nreferring\\tVBG\\t(VP*\\tO\\nto\\tIN\\t(PP*\\tO\\nhis\\tPRP$\\t(NP(NP*\\tO\\nstruggle\\tNN\\t*)\\tO\\nagainst\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nUS\\tNNP\\t*)\\tB-GPE\\n,\\t,\\t*\\tO\\nwhich\\tWDT\\t(SBAR(WHNP*)\\tO\\nstrongly\\tRB\\t(S(ADVP*)\\tO\\nsupports\\tVBZ\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\neconomic\\tJJ\\t*\\tO\\nembargo\\tNN\\t*)\\tO\\nmeant\\tVBN\\t(VP*\\tO\\nto\\tTO\\t(S(VP*\\tO\\nforce\\tVB\\t(VP*\\tO\\nhim\\tPRP\\t(NP*)\\tO\\nto\\tTO\\t(S(VP*\\tO\\ndisarm\\tVB\\t(VP*)))))))))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'After\\tIN\\t(TOP(S(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nspeech\\tNN\\t*))\\tO\\ndemonstrators\\tNNS\\t(NP*)\\tO\\ngathered\\tVBD\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\npublic\\tJJ\\t*\\tO\\nshow\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nsupport\\tNN\\t(NP(NP*)\\tO\\nfor\\tIN\\t(PP*\\tO\\ntheir\\tPRP$\\t(NP*\\tO\\nPresident\\tNNP\\t*)))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'A\\tDT\\t(TOP(S(NP(NP*\\tO\\nsense\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nnationalism\\tNN\\t(NP*)))\\tO\\nhas\\tVBZ\\t(VP*\\tO\\ngrown\\tVBN\\t(VP*\\tO\\nstronger\\tJJR\\t(ADJP*)\\tO\\nhere\\tRB\\t(ADVP*)\\tO\\nas\\tIN\\t(SBAR*\\tO\\nmany\\tJJ\\t(S(NP*\\tO\\npeople\\tNNS\\t*)\\tO\\nrally\\tVBP\\t(VP*\\tO\\nagainst\\tIN\\t(PP*\\tO\\nwhat\\tWP\\t(SBAR(WHNP*)\\tO\\nthey\\tPRP\\t(S(NP*)\\tO\\nsay\\tVBP\\t(VP*\\tO\\nis\\tVBZ\\t(SBAR(S(VP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nsiege\\tNN\\t*)\\tO\\non\\tIN\\t(PP*\\tO\\ntheir\\tPRP$\\t(NP*\\tO\\ncountry\\tNN\\t*)))))))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'Barbara\\tNNP\\t(TOP(FRAG(NP*\\tB-PERSON\\nPlett\\tNNP\\t*)\\tI-PERSON\\n,\\t,\\t*\\tO\\nBBC\\tNNP\\t(NP*\\tB-ORG\\nNews\\tNNP\\t*)\\tI-ORG\\n,\\t,\\t*\\tO\\nBaghdad\\tNNP\\t(NP*)\\tB-GPE\\n.\\t.\\t*))\\tO']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d115bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09dadc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "586b9172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.mz.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30c753da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33682301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.nw.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "711d912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ebb90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da1bf3a",
   "metadata": {},
   "source": [
    "### import data in CONLL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3174f407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|This is The World...|[{document, 0, 88...|[{document, 0, 88...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "| I am Lisa Mullins .|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|A plan was announ...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 0, A,...|[{pos, 0, 0, DT, ...|[{named_entity, 0...|\n",
      "|The Kursk Foundat...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The BBC 's James ...|[{document, 0, 66...|[{document, 0, 66...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|He says there are...|[{document, 0, 57...|[{document, 0, 57...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|Obviously the fir...|[{document, 0, 61...|[{document, 0, 61...|[{token, 0, 8, Ob...|[{pos, 0, 8, RB, ...|[{named_entity, 0...|\n",
      "|There are still t...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 4, Th...|[{pos, 0, 4, EX, ...|[{named_entity, 0...|\n",
      "|Secondly , there ...|[{document, 0, 79...|[{document, 0, 79...|[{token, 0, 7, Se...|[{pos, 0, 7, RB, ...|[{named_entity, 0...|\n",
      "|The Kursk Foundat...|[{document, 0, 30...|[{document, 0, 30...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|That 's the dange...|[{document, 0, 44...|[{document, 0, 44...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|I would think the...|[{document, 0, 56...|[{document, 0, 56...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|I mean this is a ...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|     That 's right .|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|The size of these...|[{document, 0, 51...|[{document, 0, 51...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|In fact one of th...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|Now , it 's lying...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 2, No...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|It is a huge subm...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|There are any num...|[{document, 0, 19...|[{document, 0, 19...|[{token, 0, 4, Th...|[{pos, 0, 4, EX, ...|[{named_entity, 0...|\n",
      "|Second problem , ...|[{document, 0, 77...|[{document, 0, 77...|[{token, 0, 5, Se...|[{pos, 0, 5, JJ, ...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing the training set\n",
    "from sparknlp.training import CoNLL\n",
    "\n",
    "training_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/train/sample.train')\n",
    "training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80fc3b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|In the summer of ...|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|With their unique...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 3, Wi...|[{pos, 0, 3, IN, ...|[{named_entity, 0...|\n",
      "|The world 's fift...|[{document, 0, 65...|[{document, 0, 65...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The most importan...|[{document, 0, 67...|[{document, 0, 67...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Well , for severa...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 3, We...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|Then welcome to t...|[{document, 0, 70...|[{document, 0, 70...|[{token, 0, 3, Th...|[{pos, 0, 3, RB, ...|[{named_entity, 0...|\n",
      "|The construction ...|[{document, 0, 71...|[{document, 0, 71...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|In January of tha...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|               One .|[{document, 0, 4,...|[{document, 0, 4,...|[{token, 0, 2, On...|[{pos, 0, 2, CD, ...|[{named_entity, 0...|\n",
      "|Since then , this...|[{document, 0, 65...|[{document, 0, 65...|[{token, 0, 4, Si...|[{pos, 0, 4, IN, ...|[{named_entity, 0...|\n",
      "|As its neighbor o...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 1, As...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|Mickey Mouse 's n...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 5, Mi...|[{pos, 0, 5, NNP,...|[{named_entity, 0...|\n",
      "|There 's only one...|[{document, 0, 88...|[{document, 0, 88...|[{token, 0, 4, Th...|[{pos, 0, 4, EX, ...|[{named_entity, 0...|\n",
      "|The subway to Dis...|[{document, 0, 50...|[{document, 0, 50...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|At subway station...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 1, At...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|Meanwhile , the D...|[{document, 0, 87...|[{document, 0, 87...|[{token, 0, 8, Me...|[{pos, 0, 8, RB, ...|[{named_entity, 0...|\n",
      "|For two years , D...|[{document, 0, 61...|[{document, 0, 61...|[{token, 0, 2, Fo...|[{pos, 0, 2, IN, ...|[{named_entity, 0...|\n",
      "|No media have bee...|[{document, 0, 47...|[{document, 0, 47...|[{token, 0, 1, No...|[{pos, 0, 1, DT, ...|[{named_entity, 0...|\n",
      "|We took a taxi al...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 1, We...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|However , before ...|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 6, Ho...|[{pos, 0, 6, RB, ...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/development/sample.train')\n",
    "dev_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7e4b286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Iraqi leader Sadd...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 4, Ir...|[{pos, 0, 4, JJ, ...|[{named_entity, 0...|\n",
      "|He says Iraq has ...|[{document, 0, 53...|[{document, 0, 53...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|Barbara Plett rep...|[{document, 0, 35...|[{document, 0, 35...|[{token, 0, 6, Ba...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
      "|Saddam Hussein ad...|[{document, 0, 98...|[{document, 0, 98...|[{token, 0, 5, Sa...|[{pos, 0, 5, NNP,...|[{named_entity, 0...|\n",
      "|`` Iraq has trium...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 1, ``...|[{pos, 0, 1, ``, ...|[{named_entity, 0...|\n",
      "|The Iraqi leader ...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|He was referring ...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|After the speech ...|[{document, 0, 88...|[{document, 0, 88...|[{token, 0, 4, Af...|[{pos, 0, 4, IN, ...|[{named_entity, 0...|\n",
      "|A sense of nation...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 0, A,...|[{pos, 0, 0, DT, ...|[{named_entity, 0...|\n",
      "|Barbara Plett , B...|[{document, 0, 35...|[{document, 0, 35...|[{token, 0, 6, Ba...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
      "| This is The World .|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|    I am Tony Kahn .|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|Gao Xingjian arri...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 2, Ga...|[{pos, 0, 2, NNP,...|[{named_entity, 0...|\n",
      "|Today Gao , who n...|[{document, 0, 17...|[{document, 0, 17...|[{token, 0, 4, To...|[{pos, 0, 4, NN, ...|[{named_entity, 0...|\n",
      "|Gao 's 1989 novel...|[{document, 0, 84...|[{document, 0, 84...|[{token, 0, 2, Ga...|[{pos, 0, 2, NNP,...|[{named_entity, 0...|\n",
      "|If you want to kn...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 1, If...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|His politics are ...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, Hi...|[{pos, 0, 2, PRP$...|[{named_entity, 0...|\n",
      "|This novel ` soul...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|It 's a large nov...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|It is , if you li...|[{document, 0, 62...|[{document, 0, 62...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e54ee7",
   "metadata": {},
   "source": [
    "### Training the model - 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5203e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base_cased download started this may take some time.\n",
      "Approximate size to download 389.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# use bert embeddings\n",
    "bert = BertEmbeddings.pretrained('bert_base_cased', 'en').setInputCols([\"sentence\",'token']).setOutputCol(\"bert\").setCaseSensitive(True)#.setMaxSentenceLength(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d0c34ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the training data into embeddings and saving it as parquet files\n",
    "readyTrainingData = bert.transform(training_data)\n",
    "\n",
    "readyTrainingData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab96caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "readyTrainingData = spark.read.parquet(\"/tmp/conll2003/bert_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c4f2096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the development data into embeddings and saving it as parquet files\n",
    "readyDevData = bert.transform(dev_data)\n",
    "\n",
    "readyDevData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0c284d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "readyDevData = spark.read.parquet(\"/tmp/conll2003/bert_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d64dac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa34583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "readyTestData = spark.read.parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0897336",
   "metadata": {},
   "source": [
    "### Dev set - news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7bd2f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize NER tagger\n",
    "nerTagger = NerDLApproach()\\\n",
    ".setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
    ".setLabelColumn(\"label\")\\\n",
    ".setOutputCol(\"ner\")\\\n",
    ".setMaxEpochs(10)\\\n",
    ".setBatchSize(4)\\\n",
    ".setEnableMemoryOptimizer(True)\\\n",
    ".setRandomSeed(0)\\\n",
    ".setVerbose(1)\\\n",
    ".setValidationSplit(0.2)\\\n",
    ".setEvaluationLogExtended(True)\\\n",
    ".setEnableOutputLogs(True)\\\n",
    ".setIncludeConfidence(True)\\\n",
    ".setTestDataset(\"/tmp/conll2003/bert_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68c9418e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 870 ms, sys: 478 ms, total: 1.35 s\n",
      "Wall time: 2h 45min 33s\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "%time myNerModel = nerTagger.fit(readyTrainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327a58c4",
   "metadata": {},
   "source": [
    "### Testing on News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "822a465e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.2 s, sys: 369 ms, total: 2.57 s\n",
      "Wall time: 21.2 s\n"
     ]
    }
   ],
   "source": [
    "# infer from the trained model\n",
    "%time results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c24e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c50bb9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[146, 334]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5da5e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a198acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "275e30d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9745594261157319\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "445fbe5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8826413454974243\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cdac3a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.85      0.87      0.86       616\n",
      "        DATE       0.85      0.87      0.86      1252\n",
      "       EVENT       0.71      0.54      0.62        37\n",
      "         FAC       0.49      0.67      0.56        66\n",
      "         GPE       0.96      0.93      0.95      1664\n",
      "    LANGUAGE       0.83      0.50      0.62        10\n",
      "         LAW       0.82      0.39      0.53        36\n",
      "         LOC       0.74      0.79      0.77       144\n",
      "       MONEY       0.88      0.89      0.88       279\n",
      "        NORP       0.96      0.96      0.96       579\n",
      "     ORDINAL       0.80      0.88      0.84       118\n",
      "         ORG       0.90      0.87      0.88      1494\n",
      "     PERCENT       0.92      0.92      0.92       293\n",
      "      PERSON       0.91      0.97      0.94      1099\n",
      "     PRODUCT       0.60      0.75      0.66        71\n",
      "    QUANTITY       0.70      0.80      0.75        59\n",
      "        TIME       0.60      0.61      0.60       106\n",
      " WORK_OF_ART       0.42      0.41      0.41       111\n",
      "\n",
      "   micro avg       0.88      0.89      0.88      8034\n",
      "   macro avg       0.77      0.76      0.76      8034\n",
      "weighted avg       0.88      0.89      0.88      8034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9202cdb8",
   "metadata": {},
   "source": [
    "### Testing on News Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e8a88d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.bc.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ae1562fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b6103b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "15469afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|-- basically , it...|[{document, 0, 78...|[{document, 0, 78...|[{token, 0, 1, --...|[{pos, 0, 1, :, {...|[{named_entity, 0...|\n",
      "|To express its de...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 1, To...|[{pos, 0, 1, TO, ...|[{named_entity, 0...|\n",
      "|It takes time to ...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|Dear viewers , th...|[{document, 0, 52...|[{document, 0, 52...|[{token, 0, 3, De...|[{pos, 0, 3, NNP,...|[{named_entity, 0...|\n",
      "|     This is Xu Li .|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|Thank you everyon...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 4, Th...|[{pos, 0, 4, VBP,...|[{named_entity, 0...|\n",
      "|Coming up is the ...|[{document, 0, 59...|[{document, 0, 59...|[{token, 0, 5, Co...|[{pos, 0, 5, VBG,...|[{named_entity, 0...|\n",
      "|Good-bye , dear v...|[{document, 0, 24...|[{document, 0, 24...|[{token, 0, 7, Go...|[{pos, 0, 7, UH, ...|[{named_entity, 0...|\n",
      "|Hello , dear view...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 4, He...|[{pos, 0, 4, UH, ...|[{named_entity, 0...|\n",
      "|Welcome to Focus ...|[{document, 0, 23...|[{document, 0, 23...|[{token, 0, 6, We...|[{pos, 0, 6, VBP,...|[{named_entity, 0...|\n",
      "|Today , let 's tu...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 4, To...|[{pos, 0, 4, NN, ...|[{named_entity, 0...|\n",
      "|Before dawn on Ja...|[{document, 0, 19...|[{document, 0, 19...|[{token, 0, 5, Be...|[{pos, 0, 5, IN, ...|[{named_entity, 0...|\n",
      "|Relevant departme...|[{document, 0, 94...|[{document, 0, 94...|[{token, 0, 7, Re...|[{pos, 0, 7, JJ, ...|[{named_entity, 0...|\n",
      "|The traffic admin...|[{document, 0, 94...|[{document, 0, 94...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Well , how did th...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 3, We...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|After the holiday...|[{document, 0, 72...|[{document, 0, 72...|[{token, 0, 4, Af...|[{pos, 0, 4, IN, ...|[{named_entity, 0...|\n",
      "|In addition , wha...|[{document, 0, 19...|[{document, 0, 19...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|Well , we have in...|[{document, 0, 93...|[{document, 0, 93...|[{token, 0, 3, We...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|One of the two ho...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 2, On...|[{pos, 0, 2, CD, ...|[{named_entity, 0...|\n",
      "|             Hello .|[{document, 0, 6,...|[{document, 0, 6,...|[{token, 0, 4, He...|[{pos, 0, 4, UH, ...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "71c67d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cf52b8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.23 s, sys: 309 ms, total: 1.54 s\n",
      "Wall time: 55.9 s\n"
     ]
    }
   ],
   "source": [
    "# infer from the trained model\n",
    "%time results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "985cb059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "af4130a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "25428c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fca4070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "23eb70dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.976606747106624\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "27452eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7860340196956133\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f39a1c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.69      0.74      0.71       182\n",
      "        DATE       0.67      0.67      0.67       200\n",
      "       EVENT       0.40      0.14      0.21        14\n",
      "         FAC       0.89      0.88      0.88        48\n",
      "         GPE       0.93      0.90      0.92       353\n",
      "         LAW       1.00      0.00      0.00         3\n",
      "         LOC       0.79      0.73      0.76        26\n",
      "       MONEY       1.00      1.00      1.00         3\n",
      "        NORP       0.89      0.91      0.90       138\n",
      "     ORDINAL       0.88      0.72      0.79        50\n",
      "         ORG       0.78      0.78      0.78       153\n",
      "     PERCENT       0.93      0.93      0.93        14\n",
      "      PERSON       0.87      0.87      0.87       382\n",
      "     PRODUCT       0.00      1.00      0.00         0\n",
      "    QUANTITY       0.23      0.23      0.23        40\n",
      "        TIME       0.51      0.43      0.47        63\n",
      " WORK_OF_ART       0.20      0.11      0.14        28\n",
      "\n",
      "   micro avg       0.80      0.78      0.79      1697\n",
      "   macro avg       0.69      0.65      0.60      1697\n",
      "weighted avg       0.79      0.78      0.78      1697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929acf71",
   "metadata": {},
   "source": [
    "### Testing on WebLogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0597d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.wb.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bbd25fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6524551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6c942177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|The success of al...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The Source of the...|[{document, 0, 23...|[{document, 0, 23...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The al - Jazeera ...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|In this film the ...|[{document, 0, 73...|[{document, 0, 73...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|The Hebrew channe...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|This is a unique ...|[{document, 0, 45...|[{document, 0, 45...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|There are many qu...|[{document, 0, 20...|[{document, 0, 20...|[{token, 0, 4, Th...|[{pos, 0, 4, EX, ...|[{named_entity, 0...|\n",
      "|I leave you with ...|[{document, 0, 72...|[{document, 0, 72...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|the link to the f...|[{document, 0, 22...|[{document, 0, 22...|[{token, 0, 2, th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|http://z08.zuploa...|[{document, 0, 52...|[{document, 0, 52...|[{token, 0, 52, h...|[{pos, 0, 52, ADD...|[{named_entity, 0...|\n",
      "|The Islam Diary :...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Speaking at the s...|[{document, 0, 44...|[{document, 0, 44...|[{token, 0, 7, Sp...|[{pos, 0, 7, VBG,...|[{named_entity, 0...|\n",
      "|He also told the ...|[{document, 0, 34...|[{document, 0, 34...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|The newspaper quo...|[{document, 0, 33...|[{document, 0, 33...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The newspaper des...|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|He speaks in the ...|[{document, 0, 93...|[{document, 0, 93...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|He was quoted as ...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|It pointed out th...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|This means that t...|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|\" Fadil \" was quo...|[{document, 0, 22...|[{document, 0, 22...|[{token, 0, 0, \",...|[{pos, 0, 0, ``, ...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3e3adc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bee7150b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 381 ms, sys: 44 ms, total: 425 ms\n",
      "Wall time: 31.6 s\n"
     ]
    }
   ],
   "source": [
    "# infer from the trained model\n",
    "%time results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8cb952ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ff92a64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "572ca05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "485a5704",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1e8cdd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9680126682501979\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "96bea8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7890556045895851\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6f84964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.86      0.80      0.83        85\n",
      "        DATE       0.58      0.74      0.65        74\n",
      "       EVENT       0.43      0.25      0.32        12\n",
      "         FAC       0.59      0.56      0.57        18\n",
      "         GPE       0.92      0.93      0.93       173\n",
      "    LANGUAGE       1.00      0.50      0.67         4\n",
      "         LAW       1.00      0.00      0.00         1\n",
      "         LOC       0.36      0.44      0.40         9\n",
      "       MONEY       0.78      0.72      0.75        25\n",
      "        NORP       0.90      0.89      0.90       107\n",
      "     ORDINAL       0.75      0.50      0.60        18\n",
      "         ORG       0.76      0.75      0.76       117\n",
      "     PERCENT       0.64      0.64      0.64        33\n",
      "      PERSON       0.86      0.82      0.84       407\n",
      "     PRODUCT       0.00      0.00      0.00         1\n",
      "    QUANTITY       0.20      0.33      0.25         6\n",
      "        TIME       0.65      0.85      0.74        20\n",
      " WORK_OF_ART       0.41      0.33      0.37        27\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      1137\n",
      "   macro avg       0.65      0.56      0.57      1137\n",
      "weighted avg       0.80      0.79      0.79      1137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629decfc",
   "metadata": {},
   "source": [
    "### Testing on Telephonic Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4f401797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.tc.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6b826c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f7b8fe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "74968763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|But %um , guessed...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 2, Bu...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|              What ?|[{document, 0, 5,...|[{document, 0, 5,...|[{token, 0, 3, Wh...|[{pos, 0, 3, WP, ...|[{named_entity, 0...|\n",
      "|         %um The %um|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, %u...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|             Again ?|[{document, 0, 6,...|[{document, 0, 6,...|[{token, 0, 4, Ag...|[{pos, 0, 4, RB, ...|[{named_entity, 0...|\n",
      "|%um 118.91_120.82...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 2, %u...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|       two weeks ago|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 2, tw...|[{pos, 0, 2, CD, ...|[{named_entity, 0...|\n",
      "|I know Sue is goi...|[{document, 0, 36...|[{document, 0, 36...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|         %eh Right .|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, %e...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|             Right .|[{document, 0, 6,...|[{document, 0, 6,...|[{token, 0, 4, Ri...|[{pos, 0, 4, UH, ...|[{named_entity, 0...|\n",
      "|Did she see her m...|[{document, 0, 20...|[{document, 0, 20...|[{token, 0, 2, Di...|[{pos, 0, 2, VBD,...|[{named_entity, 0...|\n",
      "|       Yes she did .|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 2, Ye...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|               And ?|[{document, 0, 4,...|[{document, 0, 4,...|[{token, 0, 2, An...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|       Yes she did .|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 2, Ye...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|               And ?|[{document, 0, 4,...|[{document, 0, 4,...|[{token, 0, 2, An...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|%um , %um , which...|[{document, 0, 73...|[{document, 0, 73...|[{token, 0, 2, %u...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|I spoke to her mo...|[{document, 0, 29...|[{document, 0, 29...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "| %um and I asked her|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 2, %u...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|       I said listen|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|I said %um please...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|things have come ...|[{document, 0, 78...|[{document, 0, 78...|[{token, 0, 5, th...|[{pos, 0, 5, NNS,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cf3efcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "eabb88c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 319 ms, sys: 29.7 ms, total: 349 ms\n",
      "Wall time: 29.4 s\n"
     ]
    }
   ],
   "source": [
    "# infer from the trained model\n",
    "%time results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "75b3e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a64ec236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ccb527cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "788c5188",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0d231fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9793185131195336\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c2cdc728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0475c395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.59      0.44      0.51        52\n",
      "        DATE       0.75      0.74      0.75        74\n",
      "       EVENT       0.00      1.00      0.00         0\n",
      "         FAC       0.25      0.33      0.29         3\n",
      "         GPE       0.87      0.90      0.88        50\n",
      "    LANGUAGE       1.00      0.50      0.67         8\n",
      "         LOC       0.00      1.00      0.00         0\n",
      "       MONEY       1.00      0.71      0.83         7\n",
      "        NORP       0.81      1.00      0.89        17\n",
      "     ORDINAL       0.89      0.89      0.89         9\n",
      "         ORG       0.62      0.19      0.29        27\n",
      "     PERCENT       0.80      0.67      0.73         6\n",
      "      PERSON       0.86      0.92      0.89       100\n",
      "     PRODUCT       0.20      0.25      0.22         4\n",
      "    QUANTITY       0.00      1.00      0.00         0\n",
      "        TIME       0.50      0.17      0.26        23\n",
      " WORK_OF_ART       0.00      1.00      0.00         0\n",
      "\n",
      "   micro avg       0.76      0.69      0.73       380\n",
      "   macro avg       0.54      0.69      0.48       380\n",
      "weighted avg       0.76      0.69      0.71       380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
