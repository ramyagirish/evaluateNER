{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1edef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  3.1.0\n",
      "Apache Spark version:  3.1.2\n"
     ]
    }
   ],
   "source": [
    "# call relevant packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "import sparknlp\n",
    "\n",
    "# to use GPU \n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb98346",
   "metadata": {},
   "source": [
    "## Creating a composite CoNLL file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4910c848",
   "metadata": {},
   "source": [
    "### Create training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "024205af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio-splits/train/fold6_train.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85935111",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769deee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'s\\tVBZ\\t(SQ*\\tO\\nhappening\\tVBG\\t(VP*\\tO\\nthere\\tRB\\t(ADVP*)\\tO\\ntonight\\tNN\\t(NP*)\\tB-TIME\\n,\\t,\\t*\\tO\\nGary\\tNNP\\t(NP*)))\\tB-PERSON\\n?\\t.\\t*))\\tO\",\n",
       " 'What\\tWP\\t(TOP(SBARQ(WHNP(WHNP*)\\tO\\nexactly\\tRB\\t(ADVP*))\\tO\\nis\\tVBZ\\t(SQ*\\tO\\nthe\\tDT\\t(NP*\\tO\\ntruth\\tNN\\t*))\\tO\\n?\\t.\\t*))\\tO',\n",
       " 'His\\tPRP$\\t(TOP(S(NP*\\tO\\nplatform\\tNN\\t*)\\tO\\nincludes\\tVBZ\\t(VP*\\tO\\nsubsidies\\tNNS\\t(NP(NP*)\\tO\\nto\\tTO\\t(SBAR(S(VP*\\tO\\nassist\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP(NP*\\tO\\nchildren\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nlow\\tJJ\\t(NP(NML*\\tO\\n-\\tHYPH\\t*\\tO\\nincome\\tNN\\t*)\\tO\\nfamilies\\tNNS\\t*)))\\tO\\nwho\\tWP\\t(SBAR(WHNP*)\\tO\\nattend\\tVBP\\t(S(VP*\\tO\\nprivate\\tJJ\\t(NP*\\tO\\nsecondary\\tJJ\\t*\\tO\\nschools\\tNNS\\t*)))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " \"One\\tCD\\t(TOP(FRAG(NP*\\tB-CARDINAL\\nexample\\tNN\\t*)\\tO\\nhe\\tPRP\\t(SBAR(S(NP*)\\tO\\ngives\\tVBZ\\t(VP*)))\\tO\\n:\\t:\\t*\\tO\\n``\\t``\\t*\\tO\\nShe\\tPRP\\t(S(NP*)\\tO\\ndid\\tVBD\\t(VP*\\tO\\nn't\\tRB\\t*\\tO\\nask\\tVB\\t(VP*\\tO\\n''\\t''\\t*\\tO\\n-LRB-\\t-LRB-\\t*\\tO\\nwhy\\tWRB\\t(SBAR(WHADVP*)\\tO\\nthe\\tDT\\t(S(NP*\\tO\\nPalestinian\\tJJ\\t*\\tB-NORP\\nchildren\\tNNS\\t*)\\tO\\nare\\tVBP\\t(VP*\\tO\\nsoldiers\\tNNS\\t(NP(NP*)\\tO\\nthrowing\\tVBG\\t(VP*\\tO\\nstones\\tNNS\\t(NP*))))))\\tO\\n-RRB-\\t-RRB-\\t*)))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'and\\tCC\\t(TOP(S*\\tO\\nthen\\tRB\\t(ADVP*)\\tO\\nhorrendous\\tJJ\\t(NP*\\tO\\nthings\\tNNS\\t*)\\tO\\nhappened\\tVBD\\t(VP*\\tO\\nto\\tIN\\t(PP*\\tO\\nlittle\\tJJ\\t(NP*\\tO\\nkids\\tNNS\\t*)))\\tO\\n/.\\t.\\t*))\\tO',\n",
       " 'Very\\tRB\\t(TOP(ADJP*\\tO\\nclose\\tJJ\\t*\\tO\\nto\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nU.S.\\tNNP\\t(NML*\\tB-ORG\\nMarines\\tNNPS\\t*)\\tI-ORG\\nuniform\\tNN\\t*))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'Yutsai\\tNNP\\t(TOP(S(NP(NP*\\tB-ORG\\nPrimary\\tNNP\\t*\\tI-ORG\\nSchool\\tNNP\\t*)\\tI-ORG\\nin\\tIN\\t(PP*\\tO\\nTaipei\\tNNP\\t(NP*\\tB-GPE\\nCounty\\tNNP\\t*)))\\tI-GPE\\ndid\\tVBD\\t(VP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nsurvey\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nhow\\tWRB\\t(SBAR(WHADVP*)\\tO\\nparents\\tNNS\\t(S(NP*)\\tO\\nfelt\\tVBD\\t(VP*\\tO\\nabout\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nprimary\\tJJ\\t*\\tO\\nEnglish\\tNNP\\t(NML*\\tB-LANGUAGE\\nteaching\\tNN\\t*)\\tO\\nprogram\\tNN\\t*))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'No\\tDT\\t(TOP(S(S(NP*\\tO\\none\\tNN\\t*)\\tO\\nhas\\tVBZ\\t(VP*\\tO\\never\\tRB\\t(ADVP*)\\tO\\ndone\\tVBN\\t(VP*\\tO\\nanything\\tNN\\t(NP(NP*)\\tO\\nlike\\tIN\\t(PP*\\tO\\nit\\tPRP\\t(NP*)))\\tO\\nbefore\\tRB\\t(ADVP*))))\\tO\\n,\\t,\\t*\\tO\\nand\\tCC\\t*\\tO\\nwe\\tPRP\\t(S(NP*)\\tO\\ndo\\tVBP\\t(VP*\\tO\\nrealize\\tVB\\t(VP*\\tO\\nthat\\tDT\\t(SBAR*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nvery\\tRB\\t(ADJP*\\tO\\nrisky\\tJJ\\t*))))\\tO\\n,\\t,\\t*\\tO\\nboth\\tCC\\t*\\tO\\nwhen\\tWRB\\t(SBAR(WHADVP*)\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\ncomes\\tVBZ\\t(VP*\\tO\\nto\\tIN\\t(PP(PP*\\tO\\nphysical\\tJJ\\t(NP(NP*\\tO\\ndangers\\tNNS\\t*)\\tO\\nfor\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\ndivers\\tNNS\\t*))))\\tO\\n,\\t,\\t*\\tO\\nand\\tCC\\t*\\tO\\nto\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\npsychological\\tJJ\\t*\\tO\\nstress\\tNN\\t*)\\tO\\ndepending\\tVBG\\t(VP*\\tO\\non\\tIN\\t(PP*\\tO\\nwhat\\tWP\\t(SBAR(WHNP*)\\tO\\nwe\\tPRP\\t(S(NP*)\\tO\\nfind\\tVBP\\t(VP*\\tO\\nthere\\tRB\\t(ADVP*)))))))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'Uh-huh\\tUH\\t(TOP(INTJ*\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'University\\tNNP\\t(TOP(S(NP(NP*)\\tB-ORG\\nof\\tIN\\t(PP*\\tI-ORG\\nMedicine\\tNNP\\t(NP*\\tI-ORG\\nand\\tCC\\t*\\tI-ORG\\nDentistry\\tNNP\\t*))\\tI-ORG\\nin\\tIN\\t(PP*\\tO\\nNew\\tNNP\\t(NP*\\tB-GPE\\nJersey\\tNNP\\t*)))\\tI-GPE\\nis\\tVBZ\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nPiscataway\\tNNP\\t(NP(NP(NP*)\\tB-GPE\\nNew\\tNNP\\t(NP*\\tB-GPE\\nJersey\\tNNP\\t*))\\tI-GPE\\npiscataway\\tNNP\\t(NP*)\\tB-GPE\\nPiscataway\\tNNP\\t(NP(NP*)\\tB-GPE\\nNew\\tNNP\\t(NP*\\tB-GPE\\nJersey\\tNNP\\t*)))))\\tI-GPE\\n/.\\t.\\t*))\\tO']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f341e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a5589da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e0b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio-splits/train/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa11d3b4",
   "metadata": {},
   "source": [
    "### Create dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "488f800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training filJe with all the news corpus\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio-splits/dev/fold6_dev.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc5fc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3f11a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['leaving\\tVBG\\t(S(VP*\\tO\\nJuilliard\\tNNP\\t(NP*))))\\tB-ORG\\n,\\t,\\t*\\tO\\nMr.\\tNNP\\t(NP*\\tO\\nMcDuffie\\tNNP\\t*)\\tB-PERSON\\nhas\\tVBZ\\t(VP*\\tO\\nmade\\tVBN\\t(VP*\\tO\\nsome\\tDT\\t(NP(NP*\\tO\\nsmart\\tJJ\\t*\\tO\\nmoves\\tNNS\\t*)\\tO\\nand\\tCC\\t*\\tO\\nsome\\tDT\\t(NP*\\tO\\ncontroversial\\tJJ\\t*\\tO\\nones\\tNNS\\t*))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " \"They\\tPRP\\t(TOP(S(NP*)\\tO\\n're\\tVBP\\t(VP*\\tO\\nsuing\\tVBG\\t(VP*\\tO\\n27\\tCD\\t(NP(NP*\\tB-CARDINAL\\ndefendants\\tNNS\\t*)\\tO\\nincluding\\tVBG\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nclub\\tNN\\t*)\\tO\\n,\\t,\\t*\\tO\\nthe\\tDT\\t(NP*\\tO\\nband\\tNN\\t*)\\tO\\n,\\t,\\t*\\tO\\ncompanies\\tNNS\\t(NP(NP*)\\tO\\nwith\\tIN\\t(PP*\\tO\\nties\\tNNS\\t(NP(NP*)\\tO\\nto\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nclub\\tNN\\t*)))))\\tO\\n,\\t,\\t*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\ntown\\tNN\\t*)\\tO\\nas\\tRB\\t(CONJP*\\tO\\nwell\\tRB\\t*\\tO\\nas\\tIN\\t*)\\tO\\nthe\\tDT\\t(NP*\\tO\\nstate\\tNN\\t*)))))))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'Well\\tUH\\t(TOP(S(INTJ*)\\tO\\n,\\t,\\t*\\tO\\nfrom\\tIN\\t(PP*\\tO\\nthis\\tDT\\t(NP*\\tO\\nfootage\\tNN\\t*))\\tO\\n,\\t,\\t*\\tO\\nwe\\tPRP\\t(NP*)\\tO\\ncan\\tMD\\t(VP*\\tO\\nsee\\tVB\\t(VP*\\tO\\nthat\\tIN\\t(SBAR*\\tO\\n,\\t,\\t*\\tO\\nha\\tUH\\t(S(S(INTJ*)\\tO\\n,\\t,\\t*\\tO\\nat\\tIN\\t(PP*\\tO\\nEast\\tNNP\\t(NP(NP*\\tB-FAC\\nThird\\tNNP\\t*\\tI-FAC\\nRing\\tNNP\\t*\\tI-FAC\\nRoad\\tNNP\\t*)\\tI-FAC\\n,\\t,\\t*\\tO\\nah\\tUH\\t(INTJ*)\\tO\\n,\\t,\\t*\\tO\\nnear\\tIN\\t(PP*\\tO\\nGuanghua\\tNNP\\t(NP*\\tB-FAC\\nBridge\\tNNP\\t*))))\\tI-FAC\\n,\\t,\\t*\\tO\\nthe\\tDT\\t(NP(NP(NP*\\tO\\nroads\\tNNS\\t*)\\tO\\nhere\\tRB\\t(ADVP*))\\tO\\nthat\\tWDT\\t(SBAR(WHNP*)\\tO\\nlead\\tVBP\\t(S(VP*\\tO\\nto\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nvicinity\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nJingguang\\tNNP\\t(NP*\\tB-FAC\\nBridge\\tNNP\\t*))))))))\\tI-FAC\\nhave\\tVBP\\t(VP*\\tO\\ncome\\tVBN\\t(VP*\\tO\\nunder\\tIN\\t(PP*\\tO\\ncontrol\\tNN\\t(NP*)))))\\tO\\nand\\tCC\\t*\\tO\\n,\\t,\\t*\\tO\\nwell\\tUH\\t(INTJ*)\\tO\\n,\\t,\\t*\\tO\\nvarious\\tJJ\\t(S(NP(NP*\\tO\\nkinds\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nsigns\\tNNS\\t(NP*)))\\tO\\nhave\\tVBP\\t(VP*\\tO\\nalso\\tRB\\t(ADVP*)\\tO\\nbeen\\tVBN\\t(VP*\\tO\\nplaced\\tVBN\\t(VP*\\tO\\nat\\tIN\\t(PP*\\tO\\nvery\\tRB\\t(NP(ADJP*\\tO\\nconspicuous\\tJJ\\t*)\\tO\\nlocations\\tNNS\\t*))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'Scientists\\tNNS\\t(TOP(S(S(NP*)\\tO\\nhave\\tVBP\\t(VP*\\tO\\nsuspected\\tVBN\\t(VP*\\tO\\nthere\\tEX\\t(SBAR(S(NP*)\\tO\\nmust\\tMD\\t(VP*\\tO\\nbe\\tVB\\t(VP*\\tO\\nsomething\\tNN\\t(NP(NP*)\\tO\\ndifferent\\tJJ\\t(ADJP*)\\tO\\nabout\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\ngenetic\\tJJ\\t*\\tO\\nmake\\tNN\\t*\\tO\\n-\\tHYPH\\t*\\tO\\nup\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\ndiabetics\\tNNS\\t(NP(NP*)\\tO\\nwho\\tWP\\t(SBAR(WHNP*)\\tO\\ndevelop\\tVBP\\t(S(VP*\\tO\\ncomplications\\tNNS\\t(NP*))))))))))))))))\\tO\\n;\\t:\\t*\\tO\\nand\\tCC\\t*\\tO\\nthat\\tDT\\t(S(NP*\\tO\\ndifference\\tNN\\t*)\\tO\\nmay\\tMD\\t(VP*\\tO\\nhave\\tVB\\t(VP*\\tO\\nto\\tTO\\t(S(VP*\\tO\\ndo\\tVB\\t(VP*\\tO\\nwith\\tIN\\t(PP*\\tO\\nhaptoglobin\\tNN\\t(NP(NP*)\\tO\\n,\\t,\\t*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nprotein\\tNN\\t*)\\tO\\nwhose\\tWP$\\t(SBAR(WHNP*\\tO\\nfunction\\tNN\\t*)\\tO\\nis\\tVBZ\\t(S(VP*\\tO\\nto\\tTO\\t(S(VP*\\tO\\nmop\\tVB\\t(VP*\\tO\\nup\\tRP\\t(PRT*)\\tO\\nblood\\tNN\\t(NP(ADJP(NML*\\tO\\nvessel\\tNN\\t*)\\tO\\ndestroying\\tVBG\\t*)\\tO\\nfree\\tJJ\\t*\\tO\\nradicals\\tNNS\\t*))))))))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'Had\\tVBD\\t(TOP(S(SBAR(SINV*\\tO\\nthey\\tPRP\\t(NP*)\\tO\\ncome\\tVBN\\t(VP*\\tO\\nto\\tIN\\t(PP*\\tO\\nCongress\\tNNP\\t(NP*))\\tB-ORG\\na\\tDT\\t(ADVP(NP(QP*\\tB-DATE\\nfew\\tJJ\\t*)\\tI-DATE\\nyears\\tNNS\\t*)\\tI-DATE\\nago\\tRB\\t*)\\tI-DATE\\non\\tIN\\t(PP*\\tO\\nthis\\tDT\\t(NP*\\tO\\nissue\\tNN\\t*)))))\\tO\\nmy\\tPRP$\\t(NP*\\tO\\nguess\\tNN\\t*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nthey\\tPRP\\t(SBAR(S(NP*)\\tO\\nwould\\tMD\\t(VP*\\tO\\nhave\\tVB\\t(VP*\\tO\\ngotten\\tVBN\\t(VP*\\tO\\nmost\\tJJS\\t(NP(NP*)\\tO\\nof\\tIN\\t(PP*\\tO\\nwhat\\tWP\\t(SBAR(WHNP*)\\tO\\nthey\\tPRP\\t(S(NP*)\\tO\\nwanted\\tVBD\\t(VP*)))))))))))\\tO\\n/.\\t.\\t*))\\tO',\n",
       " \"No\\tUH\\t(TOP(S(INTJ*)\\tO\\nI\\tPRP\\t(NP*)\\tO\\n'm\\tVBP\\t(VP*\\tO\\nnot\\tRB\\t*)\\tO\\n/.\\t.\\t*))\\tO\",\n",
       " \"In\\tIN\\t(TOP(S(PP*\\tO\\nthese\\tDT\\t(NP*\\tO\\nrespects\\tNNS\\t*))\\tO\\n,\\t,\\t*\\tO\\nEl\\tNNP\\t(NP(NP*\\tB-GPE\\nSalvador\\tNNP\\t*\\tI-GPE\\n's\\tPOS\\t*)\\tI-GPE\\nsituation\\tNN\\t*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nvery\\tRB\\t(ADJP*\\tO\\nsimilar\\tJJ\\t*\\tO\\nto\\tIN\\t(PP*\\tO\\nthat\\tDT\\t(NP(NP*)\\tO\\nof\\tIN\\t(PP*\\tO\\nTaiwan\\tNNP\\t(NP*))))))\\tB-GPE\\n.\\t.\\t*))\\tO\",\n",
       " 'The\\tDT\\t(TOP(S(NP(NP(NP*\\tO\\neconomic\\tJJ\\t*\\tO\\nconstraints\\tNNS\\t*)\\tO\\nand\\tCC\\t*\\tO\\nrepressive\\tJJ\\t(NP*\\tO\\ncensorship\\tNN\\t*))\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tB-DATE\\nera\\tNN\\t*)))\\tI-DATE\\ndid\\tVBD\\t(VP*\\tO\\nnothing\\tNN\\t(NP*)\\tO\\nto\\tTO\\t(S(VP*\\tO\\ndeter\\tVB\\t(VP*\\tO\\nthese\\tDT\\t(NP*\\tO\\nyoung\\tJJ\\t*\\tO\\nart\\tNN\\t*\\tO\\nfiends\\tNNS\\t*)))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " \"``\\t``\\t(TOP(SINV*\\tO\\nI\\tPRP\\t(S(NP*)\\tO\\n'll\\tMD\\t(VP*\\tO\\nstay\\tVB\\t(VP*\\tO\\nwith\\tIN\\t(PP*\\tO\\nBART\\tNNP\\t(NP*)))))\\tB-ORG\\n,\\t,\\t*\\tO\\n''\\t''\\t*\\tO\\nsaid\\tVBD\\t(VP*)\\tO\\none\\tCD\\t(NP(NP*\\tB-CARDINAL\\nsecretary\\tNN\\t*))\\tO\\n,\\t,\\t*\\tO\\nswallowing\\tVBG\\t(S(VP*\\tO\\nher\\tPRP$\\t(NP(NP*\\tO\\nfears\\tNNS\\t*)\\tO\\nabout\\tIN\\t(PP*\\tO\\nusing\\tVBG\\t(S(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\ntransbay\\tJJ\\t*\\tO\\ntube\\tNN\\t*)))))))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " \"Mr.\\tNNP\\t(TOP(S(NP(NP*\\tO\\nLuzon\\tNNP\\t*)\\tB-PERSON\\nand\\tCC\\t*\\tO\\nhis\\tPRP$\\t(NP*\\tO\\nteam\\tNN\\t*))\\tO\\n,\\t,\\t*\\tO\\nhowever\\tRB\\t(ADVP*)\\tO\\n,\\t,\\t*\\tO\\nsay\\tVBP\\t(VP*\\tO\\nthey\\tPRP\\t(SBAR(S(NP*)\\tO\\nare\\tVBP\\t(VP*\\tO\\nn't\\tRB\\t*\\tO\\ninterested\\tJJ\\t(ADJP*\\tO\\nin\\tIN\\t(PP*\\tO\\na\\tDT\\t(NP*\\tO\\nmerger\\tNN\\t*)))))))\\tO\\n.\\t.\\t*))\\tO\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5e15110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7901892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d997328",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio-splits/dev/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc40ca6",
   "metadata": {},
   "source": [
    "### Create test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b203802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training filJe with all the news corpus\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio-splits/test/fold6_test.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "556a0e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c848103b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subway\\tNN\\t*)\\tO\\nto\\tIN\\t(PP*\\tO\\nDisney\\tNNP\\t(NP*)))\\tB-FAC\\nhas\\tVBZ\\t(VP*\\tO\\nalready\\tRB\\t(ADVP*)\\tO\\nbeen\\tVBN\\t(VP*\\tO\\nconstructed\\tVBN\\t(VP*)))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'Ah\\tUH\\t(TOP(S(INTJ*)\\tO\\n,\\t,\\t*\\tO\\neverything\\tNN\\t(NP*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\ntop\\tJJ\\t(ADJP*\\tO\\nsecret\\tJJ\\t*))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'The\\tDT\\t(TOP(S(NP(NP*\\tO\\narea\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nHong\\tNNP\\t(NP*\\tB-GPE\\nKong\\tNNP\\t*)))\\tI-GPE\\nis\\tVBZ\\t(VP*\\tO\\nonly\\tRB\\t(NP(QP*\\tO\\none\\tCD\\t*\\tB-QUANTITY\\nthousand\\tCD\\t*\\tI-QUANTITY\\n-\\tHYPH\\t*\\tI-QUANTITY\\nplus\\tCC\\t*)\\tI-QUANTITY\\nsquare\\tJJ\\t*\\tI-QUANTITY\\nkilometers\\tNNS\\t*))\\tI-QUANTITY\\n.\\t.\\t*))\\tO',\n",
       " \"Here\\tRB\\t(TOP(S(ADVP*)\\tO\\n,\\t,\\t*\\tO\\ntourists\\tNNS\\t(NP*)\\tO\\ncan\\tMD\\t(VP(VP*\\tO\\nlearn\\tVB\\t(VP*\\tO\\nabout\\tIN\\t(PP*\\tO\\nHong\\tNNP\\t(NP(NP*\\tB-GPE\\nKong\\tNNP\\t*\\tI-GPE\\n's\\tPOS\\t*)\\tI-GPE\\nfilm\\tNN\\t*\\tO\\nhistory\\tNN\\t*))))\\tO\\n,\\t,\\t*\\tO\\nand\\tCC\\t*\\tO\\ncan\\tMD\\t(VP*\\tO\\ncome\\tVB\\t(VP*\\tO\\nup\\tRB\\t(ADVP*\\tO\\nclose\\tRB\\t*)\\tO\\nwith\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nstars\\tNNS\\t*))\\tO\\nin\\tIN\\t(PP*\\tO\\ntheir\\tPRP$\\t(NP*\\tO\\nmind\\tNN\\t*)))))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " \"The\\tDT\\t(TOP(S(NP*\\tO\\nanswer\\tNN\\t*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nthat\\tIN\\t(SBAR*\\tO\\nthis\\tDT\\t(S(NP*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\na\\tDT\\t(NP(NP(NP*\\tO\\nreward\\tNN\\t*)\\tO\\nto\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nentire\\tJJ\\t*\\tO\\nsociety\\tNN\\t*)))\\tO\\n,\\t,\\t*\\tO\\nnot\\tRB\\t*\\tO\\nan\\tDT\\t(NP(NP*\\tO\\nindividual\\tJJ\\t*\\tO\\nbuilding\\tNN\\t*\\tO\\n's\\tPOS\\t*)\\tO\\nreward\\tNN\\t*))))))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'Ha\\tUH\\t(TOP(SINV(INTJ*)\\tO\\n,\\t,\\t*\\tO\\non\\tIN\\t(PP*\\tO\\nthat\\tDT\\t(NP*\\tO\\nside\\tNN\\t*))\\tO\\nare\\tVBP\\t(VP*)\\tO\\nmany\\tJJ\\t(NP(NP*\\tO\\ntall\\tJJ\\t*\\tO\\nbuildings\\tNNS\\t*)\\tO\\n,\\t,\\t*\\tO\\nthat\\tRB\\t(ADVP*\\tO\\nis\\tRB\\t*)\\tO\\n,\\t,\\t*\\tO\\ntowns\\tNNS\\t(NP*\\tO\\nwith\\tIN\\t(PP*\\tO\\nlots\\tNNS\\t(NP(NP(NP*)\\tO\\nof\\tIN\\t(PP*\\tO\\npeople\\tNNS\\t(NP*)))\\tO\\nliving\\tVBG\\t(VP*\\tO\\nthere\\tRB\\t(ADVP*))))))\\tO\\n,\\t,\\t*))\\tO',\n",
       " \"They\\tPRP\\t(TOP(S(NP*)\\tO\\nwill\\tMD\\t(VP*\\tO\\ncomplete\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\npark\\tNN\\t*\\tO\\n's\\tPOS\\t*)\\tO\\nentire\\tJJ\\t*\\tO\\nconstruction\\tNN\\t*)\\tO\\nby\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tB-DATE\\nbeginning\\tNN\\t*)\\tI-DATE\\nof\\tIN\\t(PP*\\tI-DATE\\n2006\\tCD\\t(NP*))))\\tI-DATE\\n,\\t,\\t*\\tO\\nto\\tTO\\t(S(VP*\\tO\\nbe\\tVB\\t(VP*\\tO\\nable\\tJJ\\t(ADJP*\\tO\\nto\\tTO\\t(S(VP*\\tO\\nparticipate\\tVB\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\n2006\\tCD\\t*\\tB-DATE\\nDiscover\\tNNP\\t(NML(NML*\\tB-EVENT\\nHong\\tNNP\\t*\\tI-EVENT\\nKong\\tNNP\\t*)\\tI-EVENT\\nYear\\tNNP\\t*)\\tI-EVENT\\ncampaign\\tNN\\t*)))))))))))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'As\\tIN\\t(TOP(S(PP*\\tO\\na\\tDT\\t(NP*\\tO\\nresult\\tNN\\t*))\\tO\\n,\\t,\\t*\\tO\\nthis\\tDT\\t(NP*\\tO\\nplace\\tNN\\t*)\\tO\\nhas\\tVBZ\\t(VP*\\tO\\nbecome\\tVBN\\t(VP*\\tO\\nholy\\tJJ\\t(NP*\\tO\\nland\\tNN\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nhearts\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nmale\\tJJ\\t(NP(ADJP*\\tO\\nand\\tCC\\t*\\tO\\nfemale\\tJJ\\t*)\\tO\\ndevotees\\tNNS\\t*))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'Once\\tRB\\t(TOP(S(SBAR*\\tO\\nthe\\tDT\\t(S(NP*\\tO\\nZhuhai\\tNNP\\t(NML(NML*)\\tB-GPE\\n-\\tHYPH\\t*\\tO\\nHong\\tNNP\\t(NML*\\tO\\nKong\\tNNP\\t*)\\tO\\n-\\tHYPH\\t*\\tO\\nMacao\\tNNP\\t(NML*))\\tB-GPE\\nbridge\\tNN\\t*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nbuilt\\tVBN\\t(VP*))))\\tO\\n,\\t,\\t*\\tO\\nit\\tPRP\\t(NP(NP*))\\tO\\nwill\\tMD\\t(VP*\\tO\\nno\\tRB\\t(ADVP*\\tO\\nlonger\\tRB\\t*)\\tO\\nbe\\tVB\\t(VP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\ndream\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\ntourists\\tNNS\\t(NP*)))\\tO\\nto\\tTO\\t(S(VP*\\tO\\nenjoy\\tVB\\t(VP*\\tO\\ngourmet\\tJJ\\t(NP*\\tO\\nfood\\tNN\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\nMacao\\tNNP\\t(NP*))\\tB-GPE\\nbefore\\tIN\\t(PP*\\tO\\nhaving\\tVBG\\t(S(VP*\\tO\\nfun\\tNN\\t(NP*)\\tO\\nat\\tIN\\t(PP*\\tO\\nDisneyland\\tNNP\\t(NP*))\\tB-FAC\\njust\\tRB\\t(ADVP(NP(QP*\\tO\\nan\\tDT\\t*)\\tB-TIME\\nhour\\tNN\\t*)\\tI-TIME\\nlater\\tRB\\t*)))))))))\\tI-TIME\\n.\\t.\\t*))\\tO',\n",
       " 'However\\tRB\\t(TOP(S(ADVP*)\\tO\\n,\\t,\\t*\\tO\\neven\\tRB\\t(SBAR(ADVP*)\\tO\\nthough\\tIN\\t*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\nhas\\tVBZ\\t(VP*\\tO\\nentered\\tVBN\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nlast\\tJJ\\t*\\tO\\ncountdown\\tNN\\t*\\tO\\nperiod\\tNN\\t*)\\tO\\nbefore\\tIN\\t(PP*\\tO\\nits\\tPRP$\\t(NP*\\tO\\nfinal\\tJJ\\t*\\tO\\nopening\\tNN\\t*)))))))\\tO\\n,\\t,\\t*\\tO\\nthis\\tDT\\t(NP*\\tO\\nwonderland\\tNN\\t*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nstill\\tRB\\t(ADVP*)\\tO\\nunwilling\\tJJ\\t(ADJP*\\tO\\nto\\tTO\\t(S(VP*\\tO\\ntake\\tVB\\t(VP*\\tO\\noff\\tRP\\t(PRT*)\\tO\\nits\\tPRP$\\t(NP*\\tO\\nmysterious\\tJJ\\t*\\tO\\nveil\\tNN\\t*))))))\\tO\\n.\\t.\\t*))\\tO']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d115bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09dadc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ebb90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio-splits/test/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da1bf3a",
   "metadata": {},
   "source": [
    "### import data in CONLL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3174f407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|'s happening ther...|[{document, 0, 34...|[{document, 0, 34...|[{token, 0, 1, 's...|[{pos, 0, 1, VBZ,...|[{named_entity, 0...|\n",
      "|What exactly is t...|[{document, 0, 26...|[{document, 0, 26...|[{token, 0, 3, Wh...|[{pos, 0, 3, WP, ...|[{named_entity, 0...|\n",
      "|His platform incl...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 2, Hi...|[{pos, 0, 2, PRP$...|[{named_entity, 0...|\n",
      "|One example he gi...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 2, On...|[{pos, 0, 2, CD, ...|[{named_entity, 0...|\n",
      "|and then horrendo...|[{document, 0, 52...|[{document, 0, 52...|[{token, 0, 2, an...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|Very close to the...|[{document, 0, 39...|[{document, 0, 39...|[{token, 0, 3, Ve...|[{pos, 0, 3, RB, ...|[{named_entity, 0...|\n",
      "|Yutsai Primary Sc...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 5, Yu...|[{pos, 0, 5, NNP,...|[{named_entity, 0...|\n",
      "|No one has ever d...|[{document, 0, 20...|[{document, 0, 20...|[{token, 0, 1, No...|[{pos, 0, 1, DT, ...|[{named_entity, 0...|\n",
      "|            Uh-huh .|[{document, 0, 7,...|[{document, 0, 7,...|[{token, 0, 5, Uh...|[{pos, 0, 5, UH, ...|[{named_entity, 0...|\n",
      "|University of Med...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 9, Un...|[{pos, 0, 9, NNP,...|[{named_entity, 0...|\n",
      "|       Hell on earth|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 3, He...|[{pos, 0, 3, NN, ...|[{named_entity, 0...|\n",
      "|It does n't pay a...|[{document, 0, 57...|[{document, 0, 57...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|Director Janet Ch...|[{document, 0, 31...|[{document, 0, 31...|[{token, 0, 7, Di...|[{pos, 0, 7, NN, ...|[{named_entity, 0...|\n",
      "|Baldwin Technolog...|[{document, 0, 90...|[{document, 0, 90...|[{token, 0, 6, Ba...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
      "|Portals will be a...|[{document, 0, 71...|[{document, 0, 71...|[{token, 0, 6, Po...|[{pos, 0, 6, NNS,...|[{named_entity, 0...|\n",
      "|              Sure .|[{document, 0, 5,...|[{document, 0, 5,...|[{token, 0, 3, Su...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|President Chen al...|[{document, 0, 71...|[{document, 0, 71...|[{token, 0, 8, Pr...|[{pos, 0, 8, NNP,...|[{named_entity, 0...|\n",
      "|, so we all took ...|[{document, 0, 68...|[{document, 0, 68...|[{token, 0, 0, ,,...|[{pos, 0, 0, ,, {...|[{named_entity, 0...|\n",
      "|     -LRB- End -RRB-|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 4, -L...|[{pos, 0, 4, -LRB...|[{named_entity, 0...|\n",
      "|In more evidence ...|[{document, 0, 27...|[{document, 0, 27...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing the training set\n",
    "from sparknlp.training import CoNLL\n",
    "\n",
    "training_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio-splits/train/sample.train')\n",
    "training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80fc3b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|leaving Juilliard...|[{document, 0, 87...|[{document, 0, 87...|[{token, 0, 6, le...|[{pos, 0, 6, VBG,...|[{named_entity, 0...|\n",
      "|They 're suing 27...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 3, Th...|[{pos, 0, 3, PRP,...|[{named_entity, 0...|\n",
      "|Well , from this ...|[{document, 0, 27...|[{document, 0, 27...|[{token, 0, 3, We...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|Scientists have s...|[{document, 0, 25...|[{document, 0, 25...|[{token, 0, 9, Sc...|[{pos, 0, 9, NNS,...|[{named_entity, 0...|\n",
      "|Had they come to ...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 2, Ha...|[{pos, 0, 2, VBD,...|[{named_entity, 0...|\n",
      "|      No I 'm not /.|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 1, No...|[{pos, 0, 1, UH, ...|[{named_entity, 0...|\n",
      "|In these respects...|[{document, 0, 79...|[{document, 0, 79...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|The economic cons...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|`` I 'll stay wit...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 1, ``...|[{pos, 0, 1, ``, ...|[{named_entity, 0...|\n",
      "|Mr. Luzon and his...|[{document, 0, 75...|[{document, 0, 75...|[{token, 0, 2, Mr...|[{pos, 0, 2, NNP,...|[{named_entity, 0...|\n",
      "|Can the wax apple...|[{document, 0, 79...|[{document, 0, 79...|[{token, 0, 2, Ca...|[{pos, 0, 2, MD, ...|[{named_entity, 0...|\n",
      "|Uh um uh I 'm six...|[{document, 0, 31...|[{document, 0, 31...|[{token, 0, 1, Uh...|[{pos, 0, 1, UH, ...|[{named_entity, 0...|\n",
      "|It 's a very nice...|[{document, 0, 24...|[{document, 0, 24...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|              yeah .|[{document, 0, 5,...|[{document, 0, 5,...|[{token, 0, 3, ye...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|no Tutsis and Hut...|[{document, 0, 29...|[{document, 0, 29...|[{token, 0, 1, no...|[{pos, 0, 1, DT, ...|[{named_entity, 0...|\n",
      "|Compared to many ...|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 7, Co...|[{pos, 0, 7, VBN,...|[{named_entity, 0...|\n",
      "|When talking abou...|[{document, 0, 22...|[{document, 0, 22...|[{token, 0, 3, Wh...|[{pos, 0, 3, WRB,...|[{named_entity, 0...|\n",
      "|The administratio...|[{document, 0, 24...|[{document, 0, 24...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|how big a place i...|[{document, 0, 30...|[{document, 0, 30...|[{token, 0, 2, ho...|[{pos, 0, 2, WRB,...|[{named_entity, 0...|\n",
      "|WEIRTON STEEL Cor...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 6, WE...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio-splits/dev/sample.train')\n",
    "dev_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7e4b286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|subway to Disney ...|[{document, 0, 46...|[{document, 0, 46...|[{token, 0, 5, su...|[{pos, 0, 5, NN, ...|[{named_entity, 0...|\n",
      "|Ah , everything i...|[{document, 0, 30...|[{document, 0, 30...|[{token, 0, 1, Ah...|[{pos, 0, 1, UH, ...|[{named_entity, 0...|\n",
      "|The area of Hong ...|[{document, 0, 68...|[{document, 0, 68...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Here , tourists c...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 3, He...|[{pos, 0, 3, RB, ...|[{named_entity, 0...|\n",
      "|The answer is tha...|[{document, 0, 97...|[{document, 0, 97...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Ha , on that side...|[{document, 0, 93...|[{document, 0, 93...|[{token, 0, 1, Ha...|[{pos, 0, 1, UH, ...|[{named_entity, 0...|\n",
      "|They will complet...|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 3, Th...|[{pos, 0, 3, PRP,...|[{named_entity, 0...|\n",
      "|As a result , thi...|[{document, 0, 88...|[{document, 0, 88...|[{token, 0, 1, As...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|Once the Zhuhai -...|[{document, 0, 17...|[{document, 0, 17...|[{token, 0, 3, On...|[{pos, 0, 3, RB, ...|[{named_entity, 0...|\n",
      "|However , even th...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 6, Ho...|[{pos, 0, 6, RB, ...|[{named_entity, 0...|\n",
      "|Oh ! I had n't th...|[{document, 0, 34...|[{document, 0, 34...|[{token, 0, 1, Oh...|[{pos, 0, 1, UH, ...|[{named_entity, 0...|\n",
      "|he had me check t...|[{document, 0, 52...|[{document, 0, 52...|[{token, 0, 1, he...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|       Did you see ?|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 2, Di...|[{pos, 0, 2, VBD,...|[{named_entity, 0...|\n",
      "|               Yes .|[{document, 0, 4,...|[{document, 0, 4,...|[{token, 0, 2, Ye...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|That year Ye Dayi...|[{document, 0, 45...|[{document, 0, 45...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|So the kids then ...|[{document, 0, 94...|[{document, 0, 94...|[{token, 0, 1, So...|[{pos, 0, 1, RB, ...|[{named_entity, 0...|\n",
      "|                Mm .|[{document, 0, 3,...|[{document, 0, 3,...|[{token, 0, 1, Mm...|[{pos, 0, 1, UH, ...|[{named_entity, 0...|\n",
      "|From digging the ...|[{document, 0, 59...|[{document, 0, 59...|[{token, 0, 3, Fr...|[{pos, 0, 3, IN, ...|[{named_entity, 0...|\n",
      "|So you picked up ...|[{document, 0, 44...|[{document, 0, 44...|[{token, 0, 1, So...|[{pos, 0, 1, RB, ...|[{named_entity, 0...|\n",
      "|Did you crack som...|[{document, 0, 36...|[{document, 0, 36...|[{token, 0, 2, Di...|[{pos, 0, 2, VBD,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio-splits/test/sample.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e54ee7",
   "metadata": {},
   "source": [
    "### Training the model - 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5203e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base_cased download started this may take some time.\n",
      "Approximate size to download 389.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# use bert embeddings\n",
    "bert = BertEmbeddings.pretrained('bert_base_cased', 'en').setInputCols([\"sentence\",'token']).setOutputCol(\"bert\").setCaseSensitive(True)#.setMaxSentenceLength(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1f794d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62138"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at the shape - using count as its not very large dataset\n",
    "training_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d0c34ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the training data into embeddings and saving it as parquet files\n",
    "readyTrainingData = bert.transform(training_data)\n",
    "\n",
    "readyTrainingData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab96caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "readyTrainingData = spark.read.parquet(\"/tmp/conll2003/bert_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c4f2096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the development data into embeddings and saving it as parquet files\n",
    "readyDevData = bert.transform(dev_data)\n",
    "\n",
    "readyDevData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0c284d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "readyDevData = spark.read.parquet(\"/tmp/conll2003/bert_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d64dac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa34583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "readyTestData = spark.read.parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0897336",
   "metadata": {},
   "source": [
    "### Dev set - Fold 2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7bd2f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize NER tagger\n",
    "nerTagger = NerDLApproach()\\\n",
    ".setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
    ".setLabelColumn(\"label\")\\\n",
    ".setOutputCol(\"ner\")\\\n",
    ".setMaxEpochs(10)\\\n",
    ".setBatchSize(4)\\\n",
    ".setEnableMemoryOptimizer(True)\\\n",
    ".setRandomSeed(0)\\\n",
    ".setVerbose(1)\\\n",
    ".setValidationSplit(0.2)\\\n",
    ".setEvaluationLogExtended(True)\\\n",
    ".setEnableOutputLogs(True)\\\n",
    ".setIncludeConfidence(True)\\\n",
    ".setTestDataset(\"/tmp/conll2003/bert_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c9418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "%time myNerModel = nerTagger.fit(readyTrainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327a58c4",
   "metadata": {},
   "source": [
    "### Testing on Fold 1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "822a465e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.94 s, sys: 658 ms, total: 4.6 s\n",
      "Wall time: 32 s\n"
     ]
    }
   ],
   "source": [
    "# infer from the trained model\n",
    "%time results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c24e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c50bb9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[2465, 2673, 3216, 3742, 3842, 7121]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5da5e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a198acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "275e30d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9816502638710598\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "445fbe5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9006251749556778\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cdac3a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.84      0.88      0.86       934\n",
      "        DATE       0.86      0.90      0.88      1367\n",
      "       EVENT       0.76      0.68      0.72       103\n",
      "         FAC       0.61      0.76      0.68       103\n",
      "         GPE       0.95      0.96      0.95      2031\n",
      "    LANGUAGE       0.93      0.89      0.91        45\n",
      "         LAW       0.65      0.71      0.68        42\n",
      "         LOC       0.88      0.70      0.78       193\n",
      "       MONEY       0.91      0.92      0.91       304\n",
      "        NORP       0.96      0.93      0.95       892\n",
      "     ORDINAL       0.85      0.91      0.88       204\n",
      "         ORG       0.88      0.89      0.89      1690\n",
      "     PERCENT       0.92      0.92      0.92       231\n",
      "      PERSON       0.93      0.96      0.94      2027\n",
      "     PRODUCT       0.76      0.76      0.76        67\n",
      "    QUANTITY       0.84      0.78      0.81        86\n",
      "        TIME       0.65      0.70      0.67       169\n",
      " WORK_OF_ART       0.76      0.68      0.72       131\n",
      "\n",
      "   micro avg       0.89      0.91      0.90     10619\n",
      "   macro avg       0.83      0.83      0.83     10619\n",
      "weighted avg       0.89      0.91      0.90     10619\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcd65d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
