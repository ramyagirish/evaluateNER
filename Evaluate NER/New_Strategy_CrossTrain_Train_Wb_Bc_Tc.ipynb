{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1edef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  3.1.0\n",
      "Apache Spark version:  3.1.2\n"
     ]
    }
   ],
   "source": [
    "# call relevant packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "import sparknlp\n",
    "\n",
    "# to use GPU \n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb98346",
   "metadata": {},
   "source": [
    "## Creating a composite CoNLL file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4910c848",
   "metadata": {},
   "source": [
    "### Create training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "024205af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training file with news conversation\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/onto.wb.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85935111",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "769deee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today\\tNN\\t(TOP(S(NP*)\\tO\\non\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nMesopotamia\\tNNP\\t*\\tB-ORG\\nchannel\\tNN\\t*))\\tO\\nthey\\tPRP\\t(NP*)\\tO\\nbrought\\tVBD\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nphoto\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nSaddam\\tNNP\\t(NP(NP*)\\tB-PERSON\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\ncoffin\\tNN\\t*))\\tO\\nbefore\\tIN\\t(PP*\\tO\\nburial\\tNN\\t(NP*))))))\\tO\\nThe\\tDT\\t(FRAG(NP(NP*\\tB-PERSON\\ncoast\\tNN\\t*)\\tI-PERSON\\nof\\tIN\\t(PP*\\tI-PERSON\\nJeddah\\tNNP\\t(NP*))))))\\tI-PERSON',\n",
       " 'The\\tDT\\t(TOP(S(NP(NP*\\tO\\nphoto\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nSaddam\\tNNP\\t(NP(NP*)\\tB-PERSON\\nminutes\\tNNS\\t(PP(NP*)\\tO\\nbefore\\tIN\\t*\\tO\\nthe\\tDT\\t(NP*\\tO\\nburial\\tNN\\t*)))))\\tO\\nwas\\tVBD\\t(VP*\\tO\\nclear\\tJJ\\t(ADJP*))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'It\\tPRP\\t(TOP(S(NP*)\\tO\\nwas\\tVBD\\t(VP*\\tO\\nas\\tIN\\t(SBAR*\\tO\\nwe\\tPRP\\t(S(NP*)\\tO\\nknow\\tVBP\\t(VP*\\tO\\nhim\\tPRP\\t(NP*))))\\tO\\n,\\t,\\t*\\tO\\nexcept\\tIN\\t(PP*\\tO\\nthat\\tIN\\t(SBAR*\\tO\\nhis\\tPRP$\\t(S(NP*\\tO\\nface\\tNN\\t*)\\tO\\nwas\\tVBD\\t(VP*\\tO\\nwhiter\\tJJR\\t(ADJP(ADJP*)\\tO\\nthan\\tIN\\t(SBAR*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\nwas\\tVBD\\t(VP*\\tO\\nduring\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\ntrial\\tNN\\t*)))))))))))\\tO\\n...\\t.\\t*))\\tO',\n",
       " 'The\\tDT\\t(TOP(S(NP(NP*\\tO\\nperson\\tNN\\t*)\\tO\\nwho\\tWP\\t(SBAR(WHNP*)\\tO\\nlifted\\tVBD\\t(S(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\ncover\\tNN\\t*)\\tO\\noff\\tIN\\t(PP*\\tO\\nhis\\tPRP$\\t(NP*\\tO\\nface\\tNN\\t*))))))\\tO\\nkissed\\tVBD\\t(VP*\\tO\\nhim\\tPRP\\t(NP*)\\tO\\n-LRB-\\t-LRB-\\t(PRN*\\tO\\nmay\\tMD\\t(SINV*\\tO\\nGod\\tNNP\\t(NP*)\\tO\\nhave\\tVBP\\t(VP*\\tO\\nmercy\\tNN\\t(NP*)\\tO\\non\\tIN\\t(PP*\\tO\\nhim\\tPRP\\t(NP*))))\\tO\\n-RRB-\\t-RRB-\\t*))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'My\\tPRP$\\t(TOP(NP*\\tO\\nregards\\tNNS\\t*\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'No\\tRB\\t(TOP(S(ADVP*\\tO\\nmatter\\tRB\\t*\\tO\\nhow\\tWRB\\t(SBAR(WHADVP*\\tO\\nmuch\\tRB\\t*)\\tO\\nwe\\tPRP\\t(S(NP*)\\tO\\nmay\\tMD\\t(VP*\\tO\\ndisagree\\tVB\\t(VP*)))))\\tO\\ncivility\\tNN\\t(NP*)\\tO\\nremains\\tVBZ\\t(VP*\\tO\\nbetween\\tIN\\t(PP*\\tO\\nus\\tPRP\\t(NP*)))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'Peppermint\\tNNP\\t(TOP(FRAG(NP*)))\\tB-PERSON',\n",
       " 'May\\tMD\\t(TOP(SINV*\\tO\\nGod\\tNNP\\t(NP*)\\tO\\nhave\\tVB\\t(VP*\\tO\\nmercy\\tNN\\t(NP*)\\tO\\non\\tIN\\t(PP*\\tO\\nhim\\tPRP\\t(NP*)))\\tO\\n..\\t.\\t*))\\tO',\n",
       " 'Thank\\tVBP\\t(TOP(S(VP*\\tO\\nyou\\tPRP\\t(NP*)\\tO\\n,\\t,\\t*\\tO\\nbrother\\tNN\\t(NP*))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'The\\tDT\\t(TOP(NP(NP*\\tB-PERSON\\nCoast\\tNNP\\t*)\\tI-PERSON\\nof\\tIN\\t(PP*\\tI-PERSON\\nJeddah\\tNNP\\t(NP*))\\tI-PERSON\\n.\\t.\\t*))\\tI-PERSON']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f341e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a5589da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41459c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding telephonic conversation files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/onto.tc.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84dedece",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f31fbf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding weblog files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/onto.wb.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d711f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41e0b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa11d3b4",
   "metadata": {},
   "source": [
    "### Create dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "488f800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dev file with all the news corpus\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/development/onto.bn.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc5fc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3f11a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The\\tDT\\t(TOP(S(NP(NP*\\tO\\nIsraeli\\tJJ\\t*\\tB-NORP\\nPrime\\tNNP\\t*\\tO\\nMinister\\tNNP\\t*)\\tO\\nEhud\\tNNP\\t(NP*\\tB-PERSON\\nBarak\\tNNP\\t*))\\tI-PERSON\\nis\\tVBZ\\t(VP*\\tO\\ndue\\tJJ\\t(ADJP*\\tO\\nto\\tTO\\t(S(VP*\\tO\\nmeet\\tVB\\t(VP*\\tO\\nwith\\tIN\\t(PP*\\tO\\nEgypt\\tNNP\\t(NP(NP*\\tB-GPE\\n's\\tPOS\\t*)\\tO\\nPresident\\tNNP\\t*\\tO\\nMubarak\\tNNP\\t*))\\tB-PERSON\\nin\\tIN\\t(PP*\\tO\\nCairo\\tNNP\\t(NP*))\\tB-GPE\\non\\tIN\\t(PP*\\tO\\nThursday\\tNNP\\t(NP*)))))))\\tB-DATE\\n.\\t.\\t*))\\tO\",\n",
       " \"The\\tDT\\t(TOP(S(NP*\\tO\\nencounter\\tNN\\t*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nbeing\\tVBG\\t(VP*\\tO\\nseen\\tVBN\\t(VP*\\tO\\nas\\tIN\\t(PP*\\tO\\nan\\tDT\\t(NP(NP*\\tO\\neffort\\tNN\\t*)\\tO\\nby\\tIN\\t(PP*\\tO\\nBarak\\tNNP\\t(NP*))\\tB-PERSON\\nto\\tTO\\t(S(VP*\\tO\\ndrum\\tVB\\t(VP*\\tO\\nup\\tRP\\t(PRT*)\\tO\\nregional\\tJJ\\t(NP(NP*\\tO\\nsupport\\tNN\\t*)\\tO\\nfor\\tIN\\t(PP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nMiddle\\tNNP\\t(NML*\\tB-LOC\\nEast\\tNNP\\t*)\\tI-LOC\\npeace\\tNN\\t*\\tO\\nagreement\\tNN\\t*)\\tO\\nbased\\tVBN\\t(VP*\\tO\\non\\tIN\\t(PP*\\tO\\nPresident\\tNNP\\t(NP(NP*\\tO\\nClinton\\tNNP\\t*\\tB-PERSON\\n's\\tPOS\\t*)\\tO\\nproposals\\tNNS\\t*))))))))))))))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'They\\tPRP\\t(TOP(S(S(NP*)\\tO\\nrequire\\tVBP\\t(VP(VP*\\tO\\nIsrael\\tNNP\\t(NP*)\\tB-GPE\\nto\\tTO\\t(S(VP*\\tO\\nrelinquish\\tVB\\t(VP*\\tO\\nsovereignty\\tNN\\t(NP(NP*)\\tO\\nover\\tIN\\t(PP*\\tO\\nmost\\tJJS\\t(NP(NP*)\\tO\\nof\\tIN\\t(PP*\\tO\\nArab\\tJJ\\t(NP*\\tB-NORP\\nEast\\tNNP\\t*\\tB-GPE\\nJerusalem\\tNNP\\t*)))))))))\\tI-GPE\\nand\\tCC\\t*\\tO\\nthe\\tDT\\t(VP(NP*\\tO\\nPalestinians\\tNNPS\\t*)\\tB-NORP\\nto\\tTO\\t(S(VP*\\tO\\nscale\\tVB\\t(VP*\\tO\\nback\\tRP\\t(PRT*)\\tO\\ntheir\\tPRP$\\t(NP(NP*\\tO\\ndemand\\tNN\\t*)\\tO\\nfor\\tIN\\t(PP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nreturn\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nrefugees\\tNNS\\t(NP*)))))))))))\\tO\\n,\\t,\\t*\\tO\\nbut\\tCC\\t*\\tO\\nPalestinian\\tJJ\\t(S(NP(NML*\\tB-NORP\\nchief\\tNN\\t*\\tO\\nnegotiator\\tNN\\t*)\\tO\\nSayeb\\tNNP\\t*\\tB-PERSON\\nErekat\\tNNP\\t*)\\tI-PERSON\\nsays\\tVBZ\\t(VP*\\tO\\nmore\\tJJR\\t(SBAR(S(NP*\\tO\\ndetails\\tNNS\\t*)\\tO\\nare\\tVBP\\t(VP*\\tO\\nrequired\\tVBN\\t(VP*\\tO\\nbefore\\tIN\\t(SBAR*\\tO\\nboth\\tDT\\t(S(NP*\\tO\\nsides\\tNNS\\t*)\\tO\\ncan\\tMD\\t(VP*\\tO\\nstrike\\tVB\\t(VP*\\tO\\na\\tDT\\t(NP*\\tO\\ndeal\\tNN\\t*)))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'We\\tPRP\\t(TOP(S(NP*)\\tO\\nhave\\tVBP\\t(VP*\\tO\\nso\\tRB\\t(NP(NP(NP(ADJP*\\tO\\nmany\\tJJ\\t*)\\tO\\nquestions\\tNNS\\t*)\\tO\\nthat\\tWDT\\t(SBAR(WHNP*)\\tO\\nwe\\tPRP\\t(S(NP*)\\tO\\nhave\\tVBP\\t(VP*))))\\tO\\n...\\t:\\t*\\tO\\nso\\tRB\\t(NP(NP(ADJP*\\tO\\nmany\\tJJ\\t*)\\tO\\npoints\\tNNS\\t*)\\tO\\nthat\\tWDT\\t(SBAR(WHNP*)\\tO\\nneed\\tVBP\\t(S(VP*\\tO\\nto\\tTO\\t(S(VP*\\tO\\nbe\\tVB\\t(VP*\\tO\\nclarified\\tVBN\\t(VP*))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'The\\tDT\\t(TOP(S(NP*\\tO\\nmost\\tRBS\\t(ADJP*\\tO\\nimportant\\tJJ\\t*)\\tO\\nthing\\tNN\\t*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nclarity\\tNN\\t*))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'The\\tDT\\t(TOP(S(NP*\\tO\\nmost\\tRBS\\t(ADJP*\\tO\\nimportant\\tJJ\\t*)\\tO\\nthing\\tNN\\t*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nthat\\tIN\\t(SBAR*\\tO\\nwe\\tPRP\\t(S(NP*)\\tO\\nfocus\\tVBP\\t(VP*\\tO\\non\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\ntravails\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\npeace\\tNN\\t*\\tO\\nprocess\\tNN\\t*))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'Israeli\\tJJ\\t(TOP(S(NP*\\tB-NORP\\nsettlers\\tNNS\\t*)\\tO\\nare\\tVBP\\t(VP*\\tO\\nworried\\tJJ\\t(ADJP*\\tO\\nthat\\tIN\\t(SBAR*\\tO\\nmost\\tJJS\\t(S(NP(NP*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nsettlements\\tNNS\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tB-GPE\\nWest\\tNNP\\t*\\tI-GPE\\nBank\\tNNP\\t*)))))\\tI-GPE\\nwill\\tMD\\t(VP*\\tO\\nbe\\tVB\\t(VP*\\tO\\ndismantled\\tVBN\\t(VP*\\tO\\nif\\tIN\\t(SBAR*\\tO\\nan\\tDT\\t(S(NP*\\tO\\nagreement\\tNN\\t*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nreached\\tVBN\\t(VP*)))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " \"The\\tDT\\t(TOP(S(NP(NP(NP*\\tO\\nspokeswoman\\tNN\\t*)\\tO\\nfor\\tIN\\t(PP*\\tO\\nthis\\tDT\\t(NP(NP*\\tB-ORG\\nSettlers\\tNNPS\\t*\\tI-ORG\\n'\\tPOS\\t*)\\tI-ORG\\nCouncil\\tNNP\\t*)))\\tI-ORG\\nYahudi\\tNNP\\t(NP*\\tB-PERSON\\nTayar\\tNNP\\t*))\\tI-PERSON\\nsays\\tVBZ\\t(VP*\\tO\\nthis\\tDT\\t(SBAR(S(NP*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nunacceptable\\tJJ\\t(ADJP*)))))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'We\\tPRP\\t(TOP(S(NP*)\\tO\\nhave\\tVBP\\t(VP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nsituation\\tNN\\t*)\\tO\\nwhere\\tWRB\\t(SBAR(WHADVP*)\\tO\\ninstead\\tRB\\t(S(PP*\\tO\\nof\\tIN\\t*\\tO\\nprotecting\\tVBG\\t(S(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\ninterests\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nJewish\\tJJ\\t*\\tB-NORP\\npeople\\tNNS\\t*))))))\\tO\\n,\\t,\\t*\\tO\\nthe\\tDT\\t(NP*\\tO\\nIsraeli\\tJJ\\t*\\tB-NORP\\nPrime\\tNNP\\t*\\tO\\nMinister\\tNNP\\t*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nwilling\\tJJ\\t(ADJP*\\tO\\nto\\tTO\\t(S(VP*\\tO\\nforgo\\tVB\\t(VP*\\tO\\non\\tIN\\t(PP*\\tO\\nall\\tDT\\t(NP(NP(NP*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nvital\\tJJ\\t(ADJP*\\tO\\nand\\tCC\\t*\\tO\\nimportant\\tJJ\\t*)\\tO\\nareas\\tNNS\\t*)))\\tO\\nand\\tCC\\t*\\tO\\nnot\\tRB\\t(NP(CONJP*\\tO\\nonly\\tRB\\t*)\\tO\\nour\\tPRP$\\t(NP*\\tO\\nhistory\\tNN\\t*)\\tO\\n,\\t,\\t*\\tO\\nbut\\tCC\\t*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nsecurity\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nfuture\\tNN\\t*))\\tO\\nfor\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nJewish\\tJJ\\t*\\tB-NORP\\npeople\\tNNS\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\nIsrael\\tNNP\\t(NP*)))))))))))))))))\\tB-GPE\\n.\\t.\\t*))\\tO',\n",
       " 'More\\tJJR\\t(TOP(S(NP(QP*\\tB-CARDINAL\\nthan\\tIN\\t*\\tI-CARDINAL\\n300\\tCD\\t*)\\tI-CARDINAL\\npeople\\tNNS\\t*)\\tO\\nhave\\tVBP\\t(VP*\\tO\\nbeen\\tVBN\\t(VP*\\tO\\nkilled\\tVBN\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nWest\\tNNP\\t(NML(NML*\\tB-GPE\\nBank\\tNNP\\t*)\\tI-GPE\\nand\\tCC\\t*\\tO\\nGaza\\tNNP\\t(NML*\\tB-GPE\\nStrip\\tNNP\\t*))))\\tI-GPE\\nsince\\tIN\\t(PP*\\tO\\nSeptember\\tNNP\\t(NP*)))))\\tB-DATE\\n.\\t.\\t*))\\tO']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5e15110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7901892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56fb2402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/development/onto.mz.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2c60902",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bad97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/development/onto.nw.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa9ca65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d997328",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/development/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc40ca6",
   "metadata": {},
   "source": [
    "### Create test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b203802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training filJe with all the news corpus\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.bn.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "556a0e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c848103b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Iraqi\\tJJ\\t(TOP(S(NP(NML*\\tB-NORP\\nleader\\tNN\\t*)\\tO\\nSaddam\\tNNP\\t*\\tB-PERSON\\nHussein\\tNNP\\t*)\\tI-PERSON\\nhas\\tVBZ\\t(VP*\\tO\\ngiven\\tVBN\\t(VP*\\tO\\na\\tDT\\t(NP*\\tO\\ndefiant\\tJJ\\t*\\tO\\nspeech\\tNN\\t*)\\tO\\nto\\tTO\\t(S(VP*\\tO\\nmark\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\ntenth\\tJJ\\t*\\tB-ORDINAL\\nanniversary\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tB-EVENT\\nGulf\\tNNP\\t*\\tI-EVENT\\nWar\\tNNP\\t*))))))))\\tI-EVENT\\n.\\t.\\t*))\\tO',\n",
       " 'He\\tPRP\\t(TOP(S(NP*)\\tO\\nsays\\tVBZ\\t(VP*\\tO\\nIraq\\tNNP\\t(SBAR(S(NP*)\\tB-GPE\\nhas\\tVBZ\\t(VP*\\tO\\ntriumphed\\tVBN\\t(VP*\\tO\\nover\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nevil\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nWest\\tNNP\\t*)))))))))\\tB-LOC\\n.\\t.\\t*))\\tO',\n",
       " 'Barbara\\tNNP\\t(TOP(S(NP*\\tB-PERSON\\nPlett\\tNNP\\t*)\\tI-PERSON\\nreports\\tVBZ\\t(VP*\\tO\\nfrom\\tIN\\t(PP*\\tO\\nBaghdad\\tNNP\\t(NP*)))\\tB-GPE\\n.\\t.\\t*))\\tO',\n",
       " 'Saddam\\tNNP\\t(TOP(S(NP*\\tB-PERSON\\nHussein\\tNNP\\t*)\\tI-PERSON\\naddressed\\tVBD\\t(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nnation\\tNN\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nspeech\\tNN\\t*)\\tO\\nfilled\\tVBN\\t(VP*\\tO\\nwith\\tIN\\t(PP*\\tO\\nrhetoric\\tNN\\t(NP(NP*)\\tO\\nand\\tCC\\t*\\tO\\na\\tDT\\t(NP(NP*\\tO\\ndeclaration\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nvictory\\tNN\\t(NP*)))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " \"``\\t``\\t(TOP(S*\\tO\\nIraq\\tNNP\\t(S(NP*)\\tB-GPE\\nhas\\tVBZ\\t(VP*\\tO\\ntriumphed\\tVBN\\t(VP*\\tO\\nover\\tIN\\t(PP*\\tO\\nits\\tPRP$\\t(NP*\\tO\\nenemies\\tNNS\\t*)))))\\tO\\n,\\t,\\t*\\tO\\n''\\t''\\t*\\tO\\nhe\\tPRP\\t(PRN(S(NP*)\\tO\\nsaid\\tVBD\\t(VP*)))\\tO\\n,\\t,\\t*\\tO\\n``\\t``\\t*\\tO\\nand\\tCC\\t*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\nwill\\tMD\\t(VP*\\tO\\ntriumph\\tVB\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nremaining\\tVBG\\t*\\tO\\nrounds\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nbattle\\tNN\\t*)))))))\\tO\\n.\\t.\\t*\\tO\\n''\\t''\\t*))\\tO\",\n",
       " 'The\\tDT\\t(TOP(S(NP*\\tO\\nIraqi\\tJJ\\t*\\tB-NORP\\nleader\\tNN\\t*)\\tO\\nsaid\\tVBD\\t(VP*\\tO\\nthe\\tDT\\t(SBAR(S(NP*\\tB-EVENT\\nGulf\\tNNP\\t*\\tI-EVENT\\nWar\\tNNP\\t*)\\tI-EVENT\\nwas\\tVBD\\t(VP*\\tO\\na\\tDT\\t(NP(NP(NP*\\tO\\nconfrontation\\tNN\\t*)\\tO\\nbetween\\tIN\\t(PP*\\tO\\ngood\\tNN\\t(NP*\\tO\\nand\\tCC\\t*\\tO\\nevil\\tNN\\t*)))\\tO\\nthat\\tWDT\\t(SBAR(WHNP*)\\tO\\ncontinues\\tVBZ\\t(S(VP*\\tO\\nto\\tIN\\t(PP*\\tO\\nthis\\tDT\\t(NP*\\tO\\nday\\tNN\\t*))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'He\\tPRP\\t(TOP(S(NP*)\\tO\\nwas\\tVBD\\t(VP*\\tO\\nreferring\\tVBG\\t(VP*\\tO\\nto\\tIN\\t(PP*\\tO\\nhis\\tPRP$\\t(NP(NP*\\tO\\nstruggle\\tNN\\t*)\\tO\\nagainst\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nUS\\tNNP\\t*)\\tB-GPE\\n,\\t,\\t*\\tO\\nwhich\\tWDT\\t(SBAR(WHNP*)\\tO\\nstrongly\\tRB\\t(S(ADVP*)\\tO\\nsupports\\tVBZ\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\neconomic\\tJJ\\t*\\tO\\nembargo\\tNN\\t*)\\tO\\nmeant\\tVBN\\t(VP*\\tO\\nto\\tTO\\t(S(VP*\\tO\\nforce\\tVB\\t(VP*\\tO\\nhim\\tPRP\\t(NP*)\\tO\\nto\\tTO\\t(S(VP*\\tO\\ndisarm\\tVB\\t(VP*)))))))))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'After\\tIN\\t(TOP(S(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nspeech\\tNN\\t*))\\tO\\ndemonstrators\\tNNS\\t(NP*)\\tO\\ngathered\\tVBD\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\npublic\\tJJ\\t*\\tO\\nshow\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nsupport\\tNN\\t(NP(NP*)\\tO\\nfor\\tIN\\t(PP*\\tO\\ntheir\\tPRP$\\t(NP*\\tO\\nPresident\\tNNP\\t*)))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'A\\tDT\\t(TOP(S(NP(NP*\\tO\\nsense\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nnationalism\\tNN\\t(NP*)))\\tO\\nhas\\tVBZ\\t(VP*\\tO\\ngrown\\tVBN\\t(VP*\\tO\\nstronger\\tJJR\\t(ADJP*)\\tO\\nhere\\tRB\\t(ADVP*)\\tO\\nas\\tIN\\t(SBAR*\\tO\\nmany\\tJJ\\t(S(NP*\\tO\\npeople\\tNNS\\t*)\\tO\\nrally\\tVBP\\t(VP*\\tO\\nagainst\\tIN\\t(PP*\\tO\\nwhat\\tWP\\t(SBAR(WHNP*)\\tO\\nthey\\tPRP\\t(S(NP*)\\tO\\nsay\\tVBP\\t(VP*\\tO\\nis\\tVBZ\\t(SBAR(S(VP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nsiege\\tNN\\t*)\\tO\\non\\tIN\\t(PP*\\tO\\ntheir\\tPRP$\\t(NP*\\tO\\ncountry\\tNN\\t*)))))))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'Barbara\\tNNP\\t(TOP(FRAG(NP*\\tB-PERSON\\nPlett\\tNNP\\t*)\\tI-PERSON\\n,\\t,\\t*\\tO\\nBBC\\tNNP\\t(NP*\\tB-ORG\\nNews\\tNNP\\t*)\\tI-ORG\\n,\\t,\\t*\\tO\\nBaghdad\\tNNP\\t(NP*)\\tB-GPE\\n.\\t.\\t*))\\tO']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d115bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09dadc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "586b9172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.mz.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30c753da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33682301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.nw.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "711d912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ebb90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da1bf3a",
   "metadata": {},
   "source": [
    "### import data in CONLL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3174f407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Today on the Meso...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 4, To...|[{pos, 0, 4, NN, ...|[{named_entity, 0...|\n",
      "|The photo of Sadd...|[{document, 0, 56...|[{document, 0, 56...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|It was as we know...|[{document, 0, 87...|[{document, 0, 87...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|The person who li...|[{document, 0, 94...|[{document, 0, 94...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|        My regards .|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 1, My...|[{pos, 0, 1, PRP$...|[{named_entity, 0...|\n",
      "|No matter how muc...|[{document, 0, 63...|[{document, 0, 63...|[{token, 0, 1, No...|[{pos, 0, 1, RB, ...|[{named_entity, 0...|\n",
      "|          Peppermint|[{document, 0, 9,...|[{document, 0, 9,...|[{token, 0, 9, Pe...|[{pos, 0, 9, NNP,...|[{named_entity, 0...|\n",
      "|May God have merc...|[{document, 0, 27...|[{document, 0, 27...|[{token, 0, 2, Ma...|[{pos, 0, 2, MD, ...|[{named_entity, 0...|\n",
      "|Thank you , broth...|[{document, 0, 20...|[{document, 0, 20...|[{token, 0, 4, Th...|[{pos, 0, 4, VBP,...|[{named_entity, 0...|\n",
      "|The Coast of Jedd...|[{document, 0, 20...|[{document, 0, 20...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The Shiites are t...|[{document, 0, 41...|[{document, 0, 41...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Yes , but it is t...|[{document, 0, 45...|[{document, 0, 45...|[{token, 0, 2, Ye...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|May every Bairam ...|[{document, 0, 67...|[{document, 0, 67...|[{token, 0, 2, Ma...|[{pos, 0, 2, MD, ...|[{named_entity, 0...|\n",
      "|n33na33@hotmail.c...|[{document, 0, 20...|[{document, 0, 20...|[{token, 0, 18, n...|[{pos, 0, 18, ADD...|[{named_entity, 0...|\n",
      "|       I am the Arab|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|I hope one of the...|[{document, 0, 50...|[{document, 0, 50...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|         -----------|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 10, -...|[{pos, 0, 10, NFP...|[{named_entity, 0...|\n",
      "|          A free pen|[{document, 0, 9,...|[{document, 0, 9,...|[{token, 0, 0, A,...|[{pos, 0, 0, DT, ...|[{named_entity, 0...|\n",
      "|ana_free11@hotmai...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 21, a...|[{pos, 0, 21, ADD...|[{named_entity, 0...|\n",
      "|Concern destroys ...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 6, Co...|[{pos, 0, 6, NN, ...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing the training set\n",
    "from sparknlp.training import CoNLL\n",
    "\n",
    "training_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/train/sample.train')\n",
    "training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80fc3b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|The Israeli Prime...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The encounter is ...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|They require Isra...|[{document, 0, 26...|[{document, 0, 26...|[{token, 0, 3, Th...|[{pos, 0, 3, PRP,...|[{named_entity, 0...|\n",
      "|We have so many q...|[{document, 0, 84...|[{document, 0, 84...|[{token, 0, 1, We...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|The most importan...|[{document, 0, 40...|[{document, 0, 40...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The most importan...|[{document, 0, 79...|[{document, 0, 79...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Israeli settlers ...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 6, Is...|[{pos, 0, 6, JJ, ...|[{named_entity, 0...|\n",
      "|The spokeswoman f...|[{document, 0, 83...|[{document, 0, 83...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|We have a situati...|[{document, 0, 26...|[{document, 0, 26...|[{token, 0, 1, We...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|More than 300 peo...|[{document, 0, 86...|[{document, 0, 86...|[{token, 0, 3, Mo...|[{pos, 0, 3, JJR,...|[{named_entity, 0...|\n",
      "|The head of the P...|[{document, 0, 86...|[{document, 0, 86...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Masked gunmen ope...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 5, Ma...|[{pos, 0, 5, JJ, ...|[{named_entity, 0...|\n",
      "|You 're listening...|[{document, 0, 30...|[{document, 0, 30...|[{token, 0, 2, Yo...|[{pos, 0, 2, PRP,...|[{named_entity, 0...|\n",
      "|This is the World...|[{document, 0, 36...|[{document, 0, 36...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|Well we ca n't te...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 3, We...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|First up we have ...|[{document, 0, 50...|[{document, 0, 50...|[{token, 0, 4, Fi...|[{pos, 0, 4, RB, ...|[{named_entity, 0...|\n",
      "|We are looking fo...|[{document, 0, 72...|[{document, 0, 72...|[{token, 0, 1, We...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|The area is mostl...|[{document, 0, 90...|[{document, 0, 90...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|It also includes ...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|The answer is Abr...|[{document, 0, 22...|[{document, 0, 22...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/development/sample.train')\n",
    "dev_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7e4b286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Iraqi leader Sadd...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 4, Ir...|[{pos, 0, 4, JJ, ...|[{named_entity, 0...|\n",
      "|He says Iraq has ...|[{document, 0, 53...|[{document, 0, 53...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|Barbara Plett rep...|[{document, 0, 35...|[{document, 0, 35...|[{token, 0, 6, Ba...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
      "|Saddam Hussein ad...|[{document, 0, 98...|[{document, 0, 98...|[{token, 0, 5, Sa...|[{pos, 0, 5, NNP,...|[{named_entity, 0...|\n",
      "|`` Iraq has trium...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 1, ``...|[{pos, 0, 1, ``, ...|[{named_entity, 0...|\n",
      "|The Iraqi leader ...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|He was referring ...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|After the speech ...|[{document, 0, 88...|[{document, 0, 88...|[{token, 0, 4, Af...|[{pos, 0, 4, IN, ...|[{named_entity, 0...|\n",
      "|A sense of nation...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 0, A,...|[{pos, 0, 0, DT, ...|[{named_entity, 0...|\n",
      "|Barbara Plett , B...|[{document, 0, 35...|[{document, 0, 35...|[{token, 0, 6, Ba...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
      "| This is The World .|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|    I am Tony Kahn .|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|Gao Xingjian arri...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 2, Ga...|[{pos, 0, 2, NNP,...|[{named_entity, 0...|\n",
      "|Today Gao , who n...|[{document, 0, 17...|[{document, 0, 17...|[{token, 0, 4, To...|[{pos, 0, 4, NN, ...|[{named_entity, 0...|\n",
      "|Gao 's 1989 novel...|[{document, 0, 84...|[{document, 0, 84...|[{token, 0, 2, Ga...|[{pos, 0, 2, NNP,...|[{named_entity, 0...|\n",
      "|If you want to kn...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 1, If...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|His politics are ...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, Hi...|[{pos, 0, 2, PRP$...|[{named_entity, 0...|\n",
      "|This novel ` soul...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|It 's a large nov...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|It is , if you li...|[{document, 0, 62...|[{document, 0, 62...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e54ee7",
   "metadata": {},
   "source": [
    "### Training the model - 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5203e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base_cased download started this may take some time.\n",
      "Approximate size to download 389.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# use bert embeddings\n",
    "bert = BertEmbeddings.pretrained('bert_base_cased', 'en').setInputCols([\"sentence\",'token']).setOutputCol(\"bert\").setCaseSensitive(True)#.setMaxSentenceLength(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d0c34ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the training data into embeddings and saving it as parquet files\n",
    "readyTrainingData = bert.transform(training_data)\n",
    "\n",
    "readyTrainingData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab96caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "readyTrainingData = spark.read.parquet(\"/tmp/conll2003/bert_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c4f2096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the development data into embeddings and saving it as parquet files\n",
    "readyDevData = bert.transform(dev_data)\n",
    "\n",
    "readyDevData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0c284d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "readyDevData = spark.read.parquet(\"/tmp/conll2003/bert_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d64dac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa34583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "readyTestData = spark.read.parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0897336",
   "metadata": {},
   "source": [
    "### Dev set - news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7bd2f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize NER tagger\n",
    "nerTagger = NerDLApproach()\\\n",
    ".setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
    ".setLabelColumn(\"label\")\\\n",
    ".setOutputCol(\"ner\")\\\n",
    ".setMaxEpochs(10)\\\n",
    ".setBatchSize(4)\\\n",
    ".setEnableMemoryOptimizer(True)\\\n",
    ".setRandomSeed(0)\\\n",
    ".setVerbose(1)\\\n",
    ".setValidationSplit(0.2)\\\n",
    ".setEvaluationLogExtended(True)\\\n",
    ".setEnableOutputLogs(True)\\\n",
    ".setIncludeConfidence(True)\\\n",
    ".setTestDataset(\"/tmp/conll2003/bert_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68c9418e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 481 ms, sys: 302 ms, total: 783 ms\n",
      "Wall time: 2h 6min 42s\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "%time myNerModel = nerTagger.fit(readyTrainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327a58c4",
   "metadata": {},
   "source": [
    "### Testing on News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "822a465e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.32 s, sys: 436 ms, total: 2.76 s\n",
      "Wall time: 20 s\n"
     ]
    }
   ],
   "source": [
    "# infer from the trained model\n",
    "%time results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c24e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c50bb9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[88, 392]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5da5e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a198acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "275e30d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9730269952139295\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "445fbe5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.873664051399271\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cdac3a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.84      0.88      0.86       616\n",
      "        DATE       0.80      0.87      0.84      1252\n",
      "       EVENT       0.65      0.59      0.62        37\n",
      "         FAC       0.57      0.35      0.43        66\n",
      "         GPE       0.96      0.92      0.94      1664\n",
      "    LANGUAGE       0.80      0.40      0.53        10\n",
      "         LAW       0.83      0.56      0.67        36\n",
      "         LOC       0.69      0.70      0.70       144\n",
      "       MONEY       0.89      0.90      0.89       279\n",
      "        NORP       0.92      0.94      0.93       579\n",
      "     ORDINAL       0.79      0.89      0.84       118\n",
      "         ORG       0.85      0.89      0.87      1494\n",
      "     PERCENT       0.90      0.91      0.90       293\n",
      "      PERSON       0.92      0.96      0.94      1099\n",
      "     PRODUCT       0.49      0.51      0.50        71\n",
      "    QUANTITY       0.77      0.78      0.77        59\n",
      "        TIME       0.61      0.56      0.58       106\n",
      " WORK_OF_ART       0.84      0.58      0.68       111\n",
      "\n",
      "   micro avg       0.87      0.88      0.87      8034\n",
      "   macro avg       0.78      0.73      0.75      8034\n",
      "weighted avg       0.87      0.88      0.87      8034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9202cdb8",
   "metadata": {},
   "source": [
    "### Testing on News Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8a88d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.bc.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ae1562fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b6103b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "15469afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|-- basically , it...|[{document, 0, 78...|[{document, 0, 78...|[{token, 0, 1, --...|[{pos, 0, 1, :, {...|[{named_entity, 0...|\n",
      "|To express its de...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 1, To...|[{pos, 0, 1, TO, ...|[{named_entity, 0...|\n",
      "|It takes time to ...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|Dear viewers , th...|[{document, 0, 52...|[{document, 0, 52...|[{token, 0, 3, De...|[{pos, 0, 3, NNP,...|[{named_entity, 0...|\n",
      "|     This is Xu Li .|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|Thank you everyon...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 4, Th...|[{pos, 0, 4, VBP,...|[{named_entity, 0...|\n",
      "|Coming up is the ...|[{document, 0, 59...|[{document, 0, 59...|[{token, 0, 5, Co...|[{pos, 0, 5, VBG,...|[{named_entity, 0...|\n",
      "|Good-bye , dear v...|[{document, 0, 24...|[{document, 0, 24...|[{token, 0, 7, Go...|[{pos, 0, 7, UH, ...|[{named_entity, 0...|\n",
      "|Hello , dear view...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 4, He...|[{pos, 0, 4, UH, ...|[{named_entity, 0...|\n",
      "|Welcome to Focus ...|[{document, 0, 23...|[{document, 0, 23...|[{token, 0, 6, We...|[{pos, 0, 6, VBP,...|[{named_entity, 0...|\n",
      "|Today , let 's tu...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 4, To...|[{pos, 0, 4, NN, ...|[{named_entity, 0...|\n",
      "|Before dawn on Ja...|[{document, 0, 19...|[{document, 0, 19...|[{token, 0, 5, Be...|[{pos, 0, 5, IN, ...|[{named_entity, 0...|\n",
      "|Relevant departme...|[{document, 0, 94...|[{document, 0, 94...|[{token, 0, 7, Re...|[{pos, 0, 7, JJ, ...|[{named_entity, 0...|\n",
      "|The traffic admin...|[{document, 0, 94...|[{document, 0, 94...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Well , how did th...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 3, We...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|After the holiday...|[{document, 0, 72...|[{document, 0, 72...|[{token, 0, 4, Af...|[{pos, 0, 4, IN, ...|[{named_entity, 0...|\n",
      "|In addition , wha...|[{document, 0, 19...|[{document, 0, 19...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|Well , we have in...|[{document, 0, 93...|[{document, 0, 93...|[{token, 0, 3, We...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|One of the two ho...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 2, On...|[{pos, 0, 2, CD, ...|[{named_entity, 0...|\n",
      "|             Hello .|[{document, 0, 6,...|[{document, 0, 6,...|[{token, 0, 4, He...|[{pos, 0, 4, UH, ...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "71c67d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cf52b8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.21 s, sys: 316 ms, total: 1.53 s\n",
      "Wall time: 55.8 s\n"
     ]
    }
   ],
   "source": [
    "# infer from the trained model\n",
    "%time results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "985cb059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "af4130a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "25428c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fca4070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "23eb70dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.953952228515144\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "27452eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6065911431513903\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f39a1c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.69      0.74      0.71       182\n",
      "        DATE       0.60      0.61      0.60       200\n",
      "       EVENT       0.50      0.14      0.22        14\n",
      "         FAC       0.84      0.77      0.80        48\n",
      "         GPE       0.46      0.77      0.57       353\n",
      "         LAW       1.00      0.00      0.00         3\n",
      "         LOC       0.68      0.58      0.62        26\n",
      "       MONEY       1.00      1.00      1.00         3\n",
      "        NORP       0.55      0.59      0.57       138\n",
      "     ORDINAL       0.87      0.90      0.88        50\n",
      "         ORG       0.31      0.69      0.43       153\n",
      "     PERCENT       0.93      0.93      0.93        14\n",
      "      PERSON       0.64      0.79      0.71       382\n",
      "     PRODUCT       0.00      1.00      0.00         0\n",
      "    QUANTITY       0.63      0.55      0.59        40\n",
      "        TIME       0.49      0.40      0.44        63\n",
      " WORK_OF_ART       0.00      0.00      0.00        28\n",
      "\n",
      "   micro avg       0.54      0.69      0.61      1697\n",
      "   macro avg       0.60      0.61      0.53      1697\n",
      "weighted avg       0.56      0.69      0.61      1697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929acf71",
   "metadata": {},
   "source": [
    "### Testing on WebLogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0597d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.wb.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bbd25fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6524551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6c942177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|The success of al...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The Source of the...|[{document, 0, 23...|[{document, 0, 23...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The al - Jazeera ...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|In this film the ...|[{document, 0, 73...|[{document, 0, 73...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|The Hebrew channe...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|This is a unique ...|[{document, 0, 45...|[{document, 0, 45...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|There are many qu...|[{document, 0, 20...|[{document, 0, 20...|[{token, 0, 4, Th...|[{pos, 0, 4, EX, ...|[{named_entity, 0...|\n",
      "|I leave you with ...|[{document, 0, 72...|[{document, 0, 72...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|the link to the f...|[{document, 0, 22...|[{document, 0, 22...|[{token, 0, 2, th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|http://z08.zuploa...|[{document, 0, 52...|[{document, 0, 52...|[{token, 0, 52, h...|[{pos, 0, 52, ADD...|[{named_entity, 0...|\n",
      "|The Islam Diary :...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Speaking at the s...|[{document, 0, 44...|[{document, 0, 44...|[{token, 0, 7, Sp...|[{pos, 0, 7, VBG,...|[{named_entity, 0...|\n",
      "|He also told the ...|[{document, 0, 34...|[{document, 0, 34...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|The newspaper quo...|[{document, 0, 33...|[{document, 0, 33...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The newspaper des...|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|He speaks in the ...|[{document, 0, 93...|[{document, 0, 93...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|He was quoted as ...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|It pointed out th...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|This means that t...|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|\" Fadil \" was quo...|[{document, 0, 22...|[{document, 0, 22...|[{token, 0, 0, \",...|[{pos, 0, 0, ``, ...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3e3adc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bee7150b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 400 ms, sys: 157 ms, total: 557 ms\n",
      "Wall time: 37.4 s\n"
     ]
    }
   ],
   "source": [
    "# infer from the trained model\n",
    "%time results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8cb952ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ff92a64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "572ca05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "485a5704",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1e8cdd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9676431776194246\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "96bea8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.761820592134335\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6f84964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.82      0.73      0.77        85\n",
      "        DATE       0.53      0.73      0.61        74\n",
      "       EVENT       0.40      0.17      0.24        12\n",
      "         FAC       0.56      0.28      0.37        18\n",
      "         GPE       0.93      0.93      0.93       173\n",
      "    LANGUAGE       0.50      0.25      0.33         4\n",
      "         LAW       1.00      1.00      1.00         1\n",
      "         LOC       0.44      0.44      0.44         9\n",
      "       MONEY       0.61      0.76      0.68        25\n",
      "        NORP       0.87      0.88      0.87       107\n",
      "     ORDINAL       0.81      0.72      0.76        18\n",
      "         ORG       0.56      0.73      0.63       117\n",
      "     PERCENT       0.58      0.58      0.58        33\n",
      "      PERSON       0.88      0.78      0.83       407\n",
      "     PRODUCT       0.00      0.00      0.00         1\n",
      "    QUANTITY       0.50      0.33      0.40         6\n",
      "        TIME       0.64      0.70      0.67        20\n",
      " WORK_OF_ART       0.58      0.26      0.36        27\n",
      "\n",
      "   micro avg       0.77      0.76      0.76      1137\n",
      "   macro avg       0.62      0.57      0.58      1137\n",
      "weighted avg       0.78      0.76      0.76      1137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629decfc",
   "metadata": {},
   "source": [
    "### Testing on Telephonic Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4f401797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.tc.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6b826c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f7b8fe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "74968763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|But %um , guessed...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 2, Bu...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|              What ?|[{document, 0, 5,...|[{document, 0, 5,...|[{token, 0, 3, Wh...|[{pos, 0, 3, WP, ...|[{named_entity, 0...|\n",
      "|         %um The %um|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, %u...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|             Again ?|[{document, 0, 6,...|[{document, 0, 6,...|[{token, 0, 4, Ag...|[{pos, 0, 4, RB, ...|[{named_entity, 0...|\n",
      "|%um 118.91_120.82...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 2, %u...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|       two weeks ago|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 2, tw...|[{pos, 0, 2, CD, ...|[{named_entity, 0...|\n",
      "|I know Sue is goi...|[{document, 0, 36...|[{document, 0, 36...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|         %eh Right .|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, %e...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|             Right .|[{document, 0, 6,...|[{document, 0, 6,...|[{token, 0, 4, Ri...|[{pos, 0, 4, UH, ...|[{named_entity, 0...|\n",
      "|Did she see her m...|[{document, 0, 20...|[{document, 0, 20...|[{token, 0, 2, Di...|[{pos, 0, 2, VBD,...|[{named_entity, 0...|\n",
      "|       Yes she did .|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 2, Ye...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|               And ?|[{document, 0, 4,...|[{document, 0, 4,...|[{token, 0, 2, An...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|       Yes she did .|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 2, Ye...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|               And ?|[{document, 0, 4,...|[{document, 0, 4,...|[{token, 0, 2, An...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|%um , %um , which...|[{document, 0, 73...|[{document, 0, 73...|[{token, 0, 2, %u...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|I spoke to her mo...|[{document, 0, 29...|[{document, 0, 29...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "| %um and I asked her|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 2, %u...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|       I said listen|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|I said %um please...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|things have come ...|[{document, 0, 78...|[{document, 0, 78...|[{token, 0, 5, th...|[{pos, 0, 5, NNS,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cf3efcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "eabb88c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 325 ms, sys: 40 ms, total: 365 ms\n",
      "Wall time: 29.6 s\n"
     ]
    }
   ],
   "source": [
    "# infer from the trained model\n",
    "%time results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "75b3e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a64ec236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ccb527cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "788c5188",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0d231fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9788629737609329\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c2cdc728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7311827956989246\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0475c395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.76      0.75      0.76        52\n",
      "        DATE       0.67      0.72      0.69        74\n",
      "         FAC       1.00      0.00      0.00         3\n",
      "         GPE       0.88      0.84      0.86        50\n",
      "    LANGUAGE       1.00      0.38      0.55         8\n",
      "         LOC       0.00      1.00      0.00         0\n",
      "       MONEY       0.67      0.57      0.62         7\n",
      "        NORP       0.77      1.00      0.87        17\n",
      "     ORDINAL       0.89      0.89      0.89         9\n",
      "         ORG       0.32      0.26      0.29        27\n",
      "     PERCENT       1.00      0.33      0.50         6\n",
      "      PERSON       0.85      0.93      0.89       100\n",
      "     PRODUCT       0.00      0.00      0.00         4\n",
      "    QUANTITY       0.00      1.00      0.00         0\n",
      "        TIME       0.44      0.17      0.25        23\n",
      "\n",
      "   micro avg       0.75      0.72      0.73       380\n",
      "   macro avg       0.62      0.59      0.48       380\n",
      "weighted avg       0.74      0.72      0.71       380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
