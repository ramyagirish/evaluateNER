{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1edef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  3.1.0\n",
      "Apache Spark version:  3.1.2\n"
     ]
    }
   ],
   "source": [
    "# call relevant packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "import sparknlp\n",
    "\n",
    "# to use GPU \n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb98346",
   "metadata": {},
   "source": [
    "## Creating a composite CoNLL file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4910c848",
   "metadata": {},
   "source": [
    "### Create training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "024205af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training filJe with all the news corpus\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/onto.bn.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85935111",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769deee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This\\tDT\\t(TOP(S(NP*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nThe\\tDT\\t(NP(NP*\\tB-ORG\\nWorld\\tNNP\\t*)\\tI-ORG\\n,\\t,\\t*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nco-production\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tB-ORG\\nBBC\\tNNP\\t*\\tI-ORG\\nWorld\\tNNP\\t*\\tI-ORG\\nService\\tNNP\\t*)\\tI-ORG\\n,\\t,\\t*\\tO\\nPRI\\tNNP\\t(NP*)\\tB-ORG\\n,\\t,\\t*\\tO\\nand\\tCC\\t*\\tO\\nWGBH\\tNNP\\t(NP(NP*)\\tB-ORG\\nin\\tIN\\t(PP*\\tO\\nBoston\\tNNP\\t(NP*))))))))\\tB-GPE\\n.\\t.\\t*))\\tO',\n",
       " 'I\\tPRP\\t(TOP(S(NP*)\\tO\\nam\\tVBP\\t(VP*\\tO\\nLisa\\tNNP\\t(NP*\\tB-PERSON\\nMullins\\tNNP\\t*))\\tI-PERSON\\n.\\t.\\t*))\\tO',\n",
       " \"A\\tDT\\t(TOP(S(NP*\\tO\\nplan\\tNN\\t*)\\tO\\nwas\\tVBD\\t(VP*\\tO\\nannounced\\tVBN\\t(VP*\\tO\\ntoday\\tNN\\t(NP*)\\tB-DATE\\nto\\tTO\\t(S(VP*\\tO\\nraise\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nRussian\\tJJ\\t*\\tB-NORP\\nnuclear\\tJJ\\t*\\tO\\nsubmarine\\tNN\\t*)\\tO\\n`\\t''\\t(NP*\\tO\\nKursk\\tNNP\\t*\\tB-PRODUCT\\n'\\t''\\t*))\\tO\\n,\\t,\\t*\\tO\\nnext\\tJJ\\t(NP(NP*\\tB-DATE\\nsummer\\tNN\\t*)\\tI-DATE\\n,\\t,\\t*\\tO\\nalmost\\tRB\\t(SBAR(NP(QP*\\tB-DATE\\na\\tDT\\t*)\\tI-DATE\\nyear\\tNN\\t*)\\tI-DATE\\nafter\\tIN\\t*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\nhit\\tVBD\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nbottom\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tB-LOC\\nBarents\\tNNP\\t*\\tI-LOC\\nSea\\tNNP\\t*)))\\tI-LOC\\nfollowing\\tVBG\\t(PP*\\tO\\na\\tDT\\t(NP*\\tO\\nmysterious\\tJJ\\t*\\tO\\naccident\\tNN\\t*)))))))))))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'The\\tDT\\t(TOP(S(NP(NP*\\tB-ORG\\nKursk\\tNNP\\t*\\tI-ORG\\nFoundation\\tNNP\\t*)\\tI-ORG\\n,\\t,\\t*\\tO\\nan\\tDT\\t(NP(NP*\\tO\\ninternational\\tJJ\\t*\\tO\\nconsortium\\tNN\\t*)\\tO\\nled\\tVBN\\t(VP*\\tO\\nby\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\ngovernments\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nRussia\\tNNP\\t(NP(NP*)\\tB-GPE\\nand\\tCC\\t*\\tO\\nThe\\tDT\\t(NP*\\tB-GPE\\nNetherlands\\tNNP\\t*))))))))\\tI-GPE\\nsays\\tVBZ\\t(VP*\\tO\\nit\\tPRP\\t(SBAR(S(NP*)\\tO\\ncan\\tMD\\t(VP*\\tO\\ndo\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\njob\\tNN\\t*)\\tO\\nsafely\\tRB\\t(ADVP*)\\tO\\nif\\tIN\\t(SBAR*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\ncan\\tMD\\t(VP*\\tO\\nsecure\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nfunding\\tNN\\t*))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " \"The\\tDT\\t(TOP(S(NP(NP*\\tO\\nBBC\\tNNP\\t*\\tB-ORG\\n's\\tPOS\\t*)\\tO\\nJames\\tNNP\\t*\\tB-PERSON\\nRogers\\tNNP\\t*)\\tI-PERSON\\nwas\\tVBD\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nBrussels\\tNNP\\t(NP*))\\tB-GPE\\nfor\\tIN\\t(PP*\\tO\\ntoday\\tNN\\t(NP(NP*\\tB-DATE\\n's\\tPOS\\t*)\\tO\\nannouncement\\tNN\\t*)))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'He\\tPRP\\t(TOP(S(NP*)\\tO\\nsays\\tVBZ\\t(VP*\\tO\\nthere\\tEX\\t(SBAR(S(NP*)\\tO\\nare\\tVBP\\t(VP*\\tO\\ntwo\\tCD\\t(NP(NP*\\tB-CARDINAL\\nmain\\tJJ\\t*\\tO\\nreasons\\tNNS\\t*)\\tO\\nfor\\tIN\\t(PP*\\tO\\nraising\\tVBG\\t(S(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nKursk\\tNNP\\t*)))))))))\\tB-PRODUCT\\n.\\t.\\t*))\\tO',\n",
       " 'Obviously\\tRB\\t(TOP(S(ADVP*)\\tO\\nthe\\tDT\\t(NP*\\tO\\nfirst\\tJJ\\t*\\tB-ORDINAL\\none\\tNN\\t*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\nto\\tTO\\t(S(VP*\\tO\\nrecover\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nbodies\\tNNS\\t*)\\tO\\nthat\\tWDT\\t(SBAR(WHNP*)\\tO\\nremain\\tVBP\\t(S(VP*))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'There\\tEX\\t(TOP(S(NP*)\\tO\\nare\\tVBP\\t(VP*\\tO\\nstill\\tRB\\t(ADVP*)\\tO\\nthe\\tDT\\t(NP(NP(NP*\\tO\\nbodies\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nmore\\tJJR\\t(NP(QP*\\tB-CARDINAL\\nthan\\tIN\\t*\\tI-CARDINAL\\n100\\tCD\\t*)\\tI-CARDINAL\\nsailors\\tNNS\\t*)))\\tO\\ntrapped\\tVBN\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nvessel\\tNN\\t*))\\tO\\nsince\\tIN\\t(SBAR*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\nsank\\tVBD\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tB-LOC\\nBarents\\tNNP\\t*\\tI-LOC\\nSea\\tNNP\\t*))\\tI-LOC\\nin\\tIN\\t(PP*\\tO\\nAugust\\tNNP\\t(NP(NP*)\\tB-DATE\\nof\\tIN\\t(PP*\\tI-DATE\\nlast\\tJJ\\t(NP*\\tI-DATE\\nyear\\tNN\\t*))))))))))\\tI-DATE\\n.\\t.\\t*))\\tO',\n",
       " \"Secondly\\tRB\\t(TOP(S(ADVP*)\\tB-ORDINAL\\n,\\t,\\t*\\tO\\nthere\\tEX\\t(S(NP*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\na\\tDT\\t(NP*\\tO\\nlonger\\tJJR\\t(NML*\\tO\\n-\\tHYPH\\t*\\tO\\nterm\\tNN\\t*)\\tO\\nthreat\\tNN\\t*)))\\tO\\nand\\tCC\\t*\\tO\\nthat\\tDT\\t(S(NP*)\\tO\\n's\\tVBZ\\t(VP*\\tO\\nan\\tDT\\t(NP*\\tO\\nenvironmental\\tJJ\\t*\\tO\\nthreat\\tNN\\t*)))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'The\\tDT\\t(TOP(S(NP*\\tB-ORG\\nKursk\\tNNP\\t*\\tI-ORG\\nFoundation\\tNNP\\t*)\\tI-ORG\\nsays\\tVBZ\\t(VP*\\tO\\nthat\\tIN\\t(SBAR(SBAR*\\tO\\nthe\\tDT\\t(S(NP(NP*\\tO\\nnuclear\\tJJ\\t*\\tO\\nreactors\\tNNS\\t*)\\tO\\n,\\t,\\t*\\tO\\nthere\\tEX\\t(PRN(S(NP*)\\tO\\nare\\tVBP\\t(VP*\\tO\\ntwo\\tCD\\t(NP(NP*)\\tB-CARDINAL\\non\\tIN\\t(PP*\\tO\\nboard\\tNN\\t(NP*)))))))\\tO\\n,\\t,\\t*\\tO\\nare\\tVBP\\t(VP*\\tO\\nsafe\\tJJ\\t(ADJP*\\tO\\nfor\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\ntime\\tNN\\t*\\tO\\nbeing\\tNN\\t*))))))\\tO\\nbut\\tCC\\t*\\tO\\nthat\\tIN\\t(SBAR*\\tO\\nthe\\tDT\\t(S(NP(NP*\\tO\\ncorrosive\\tJJ\\t*\\tO\\neffects\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nseawater\\tNN\\t(NP*)))\\tO\\ncould\\tMD\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\ntime\\tNN\\t(NP*))\\tO\\nlead\\tVB\\t(VP*\\tO\\nto\\tIN\\t(PP*\\tO\\nradioactive\\tJJ\\t(NP(NP(NP*\\tO\\nleaks\\tNNS\\t*)\\tO\\ninto\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nocean\\tNN\\t*)))\\tO\\n,\\t,\\t*\\tO\\nwhich\\tWDT\\t(SBAR(WHNP*)\\tO\\nobviously\\tRB\\t(S(ADVP*)\\tO\\ncould\\tMD\\t(VP*\\tO\\nhave\\tVB\\t(VP*\\tO\\ndevastating\\tJJ\\t(NP(NP*\\tO\\nconsequences\\tNNS\\t*)\\tO\\non\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nfishery\\tNN\\t*)\\tO\\nand\\tCC\\t*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nenvironment\\tNN\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\ngeneral\\tJJ\\t(ADJP*))))))))))))))))))\\tO\\n.\\t.\\t*))\\tO']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f341e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a5589da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41459c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/onto.mz.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84dedece",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f31fbf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/onto.nw.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5134538",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "053567a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding bc files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/onto.bc.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84579d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac2b9a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding wb files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/onto.wb.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d711f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41e0b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/train/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa11d3b4",
   "metadata": {},
   "source": [
    "### Create dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "488f800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training filJe with all the news corpus\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/development/onto.tc.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc5fc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3f11a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yeah\\tUH\\t(TOP(INTJ*))\\tO',\n",
       " \"until\\tIN\\t(TOP(SBAR*\\tO\\nlike\\tUH\\t(S(INTJ*)\\tO\\nyou\\tPRP\\t(NP*)\\tO\\nfind\\tVBP\\t(VP*\\tO\\nsomething\\tNN\\t(NP(NP*)\\tO\\nthat\\tWDT\\t(SBAR(WHNP*)\\tO\\nis\\tVBZ\\t(S(VP*\\tO\\nn't\\tRB\\t*\\tO\\nstressful\\tJJ\\t(ADJP(ADJP*)\\tO\\nand\\tCC\\t*\\tO\\nnot\\tRB\\t(ADJP*\\tO\\nboring\\tJJ\\t*))))))))\\tO\\n.\\t.\\t*))\\tO\",\n",
       " 'yeah\\tUH\\t(TOP(INTJ*))\\tO',\n",
       " \"that\\tDT\\t(TOP(SQ(S(NP*)\\tO\\n's\\tVBZ\\t(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nmain\\tJJ\\t*\\tO\\ntwo\\tCD\\t*\\tB-CARDINAL\\ncombinations\\tNNS\\t*)))\\tO\\nright\\tJJ\\t(SQ(ADJP*))\\tO\\n?\\t.\\t*))\\tO\",\n",
       " 'yeah\\tUH\\t(TOP(INTJ*\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'Not\\tRB\\t(TOP(ADJP(ADJP*\\tO\\nstressful\\tJJ\\t*)\\tO\\nand\\tCC\\t*\\tO\\nnot\\tRB\\t(ADJP*\\tO\\nboring\\tJJ\\t*)))\\tO',\n",
       " 'yeah\\tUH\\t(TOP(INTJ*\\tO\\n.\\t.\\t*))\\tO',\n",
       " \"Well\\tUH\\t(TOP(S(INTJ*)\\tO\\nit\\tPRP\\t(NP*)\\tO\\n's\\tVBZ\\t(VP*\\tO\\nlike\\tIN\\t(SBAR*\\tO\\nI\\tPRP\\t(S(NP*)\\tO\\nhave\\tVBP\\t(VP*\\tO\\nno\\tDT\\t(NP(NP*\\tO\\nproblems\\tNNS\\t*)\\tO\\nwith\\tIN\\t(PP*\\tO\\nsecretarial\\tJJ\\t(NP*\\tO\\nwork\\tNN\\t*)))))))))\\tO\",\n",
       " \"okay\\tUH\\t(TOP(S(INTJ*)\\tO\\nthat\\tDT\\t(NP*)\\tO\\n's\\tVBZ\\t(VP*\\tO\\nwhat\\tWP\\t(SBAR(WHNP*)\\tO\\nfitted\\tVBD\\t(S(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nto\\tIN\\t(PP*\\tO\\nthat\\tDT\\t(NP*\\tO\\ncategory\\tNN\\t*)))\\tO\\nbecause\\tIN\\t(SBAR*\\tO\\nif\\tIN\\t(S(SBAR*\\tO\\nyou\\tPRP\\t(S(NP*)\\tO\\nare\\tVBP\\t(VP*\\tO\\na\\tDT\\t(NP*\\tO\\nbusy\\tJJ\\t(ADJP*\\tO\\nenough\\tRB\\t*)\\tO\\nsecretary\\tNN\\t*))))\\tO\\nyou\\tPRP\\t(PRN(S(NP*)\\tO\\nknow\\tVBP\\t(VP*)))\\tO\\n%eh\\tUH\\t(INTJ*)\\tO\\n%mm\\tUH\\t(EMBED(INTJ*\\tO\\n.\\t.\\t*))\\tO\\nyou\\tPRP\\t(NP*)\\tO\\ndo\\tVBP\\t(VP*\\tO\\nn't\\tRB\\t*\\tO\\nget\\tVB\\t(VP*\\tO\\nbored\\tJJ\\t(ADJP*)\\tO\\nbecause\\tIN\\t(SBAR*\\tO\\nyou\\tPRP\\t(S(NP*)\\tO\\nare\\tVBP\\t(VP*\\tO\\nalways\\tRB\\t(ADVP*)\\tO\\ndoing\\tVBG\\t(VP*\\tO\\nsomething\\tNN\\t(NP(NP*)\\tO\\ndifferent\\tJJ\\t(ADJP*))))))))))))))))\\tO\",\n",
       " \"and\\tCC\\t(TOP(FRAG*\\tO\\nif\\tIN\\t(SBAR*\\tO\\nyour\\tPRP$\\t(S(NP*\\tO\\nemployer\\tNN\\t*)\\tO\\nis\\tVBZ\\t(VP*\\tO\\na\\tDT\\t(NP*\\tO\\nrelatively\\tRB\\t(ADJP*\\tO\\ndecent\\tJJ\\t*)\\tO\\nhuman\\tNN\\t*\\tO\\nbeing\\tNN\\t*))))\\tO\\nyou\\tPRP\\t(PRN(S(NP*)\\tO\\nknow\\tVBP\\t(VP*)))\\tO\\ntwo\\tCD\\t(NP*\\tB-CARDINAL\\nif's\\tNNS\\t*)\\tO\\nthere\\tRB\\t(ADVP*)))\\tO\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5e15110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7901892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d997328",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/development/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc40ca6",
   "metadata": {},
   "source": [
    "### Create test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b203802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training filJe with all the news corpus\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.bn.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "556a0e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c848103b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Iraqi\\tJJ\\t(TOP(S(NP(NML*\\tB-NORP\\nleader\\tNN\\t*)\\tO\\nSaddam\\tNNP\\t*\\tB-PERSON\\nHussein\\tNNP\\t*)\\tI-PERSON\\nhas\\tVBZ\\t(VP*\\tO\\ngiven\\tVBN\\t(VP*\\tO\\na\\tDT\\t(NP*\\tO\\ndefiant\\tJJ\\t*\\tO\\nspeech\\tNN\\t*)\\tO\\nto\\tTO\\t(S(VP*\\tO\\nmark\\tVB\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\ntenth\\tJJ\\t*\\tB-ORDINAL\\nanniversary\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tB-EVENT\\nGulf\\tNNP\\t*\\tI-EVENT\\nWar\\tNNP\\t*))))))))\\tI-EVENT\\n.\\t.\\t*))\\tO',\n",
       " 'He\\tPRP\\t(TOP(S(NP*)\\tO\\nsays\\tVBZ\\t(VP*\\tO\\nIraq\\tNNP\\t(SBAR(S(NP*)\\tB-GPE\\nhas\\tVBZ\\t(VP*\\tO\\ntriumphed\\tVBN\\t(VP*\\tO\\nover\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nevil\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nWest\\tNNP\\t*)))))))))\\tB-LOC\\n.\\t.\\t*))\\tO',\n",
       " 'Barbara\\tNNP\\t(TOP(S(NP*\\tB-PERSON\\nPlett\\tNNP\\t*)\\tI-PERSON\\nreports\\tVBZ\\t(VP*\\tO\\nfrom\\tIN\\t(PP*\\tO\\nBaghdad\\tNNP\\t(NP*)))\\tB-GPE\\n.\\t.\\t*))\\tO',\n",
       " 'Saddam\\tNNP\\t(TOP(S(NP*\\tB-PERSON\\nHussein\\tNNP\\t*)\\tI-PERSON\\naddressed\\tVBD\\t(VP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nnation\\tNN\\t*)\\tO\\nin\\tIN\\t(PP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nspeech\\tNN\\t*)\\tO\\nfilled\\tVBN\\t(VP*\\tO\\nwith\\tIN\\t(PP*\\tO\\nrhetoric\\tNN\\t(NP(NP*)\\tO\\nand\\tCC\\t*\\tO\\na\\tDT\\t(NP(NP*\\tO\\ndeclaration\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nvictory\\tNN\\t(NP*)))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " \"``\\t``\\t(TOP(S*\\tO\\nIraq\\tNNP\\t(S(NP*)\\tB-GPE\\nhas\\tVBZ\\t(VP*\\tO\\ntriumphed\\tVBN\\t(VP*\\tO\\nover\\tIN\\t(PP*\\tO\\nits\\tPRP$\\t(NP*\\tO\\nenemies\\tNNS\\t*)))))\\tO\\n,\\t,\\t*\\tO\\n''\\t''\\t*\\tO\\nhe\\tPRP\\t(PRN(S(NP*)\\tO\\nsaid\\tVBD\\t(VP*)))\\tO\\n,\\t,\\t*\\tO\\n``\\t``\\t*\\tO\\nand\\tCC\\t*\\tO\\nit\\tPRP\\t(S(NP*)\\tO\\nwill\\tMD\\t(VP*\\tO\\ntriumph\\tVB\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nremaining\\tVBG\\t*\\tO\\nrounds\\tNNS\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nbattle\\tNN\\t*)))))))\\tO\\n.\\t.\\t*\\tO\\n''\\t''\\t*))\\tO\",\n",
       " 'The\\tDT\\t(TOP(S(NP*\\tO\\nIraqi\\tJJ\\t*\\tB-NORP\\nleader\\tNN\\t*)\\tO\\nsaid\\tVBD\\t(VP*\\tO\\nthe\\tDT\\t(SBAR(S(NP*\\tB-EVENT\\nGulf\\tNNP\\t*\\tI-EVENT\\nWar\\tNNP\\t*)\\tI-EVENT\\nwas\\tVBD\\t(VP*\\tO\\na\\tDT\\t(NP(NP(NP*\\tO\\nconfrontation\\tNN\\t*)\\tO\\nbetween\\tIN\\t(PP*\\tO\\ngood\\tNN\\t(NP*\\tO\\nand\\tCC\\t*\\tO\\nevil\\tNN\\t*)))\\tO\\nthat\\tWDT\\t(SBAR(WHNP*)\\tO\\ncontinues\\tVBZ\\t(S(VP*\\tO\\nto\\tIN\\t(PP*\\tO\\nthis\\tDT\\t(NP*\\tO\\nday\\tNN\\t*))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'He\\tPRP\\t(TOP(S(NP*)\\tO\\nwas\\tVBD\\t(VP*\\tO\\nreferring\\tVBG\\t(VP*\\tO\\nto\\tIN\\t(PP*\\tO\\nhis\\tPRP$\\t(NP(NP*\\tO\\nstruggle\\tNN\\t*)\\tO\\nagainst\\tIN\\t(PP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\nUS\\tNNP\\t*)\\tB-GPE\\n,\\t,\\t*\\tO\\nwhich\\tWDT\\t(SBAR(WHNP*)\\tO\\nstrongly\\tRB\\t(S(ADVP*)\\tO\\nsupports\\tVBZ\\t(VP*\\tO\\nthe\\tDT\\t(NP(NP*\\tO\\neconomic\\tJJ\\t*\\tO\\nembargo\\tNN\\t*)\\tO\\nmeant\\tVBN\\t(VP*\\tO\\nto\\tTO\\t(S(VP*\\tO\\nforce\\tVB\\t(VP*\\tO\\nhim\\tPRP\\t(NP*)\\tO\\nto\\tTO\\t(S(VP*\\tO\\ndisarm\\tVB\\t(VP*)))))))))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'After\\tIN\\t(TOP(S(PP*\\tO\\nthe\\tDT\\t(NP*\\tO\\nspeech\\tNN\\t*))\\tO\\ndemonstrators\\tNNS\\t(NP*)\\tO\\ngathered\\tVBD\\t(VP*\\tO\\nin\\tIN\\t(PP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\npublic\\tJJ\\t*\\tO\\nshow\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nsupport\\tNN\\t(NP(NP*)\\tO\\nfor\\tIN\\t(PP*\\tO\\ntheir\\tPRP$\\t(NP*\\tO\\nPresident\\tNNP\\t*)))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'A\\tDT\\t(TOP(S(NP(NP*\\tO\\nsense\\tNN\\t*)\\tO\\nof\\tIN\\t(PP*\\tO\\nnationalism\\tNN\\t(NP*)))\\tO\\nhas\\tVBZ\\t(VP*\\tO\\ngrown\\tVBN\\t(VP*\\tO\\nstronger\\tJJR\\t(ADJP*)\\tO\\nhere\\tRB\\t(ADVP*)\\tO\\nas\\tIN\\t(SBAR*\\tO\\nmany\\tJJ\\t(S(NP*\\tO\\npeople\\tNNS\\t*)\\tO\\nrally\\tVBP\\t(VP*\\tO\\nagainst\\tIN\\t(PP*\\tO\\nwhat\\tWP\\t(SBAR(WHNP*)\\tO\\nthey\\tPRP\\t(S(NP*)\\tO\\nsay\\tVBP\\t(VP*\\tO\\nis\\tVBZ\\t(SBAR(S(VP*\\tO\\na\\tDT\\t(NP(NP*\\tO\\nsiege\\tNN\\t*)\\tO\\non\\tIN\\t(PP*\\tO\\ntheir\\tPRP$\\t(NP*\\tO\\ncountry\\tNN\\t*)))))))))))))))\\tO\\n.\\t.\\t*))\\tO',\n",
       " 'Barbara\\tNNP\\t(TOP(FRAG(NP*\\tB-PERSON\\nPlett\\tNNP\\t*)\\tI-PERSON\\n,\\t,\\t*\\tO\\nBBC\\tNNP\\t(NP*\\tB-ORG\\nNews\\tNNP\\t*)\\tI-ORG\\n,\\t,\\t*\\tO\\nBaghdad\\tNNP\\t(NP*)\\tB-GPE\\n.\\t.\\t*))\\tO']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d115bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09dadc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "586b9172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.mz.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30c753da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33682301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.nw.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "711d912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ebb90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da1bf3a",
   "metadata": {},
   "source": [
    "### import data in CONLL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3174f407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|This is The World...|[{document, 0, 88...|[{document, 0, 88...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "| I am Lisa Mullins .|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|A plan was announ...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 0, A,...|[{pos, 0, 0, DT, ...|[{named_entity, 0...|\n",
      "|The Kursk Foundat...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The BBC 's James ...|[{document, 0, 66...|[{document, 0, 66...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|He says there are...|[{document, 0, 57...|[{document, 0, 57...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|Obviously the fir...|[{document, 0, 61...|[{document, 0, 61...|[{token, 0, 8, Ob...|[{pos, 0, 8, RB, ...|[{named_entity, 0...|\n",
      "|There are still t...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 4, Th...|[{pos, 0, 4, EX, ...|[{named_entity, 0...|\n",
      "|Secondly , there ...|[{document, 0, 79...|[{document, 0, 79...|[{token, 0, 7, Se...|[{pos, 0, 7, RB, ...|[{named_entity, 0...|\n",
      "|The Kursk Foundat...|[{document, 0, 30...|[{document, 0, 30...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|That 's the dange...|[{document, 0, 44...|[{document, 0, 44...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|I would think the...|[{document, 0, 56...|[{document, 0, 56...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|I mean this is a ...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|     That 's right .|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|The size of these...|[{document, 0, 51...|[{document, 0, 51...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|In fact one of th...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|Now , it 's lying...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 2, No...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|It is a huge subm...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|There are any num...|[{document, 0, 19...|[{document, 0, 19...|[{token, 0, 4, Th...|[{pos, 0, 4, EX, ...|[{named_entity, 0...|\n",
      "|Second problem , ...|[{document, 0, 77...|[{document, 0, 77...|[{token, 0, 5, Se...|[{pos, 0, 5, JJ, ...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing the training set\n",
    "from sparknlp.training import CoNLL\n",
    "\n",
    "training_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/train/sample.train')\n",
    "training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80fc3b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                yeah|[{document, 0, 3,...|[{document, 0, 3,...|[{token, 0, 3, ye...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|until like you fi...|[{document, 0, 67...|[{document, 0, 67...|[{token, 0, 4, un...|[{pos, 0, 4, IN, ...|[{named_entity, 0...|\n",
      "|                yeah|[{document, 0, 3,...|[{document, 0, 3,...|[{token, 0, 3, ye...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|that 's the main ...|[{document, 0, 40...|[{document, 0, 40...|[{token, 0, 3, th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|              yeah .|[{document, 0, 5,...|[{document, 0, 5,...|[{token, 0, 3, ye...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|Not stressful and...|[{document, 0, 27...|[{document, 0, 27...|[{token, 0, 2, No...|[{pos, 0, 2, RB, ...|[{named_entity, 0...|\n",
      "|              yeah .|[{document, 0, 5,...|[{document, 0, 5,...|[{token, 0, 3, ye...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|Well it 's like I...|[{document, 0, 55...|[{document, 0, 55...|[{token, 0, 3, We...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|okay that 's what...|[{document, 0, 17...|[{document, 0, 17...|[{token, 0, 3, ok...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|and if your emplo...|[{document, 0, 78...|[{document, 0, 78...|[{token, 0, 2, an...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|               mhm .|[{document, 0, 4,...|[{document, 0, 4,...|[{token, 0, 2, mh...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|but you get no re...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 2, bu...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|I mean oh you 're...|[{document, 0, 28...|[{document, 0, 28...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|and the pay is pr...|[{document, 0, 27...|[{document, 0, 27...|[{token, 0, 2, an...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|well it 's pretty...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 3, we...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|but you know it '...|[{document, 0, 47...|[{document, 0, 47...|[{token, 0, 2, bu...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|so I 've just bee...|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 1, so...|[{pos, 0, 1, RB, ...|[{named_entity, 0...|\n",
      "|        But anyway ,|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 2, Bu...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|and you can think...|[{document, 0, 43...|[{document, 0, 43...|[{token, 0, 2, an...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|             oh yeah|[{document, 0, 6,...|[{document, 0, 6,...|[{token, 0, 1, oh...|[{pos, 0, 1, UH, ...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/development/sample.train')\n",
    "dev_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7e4b286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Iraqi leader Sadd...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 4, Ir...|[{pos, 0, 4, JJ, ...|[{named_entity, 0...|\n",
      "|He says Iraq has ...|[{document, 0, 53...|[{document, 0, 53...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|Barbara Plett rep...|[{document, 0, 35...|[{document, 0, 35...|[{token, 0, 6, Ba...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
      "|Saddam Hussein ad...|[{document, 0, 98...|[{document, 0, 98...|[{token, 0, 5, Sa...|[{pos, 0, 5, NNP,...|[{named_entity, 0...|\n",
      "|`` Iraq has trium...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 1, ``...|[{pos, 0, 1, ``, ...|[{named_entity, 0...|\n",
      "|The Iraqi leader ...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|He was referring ...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|After the speech ...|[{document, 0, 88...|[{document, 0, 88...|[{token, 0, 4, Af...|[{pos, 0, 4, IN, ...|[{named_entity, 0...|\n",
      "|A sense of nation...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 0, A,...|[{pos, 0, 0, DT, ...|[{named_entity, 0...|\n",
      "|Barbara Plett , B...|[{document, 0, 35...|[{document, 0, 35...|[{token, 0, 6, Ba...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
      "| This is The World .|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|    I am Tony Kahn .|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|Gao Xingjian arri...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 2, Ga...|[{pos, 0, 2, NNP,...|[{named_entity, 0...|\n",
      "|Today Gao , who n...|[{document, 0, 17...|[{document, 0, 17...|[{token, 0, 4, To...|[{pos, 0, 4, NN, ...|[{named_entity, 0...|\n",
      "|Gao 's 1989 novel...|[{document, 0, 84...|[{document, 0, 84...|[{token, 0, 2, Ga...|[{pos, 0, 2, NNP,...|[{named_entity, 0...|\n",
      "|If you want to kn...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 1, If...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|His politics are ...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, Hi...|[{pos, 0, 2, PRP$...|[{named_entity, 0...|\n",
      "|This novel ` soul...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|It 's a large nov...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|It is , if you li...|[{document, 0, 62...|[{document, 0, 62...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e54ee7",
   "metadata": {},
   "source": [
    "### Training the model - 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5203e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base_cased download started this may take some time.\n",
      "Approximate size to download 389.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# use bert embeddings\n",
    "bert = BertEmbeddings.pretrained('bert_base_cased', 'en').setInputCols([\"sentence\",'token']).setOutputCol(\"bert\").setCaseSensitive(True)#.setMaxSentenceLength(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d0c34ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the training data into embeddings and saving it as parquet files\n",
    "readyTrainingData = bert.transform(training_data)\n",
    "\n",
    "readyTrainingData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab96caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "readyTrainingData = spark.read.parquet(\"/tmp/conll2003/bert_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c4f2096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the development data into embeddings and saving it as parquet files\n",
    "readyDevData = bert.transform(dev_data)\n",
    "\n",
    "readyDevData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0c284d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "readyDevData = spark.read.parquet(\"/tmp/conll2003/bert_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d64dac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa34583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "readyTestData = spark.read.parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0897336",
   "metadata": {},
   "source": [
    "### Dev set - news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7bd2f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize NER tagger\n",
    "nerTagger = NerDLApproach()\\\n",
    ".setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
    ".setLabelColumn(\"label\")\\\n",
    ".setOutputCol(\"ner\")\\\n",
    ".setMaxEpochs(10)\\\n",
    ".setBatchSize(4)\\\n",
    ".setEnableMemoryOptimizer(True)\\\n",
    ".setRandomSeed(0)\\\n",
    ".setVerbose(1)\\\n",
    ".setValidationSplit(0.2)\\\n",
    ".setEvaluationLogExtended(True)\\\n",
    ".setEnableOutputLogs(True)\\\n",
    ".setIncludeConfidence(True)\\\n",
    ".setTestDataset(\"/tmp/conll2003/bert_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68c9418e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 748 ms, sys: 487 ms, total: 1.23 s\n",
      "Wall time: 11h 4min 4s\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "%time myNerModel = nerTagger.fit(readyTrainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327a58c4",
   "metadata": {},
   "source": [
    "### Testing on News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "822a465e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.26 s, sys: 401 ms, total: 2.66 s\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "# infer from the trained model\n",
    "%time results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c24e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c50bb9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[88, 392]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5da5e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a198acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "275e30d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9764138895983476\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "445fbe5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8906152889993785\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cdac3a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.88      0.85      0.87       616\n",
      "        DATE       0.85      0.88      0.87      1252\n",
      "       EVENT       0.66      0.57      0.61        37\n",
      "         FAC       0.45      0.74      0.56        66\n",
      "         GPE       0.96      0.94      0.95      1664\n",
      "    LANGUAGE       0.83      0.50      0.62        10\n",
      "         LAW       0.76      0.53      0.62        36\n",
      "         LOC       0.74      0.76      0.75       144\n",
      "       MONEY       0.88      0.90      0.89       279\n",
      "        NORP       0.94      0.96      0.95       579\n",
      "     ORDINAL       0.82      0.92      0.87       118\n",
      "         ORG       0.92      0.86      0.89      1494\n",
      "     PERCENT       0.93      0.94      0.93       293\n",
      "      PERSON       0.92      0.97      0.94      1099\n",
      "     PRODUCT       0.68      0.73      0.71        71\n",
      "    QUANTITY       0.70      0.80      0.75        59\n",
      "        TIME       0.59      0.65      0.62       106\n",
      " WORK_OF_ART       0.65      0.46      0.54       111\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      8034\n",
      "   macro avg       0.79      0.78      0.77      8034\n",
      "weighted avg       0.89      0.89      0.89      8034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9202cdb8",
   "metadata": {},
   "source": [
    "### Testing on News Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8a88d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.bc.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae1562fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6103b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "15469afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|-- basically , it...|[{document, 0, 78...|[{document, 0, 78...|[{token, 0, 1, --...|[{pos, 0, 1, :, {...|[{named_entity, 0...|\n",
      "|To express its de...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 1, To...|[{pos, 0, 1, TO, ...|[{named_entity, 0...|\n",
      "|It takes time to ...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|Dear viewers , th...|[{document, 0, 52...|[{document, 0, 52...|[{token, 0, 3, De...|[{pos, 0, 3, NNP,...|[{named_entity, 0...|\n",
      "|     This is Xu Li .|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|Thank you everyon...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 4, Th...|[{pos, 0, 4, VBP,...|[{named_entity, 0...|\n",
      "|Coming up is the ...|[{document, 0, 59...|[{document, 0, 59...|[{token, 0, 5, Co...|[{pos, 0, 5, VBG,...|[{named_entity, 0...|\n",
      "|Good-bye , dear v...|[{document, 0, 24...|[{document, 0, 24...|[{token, 0, 7, Go...|[{pos, 0, 7, UH, ...|[{named_entity, 0...|\n",
      "|Hello , dear view...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 4, He...|[{pos, 0, 4, UH, ...|[{named_entity, 0...|\n",
      "|Welcome to Focus ...|[{document, 0, 23...|[{document, 0, 23...|[{token, 0, 6, We...|[{pos, 0, 6, VBP,...|[{named_entity, 0...|\n",
      "|Today , let 's tu...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 4, To...|[{pos, 0, 4, NN, ...|[{named_entity, 0...|\n",
      "|Before dawn on Ja...|[{document, 0, 19...|[{document, 0, 19...|[{token, 0, 5, Be...|[{pos, 0, 5, IN, ...|[{named_entity, 0...|\n",
      "|Relevant departme...|[{document, 0, 94...|[{document, 0, 94...|[{token, 0, 7, Re...|[{pos, 0, 7, JJ, ...|[{named_entity, 0...|\n",
      "|The traffic admin...|[{document, 0, 94...|[{document, 0, 94...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Well , how did th...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 3, We...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|After the holiday...|[{document, 0, 72...|[{document, 0, 72...|[{token, 0, 4, Af...|[{pos, 0, 4, IN, ...|[{named_entity, 0...|\n",
      "|In addition , wha...|[{document, 0, 19...|[{document, 0, 19...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|Well , we have in...|[{document, 0, 93...|[{document, 0, 93...|[{token, 0, 3, We...|[{pos, 0, 3, UH, ...|[{named_entity, 0...|\n",
      "|One of the two ho...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 2, On...|[{pos, 0, 2, CD, ...|[{named_entity, 0...|\n",
      "|             Hello .|[{document, 0, 6,...|[{document, 0, 6,...|[{token, 0, 4, He...|[{pos, 0, 4, UH, ...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "71c67d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cf52b8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.28 s, sys: 330 ms, total: 1.61 s\n",
      "Wall time: 58.7 s\n"
     ]
    }
   ],
   "source": [
    "# infer from the trained model\n",
    "%time results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "985cb059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af4130a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "25428c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fca4070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "23eb70dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9797155872937701\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "27452eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8262910798122065\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f39a1c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.84      0.74      0.78       182\n",
      "        DATE       0.73      0.84      0.78       200\n",
      "       EVENT       0.80      0.29      0.42        14\n",
      "         FAC       0.70      0.94      0.80        48\n",
      "         GPE       0.92      0.93      0.93       353\n",
      "         LAW       0.00      0.00      0.00         3\n",
      "         LOC       0.84      0.62      0.71        26\n",
      "       MONEY       1.00      1.00      1.00         3\n",
      "        NORP       0.59      0.62      0.60       138\n",
      "     ORDINAL       0.92      0.90      0.91        50\n",
      "         ORG       0.80      0.72      0.76       153\n",
      "     PERCENT       1.00      1.00      1.00        14\n",
      "      PERSON       0.92      0.96      0.94       382\n",
      "     PRODUCT       0.00      1.00      0.00         0\n",
      "    QUANTITY       0.93      0.93      0.93        40\n",
      "        TIME       0.66      0.63      0.65        63\n",
      " WORK_OF_ART       0.50      0.46      0.48        28\n",
      "\n",
      "   micro avg       0.82      0.83      0.83      1697\n",
      "   macro avg       0.71      0.74      0.69      1697\n",
      "weighted avg       0.83      0.83      0.82      1697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929acf71",
   "metadata": {},
   "source": [
    "### Testing on WebLogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0597d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.wb.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bbd25fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6524551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6c942177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|The success of al...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The Source of the...|[{document, 0, 23...|[{document, 0, 23...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The al - Jazeera ...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|In this film the ...|[{document, 0, 73...|[{document, 0, 73...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|The Hebrew channe...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|This is a unique ...|[{document, 0, 45...|[{document, 0, 45...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|There are many qu...|[{document, 0, 20...|[{document, 0, 20...|[{token, 0, 4, Th...|[{pos, 0, 4, EX, ...|[{named_entity, 0...|\n",
      "|I leave you with ...|[{document, 0, 72...|[{document, 0, 72...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|the link to the f...|[{document, 0, 22...|[{document, 0, 22...|[{token, 0, 2, th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|http://z08.zuploa...|[{document, 0, 52...|[{document, 0, 52...|[{token, 0, 52, h...|[{pos, 0, 52, ADD...|[{named_entity, 0...|\n",
      "|The Islam Diary :...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Speaking at the s...|[{document, 0, 44...|[{document, 0, 44...|[{token, 0, 7, Sp...|[{pos, 0, 7, VBG,...|[{named_entity, 0...|\n",
      "|He also told the ...|[{document, 0, 34...|[{document, 0, 34...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|The newspaper quo...|[{document, 0, 33...|[{document, 0, 33...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The newspaper des...|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|He speaks in the ...|[{document, 0, 93...|[{document, 0, 93...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|He was quoted as ...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|It pointed out th...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|This means that t...|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|\" Fadil \" was quo...|[{document, 0, 22...|[{document, 0, 22...|[{token, 0, 0, \",...|[{pos, 0, 0, ``, ...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e3adc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bee7150b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 418 ms, sys: 314 ms, total: 732 ms\n",
      "Wall time: 32.6 s\n"
     ]
    }
   ],
   "source": [
    "# infer from the trained model\n",
    "%time results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8cb952ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ff92a64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "572ca05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "485a5704",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1e8cdd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9660068619688572\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "96bea8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7695709862892526\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6f84964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.86      0.72      0.78        85\n",
      "        DATE       0.57      0.80      0.67        74\n",
      "       EVENT       0.50      0.25      0.33        12\n",
      "         FAC       0.39      0.50      0.44        18\n",
      "         GPE       0.94      0.94      0.94       173\n",
      "    LANGUAGE       1.00      0.50      0.67         4\n",
      "         LAW       1.00      1.00      1.00         1\n",
      "         LOC       0.62      0.56      0.59         9\n",
      "       MONEY       0.62      0.72      0.67        25\n",
      "        NORP       0.83      0.86      0.84       107\n",
      "     ORDINAL       0.80      0.67      0.73        18\n",
      "         ORG       0.70      0.68      0.69       117\n",
      "     PERCENT       0.55      0.55      0.55        33\n",
      "      PERSON       0.87      0.81      0.84       407\n",
      "     PRODUCT       0.00      0.00      0.00         1\n",
      "    QUANTITY       0.22      0.33      0.27         6\n",
      "        TIME       0.42      0.50      0.45        20\n",
      " WORK_OF_ART       0.32      0.22      0.26        27\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1137\n",
      "   macro avg       0.62      0.59      0.60      1137\n",
      "weighted avg       0.78      0.77      0.77      1137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629decfc",
   "metadata": {},
   "source": [
    "### Testing on Telephonic Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4f401797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/onto.tc.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6b826c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f7b8fe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "74968763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|But %um , guessed...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 2, Bu...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|              What ?|[{document, 0, 5,...|[{document, 0, 5,...|[{token, 0, 3, Wh...|[{pos, 0, 3, WP, ...|[{named_entity, 0...|\n",
      "|         %um The %um|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, %u...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|             Again ?|[{document, 0, 6,...|[{document, 0, 6,...|[{token, 0, 4, Ag...|[{pos, 0, 4, RB, ...|[{named_entity, 0...|\n",
      "|%um 118.91_120.82...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 2, %u...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|       two weeks ago|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 2, tw...|[{pos, 0, 2, CD, ...|[{named_entity, 0...|\n",
      "|I know Sue is goi...|[{document, 0, 36...|[{document, 0, 36...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|         %eh Right .|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 2, %e...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|             Right .|[{document, 0, 6,...|[{document, 0, 6,...|[{token, 0, 4, Ri...|[{pos, 0, 4, UH, ...|[{named_entity, 0...|\n",
      "|Did she see her m...|[{document, 0, 20...|[{document, 0, 20...|[{token, 0, 2, Di...|[{pos, 0, 2, VBD,...|[{named_entity, 0...|\n",
      "|       Yes she did .|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 2, Ye...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|               And ?|[{document, 0, 4,...|[{document, 0, 4,...|[{token, 0, 2, An...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|       Yes she did .|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 2, Ye...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|               And ?|[{document, 0, 4,...|[{document, 0, 4,...|[{token, 0, 2, An...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|%um , %um , which...|[{document, 0, 73...|[{document, 0, 73...|[{token, 0, 2, %u...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|I spoke to her mo...|[{document, 0, 29...|[{document, 0, 29...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "| %um and I asked her|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 2, %u...|[{pos, 0, 2, UH, ...|[{named_entity, 0...|\n",
      "|       I said listen|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|I said %um please...|[{document, 0, 32...|[{document, 0, 32...|[{token, 0, 0, I,...|[{pos, 0, 0, PRP,...|[{named_entity, 0...|\n",
      "|things have come ...|[{document, 0, 78...|[{document, 0, 78...|[{token, 0, 5, th...|[{pos, 0, 5, NNS,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramyabala/Research Projects/Evaluate NER/bio/test/sample.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cf3efcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the test data into embeddings and saving it as parquet files\n",
    "readyTestData = bert.transform(test_data)\n",
    "\n",
    "readyTestData.write.mode(\"Overwrite\").parquet(\"/tmp/conll2003/bert_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eabb88c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 359 ms, sys: 312 ms, total: 670 ms\n",
      "Wall time: 28.8 s\n"
     ]
    }
   ],
   "source": [
    "# infer from the trained model\n",
    "%time results = myNerModel.transform(readyTestData).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "75b3e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a64ec236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ccb527cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "788c5188",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0d231fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9764030612244898\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c2cdc728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7081712062256811\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0475c395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL       0.63      0.46      0.53        52\n",
      "        DATE       0.68      0.73      0.71        74\n",
      "       EVENT       0.00      1.00      0.00         0\n",
      "         FAC       0.22      0.67      0.33         3\n",
      "         GPE       0.89      0.94      0.91        50\n",
      "    LANGUAGE       1.00      0.62      0.77         8\n",
      "         LOC       0.00      1.00      0.00         0\n",
      "       MONEY       1.00      1.00      1.00         7\n",
      "        NORP       0.77      1.00      0.87        17\n",
      "     ORDINAL       0.73      0.89      0.80         9\n",
      "         ORG       0.29      0.07      0.12        27\n",
      "     PERCENT       0.20      0.50      0.29         6\n",
      "      PERSON       0.79      0.96      0.86       100\n",
      "     PRODUCT       1.00      0.50      0.67         4\n",
      "        TIME       0.46      0.26      0.33        23\n",
      " WORK_OF_ART       0.00      1.00      0.00         0\n",
      "\n",
      "   micro avg       0.70      0.72      0.71       380\n",
      "   macro avg       0.54      0.73      0.51       380\n",
      "weighted avg       0.70      0.72      0.69       380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a27f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
