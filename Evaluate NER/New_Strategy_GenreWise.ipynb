{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "political-corruption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  2.7.3\n",
      "Apache Spark version:  2.4.4\n"
     ]
    }
   ],
   "source": [
    "# call relevant packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "import sparknlp\n",
    "\n",
    "# to use GPU \n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-seattle",
   "metadata": {},
   "source": [
    "### Create the test sets for experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-testimony",
   "metadata": {},
   "source": [
    "#### All News test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "trying-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training file\n",
    "with open(\"/Users/ramybal/Downloads/bio/test/onto.bn.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "golden-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "single-belgium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "otherwise-relief",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Pos_special</th>\n",
       "      <th>Entity_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He</td>\n",
       "      <td>PRP</td>\n",
       "      <td>(TOP(S(NP*)</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>says</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>(VP*</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(SBAR(S(NP*)</td>\n",
       "      <td>B-GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>has</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>(VP*</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>triumphed</td>\n",
       "      <td>VBN</td>\n",
       "      <td>(VP*</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>over</td>\n",
       "      <td>IN</td>\n",
       "      <td>(PP*</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>(NP(NP*</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>evil</td>\n",
       "      <td>NN</td>\n",
       "      <td>*)</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>(PP*</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>(NP*</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>West</td>\n",
       "      <td>NNP</td>\n",
       "      <td>*)))))))))</td>\n",
       "      <td>B-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>*))</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Token  Pos   Pos_special Entity_label\n",
       "0          He  PRP   (TOP(S(NP*)            O\n",
       "1        says  VBZ          (VP*            O\n",
       "2        Iraq  NNP  (SBAR(S(NP*)        B-GPE\n",
       "3         has  VBZ          (VP*            O\n",
       "4   triumphed  VBN          (VP*            O\n",
       "5        over   IN          (PP*            O\n",
       "6         the   DT       (NP(NP*            O\n",
       "7        evil   NN            *)            O\n",
       "8          of   IN          (PP*            O\n",
       "9         the   DT          (NP*            O\n",
       "10       West  NNP    *)))))))))        B-LOC\n",
       "11          .    .           *))            O"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "recent-singing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1346"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "municipal-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "unavailable-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramybal/Downloads/bio/test/onto.mz.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "olympic-general",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "826"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unknown-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "delayed-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding other news files\n",
    "with open(\"/Users/ramybal/Downloads/bio/test/onto.nw.ner\") as fp:\n",
    "    text = fp.readlines()\n",
    "text = \"\".join(text[1:]).split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "worth-organizer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1988"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "academic-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "needed-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramybal/Downloads/bio/test/sample_genre1.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-leisure",
   "metadata": {},
   "source": [
    "#### News conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "coordinated-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training file\n",
    "with open(\"/Users/ramybal/Downloads/bio/test/onto.bc.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "reserved-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "charged-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dying-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cooperative-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramybal/Downloads/bio/test/sample_genre2.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-timeline",
   "metadata": {},
   "source": [
    "#### Phone conservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "certain-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training file\n",
    "with open(\"/Users/ramybal/Downloads/bio/test/onto.tc.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fossil-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "subtle-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "postal-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "reasonable-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramybal/Downloads/bio/test/sample_genre3.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "flying-deficit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1383"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-poker",
   "metadata": {},
   "source": [
    "#### Web logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "explicit-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training file\n",
    "with open(\"/Users/ramybal/Downloads/bio/test/onto.wb.ner\") as fp:\n",
    "    text = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "reserved-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text[1:]).split(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ahead-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([x.split('\\t') for x in text[1].split('\\n')], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "blond-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data\n",
    "conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "for t in range(len(text)):    \n",
    "    df = pd.DataFrame([x.split('\\t') for x in text[t].split('\\n') if len(x.split('\\t')) == 4], columns=[\"Token\",\"Pos\",\"Pos_special\",\"Entity_label\"])\n",
    "    tokens = df.Token.tolist()\n",
    "    pos_labels = df.Pos.tolist()\n",
    "    entity_labels = df.Entity_label.tolist()\n",
    "    for token, pos, label in zip(tokens,pos_labels,entity_labels):\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "    conll_lines += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "detected-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ramybal/Downloads/bio/test/sample_genre4.train\",\"w\") as fp:\n",
    "    for line in conll_lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-shock",
   "metadata": {},
   "source": [
    "### Import Data in CoNLL format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-wales",
   "metadata": {},
   "source": [
    "#### All the news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "accepted-general",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Iraqi leader Sadd...|[[document, 0, 10...|[[document, 0, 10...|[[token, 0, 4, Ir...|[[pos, 0, 4, JJ, ...|[[named_entity, 0...|\n",
      "|He says Iraq has ...|[[document, 0, 53...|[[document, 0, 53...|[[token, 0, 1, He...|[[pos, 0, 1, PRP,...|[[named_entity, 0...|\n",
      "|Barbara Plett rep...|[[document, 0, 35...|[[document, 0, 35...|[[token, 0, 6, Ba...|[[pos, 0, 6, NNP,...|[[named_entity, 0...|\n",
      "|Saddam Hussein ad...|[[document, 0, 98...|[[document, 0, 98...|[[token, 0, 5, Sa...|[[pos, 0, 5, NNP,...|[[named_entity, 0...|\n",
      "|`` Iraq has trium...|[[document, 0, 11...|[[document, 0, 11...|[[token, 0, 1, ``...|[[pos, 0, 1, ``, ...|[[named_entity, 0...|\n",
      "|The Iraqi leader ...|[[document, 0, 10...|[[document, 0, 10...|[[token, 0, 2, Th...|[[pos, 0, 2, DT, ...|[[named_entity, 0...|\n",
      "|He was referring ...|[[document, 0, 12...|[[document, 0, 12...|[[token, 0, 1, He...|[[pos, 0, 1, PRP,...|[[named_entity, 0...|\n",
      "|After the speech ...|[[document, 0, 88...|[[document, 0, 88...|[[token, 0, 4, Af...|[[pos, 0, 4, IN, ...|[[named_entity, 0...|\n",
      "|A sense of nation...|[[document, 0, 11...|[[document, 0, 11...|[[token, 0, 0, A,...|[[pos, 0, 0, DT, ...|[[named_entity, 0...|\n",
      "|Barbara Plett , B...|[[document, 0, 35...|[[document, 0, 35...|[[token, 0, 6, Ba...|[[pos, 0, 6, NNP,...|[[named_entity, 0...|\n",
      "| This is The World .|[[document, 0, 18...|[[document, 0, 18...|[[token, 0, 3, Th...|[[pos, 0, 3, DT, ...|[[named_entity, 0...|\n",
      "|    I am Tony Kahn .|[[document, 0, 15...|[[document, 0, 15...|[[token, 0, 0, I,...|[[pos, 0, 0, PRP,...|[[named_entity, 0...|\n",
      "|Gao Xingjian arri...|[[document, 0, 16...|[[document, 0, 16...|[[token, 0, 2, Ga...|[[pos, 0, 2, NNP,...|[[named_entity, 0...|\n",
      "|Today Gao , who n...|[[document, 0, 17...|[[document, 0, 17...|[[token, 0, 4, To...|[[pos, 0, 4, NN, ...|[[named_entity, 0...|\n",
      "|Gao 's 1989 novel...|[[document, 0, 84...|[[document, 0, 84...|[[token, 0, 2, Ga...|[[pos, 0, 2, NNP,...|[[named_entity, 0...|\n",
      "|If you want to kn...|[[document, 0, 18...|[[document, 0, 18...|[[token, 0, 1, If...|[[pos, 0, 1, IN, ...|[[named_entity, 0...|\n",
      "|His politics are ...|[[document, 0, 10...|[[document, 0, 10...|[[token, 0, 2, Hi...|[[pos, 0, 2, PRP$...|[[named_entity, 0...|\n",
      "|This novel ` soul...|[[document, 0, 15...|[[document, 0, 15...|[[token, 0, 3, Th...|[[pos, 0, 3, DT, ...|[[named_entity, 0...|\n",
      "|It 's a large nov...|[[document, 0, 11...|[[document, 0, 11...|[[token, 0, 1, It...|[[pos, 0, 1, PRP,...|[[named_entity, 0...|\n",
      "|It is , if you li...|[[document, 0, 62...|[[document, 0, 62...|[[token, 0, 1, It...|[[pos, 0, 1, PRP,...|[[named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.training import CoNLL\n",
    "\n",
    "test_data = CoNLL().readDataset(spark, '/Users/ramybal/Downloads/bio/test/sample_genre1.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-harmony",
   "metadata": {},
   "source": [
    "#### News conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "underlying-environment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|-- basically , it...|[[document, 0, 78...|[[document, 0, 78...|[[token, 0, 1, --...|[[pos, 0, 1, :, [...|[[named_entity, 0...|\n",
      "|To express its de...|[[document, 0, 13...|[[document, 0, 13...|[[token, 0, 1, To...|[[pos, 0, 1, TO, ...|[[named_entity, 0...|\n",
      "|It takes time to ...|[[document, 0, 16...|[[document, 0, 16...|[[token, 0, 1, It...|[[pos, 0, 1, PRP,...|[[named_entity, 0...|\n",
      "|Dear viewers , th...|[[document, 0, 52...|[[document, 0, 52...|[[token, 0, 3, De...|[[pos, 0, 3, NNP,...|[[named_entity, 0...|\n",
      "|     This is Xu Li .|[[document, 0, 14...|[[document, 0, 14...|[[token, 0, 3, Th...|[[pos, 0, 3, DT, ...|[[named_entity, 0...|\n",
      "|Thank you everyon...|[[document, 0, 32...|[[document, 0, 32...|[[token, 0, 4, Th...|[[pos, 0, 4, VBP,...|[[named_entity, 0...|\n",
      "|Coming up is the ...|[[document, 0, 59...|[[document, 0, 59...|[[token, 0, 5, Co...|[[pos, 0, 5, VBG,...|[[named_entity, 0...|\n",
      "|Good-bye , dear v...|[[document, 0, 24...|[[document, 0, 24...|[[token, 0, 7, Go...|[[pos, 0, 7, UH, ...|[[named_entity, 0...|\n",
      "|Hello , dear view...|[[document, 0, 21...|[[document, 0, 21...|[[token, 0, 4, He...|[[pos, 0, 4, UH, ...|[[named_entity, 0...|\n",
      "|Welcome to Focus ...|[[document, 0, 23...|[[document, 0, 23...|[[token, 0, 6, We...|[[pos, 0, 6, VBP,...|[[named_entity, 0...|\n",
      "|Today , let 's tu...|[[document, 0, 10...|[[document, 0, 10...|[[token, 0, 4, To...|[[pos, 0, 4, NN, ...|[[named_entity, 0...|\n",
      "|Before dawn on Ja...|[[document, 0, 19...|[[document, 0, 19...|[[token, 0, 5, Be...|[[pos, 0, 5, IN, ...|[[named_entity, 0...|\n",
      "|Relevant departme...|[[document, 0, 94...|[[document, 0, 94...|[[token, 0, 7, Re...|[[pos, 0, 7, JJ, ...|[[named_entity, 0...|\n",
      "|The traffic admin...|[[document, 0, 94...|[[document, 0, 94...|[[token, 0, 2, Th...|[[pos, 0, 2, DT, ...|[[named_entity, 0...|\n",
      "|Well , how did th...|[[document, 0, 12...|[[document, 0, 12...|[[token, 0, 3, We...|[[pos, 0, 3, UH, ...|[[named_entity, 0...|\n",
      "|After the holiday...|[[document, 0, 72...|[[document, 0, 72...|[[token, 0, 4, Af...|[[pos, 0, 4, IN, ...|[[named_entity, 0...|\n",
      "|In addition , wha...|[[document, 0, 19...|[[document, 0, 19...|[[token, 0, 1, In...|[[pos, 0, 1, IN, ...|[[named_entity, 0...|\n",
      "|Well , we have in...|[[document, 0, 93...|[[document, 0, 93...|[[token, 0, 3, We...|[[pos, 0, 3, UH, ...|[[named_entity, 0...|\n",
      "|One of the two ho...|[[document, 0, 13...|[[document, 0, 13...|[[token, 0, 2, On...|[[pos, 0, 2, CD, ...|[[named_entity, 0...|\n",
      "|             Hello .|[[document, 0, 6,...|[[document, 0, 6,...|[[token, 0, 4, He...|[[pos, 0, 4, UH, ...|[[named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramybal/Downloads/bio/test/sample_genre2.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-breach",
   "metadata": {},
   "source": [
    "#### Phone conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "exotic-central",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|But %um , guessed...|[[document, 0, 32...|[[document, 0, 32...|[[token, 0, 2, Bu...|[[pos, 0, 2, CC, ...|[[named_entity, 0...|\n",
      "|              What ?|[[document, 0, 5,...|[[document, 0, 5,...|[[token, 0, 3, Wh...|[[pos, 0, 3, WP, ...|[[named_entity, 0...|\n",
      "|         %um The %um|[[document, 0, 10...|[[document, 0, 10...|[[token, 0, 2, %u...|[[pos, 0, 2, UH, ...|[[named_entity, 0...|\n",
      "|             Again ?|[[document, 0, 6,...|[[document, 0, 6,...|[[token, 0, 4, Ag...|[[pos, 0, 4, RB, ...|[[named_entity, 0...|\n",
      "|%um 118.91_120.82...|[[document, 0, 32...|[[document, 0, 32...|[[token, 0, 2, %u...|[[pos, 0, 2, UH, ...|[[named_entity, 0...|\n",
      "|       two weeks ago|[[document, 0, 12...|[[document, 0, 12...|[[token, 0, 2, tw...|[[pos, 0, 2, CD, ...|[[named_entity, 0...|\n",
      "|I know Sue is goi...|[[document, 0, 36...|[[document, 0, 36...|[[token, 0, 0, I,...|[[pos, 0, 0, PRP,...|[[named_entity, 0...|\n",
      "|         %eh Right .|[[document, 0, 10...|[[document, 0, 10...|[[token, 0, 2, %e...|[[pos, 0, 2, UH, ...|[[named_entity, 0...|\n",
      "|             Right .|[[document, 0, 6,...|[[document, 0, 6,...|[[token, 0, 4, Ri...|[[pos, 0, 4, UH, ...|[[named_entity, 0...|\n",
      "|Did she see her m...|[[document, 0, 20...|[[document, 0, 20...|[[token, 0, 2, Di...|[[pos, 0, 2, VBD,...|[[named_entity, 0...|\n",
      "|       Yes she did .|[[document, 0, 12...|[[document, 0, 12...|[[token, 0, 2, Ye...|[[pos, 0, 2, UH, ...|[[named_entity, 0...|\n",
      "|               And ?|[[document, 0, 4,...|[[document, 0, 4,...|[[token, 0, 2, An...|[[pos, 0, 2, CC, ...|[[named_entity, 0...|\n",
      "|       Yes she did .|[[document, 0, 12...|[[document, 0, 12...|[[token, 0, 2, Ye...|[[pos, 0, 2, UH, ...|[[named_entity, 0...|\n",
      "|               And ?|[[document, 0, 4,...|[[document, 0, 4,...|[[token, 0, 2, An...|[[pos, 0, 2, CC, ...|[[named_entity, 0...|\n",
      "|%um , %um , which...|[[document, 0, 73...|[[document, 0, 73...|[[token, 0, 2, %u...|[[pos, 0, 2, UH, ...|[[named_entity, 0...|\n",
      "|I spoke to her mo...|[[document, 0, 29...|[[document, 0, 29...|[[token, 0, 0, I,...|[[pos, 0, 0, PRP,...|[[named_entity, 0...|\n",
      "| %um and I asked her|[[document, 0, 18...|[[document, 0, 18...|[[token, 0, 2, %u...|[[pos, 0, 2, UH, ...|[[named_entity, 0...|\n",
      "|       I said listen|[[document, 0, 12...|[[document, 0, 12...|[[token, 0, 0, I,...|[[pos, 0, 0, PRP,...|[[named_entity, 0...|\n",
      "|I said %um please...|[[document, 0, 32...|[[document, 0, 32...|[[token, 0, 0, I,...|[[pos, 0, 0, PRP,...|[[named_entity, 0...|\n",
      "|things have come ...|[[document, 0, 78...|[[document, 0, 78...|[[token, 0, 5, th...|[[pos, 0, 5, NNS,...|[[named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramybal/Downloads/bio/test/sample_genre3.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-barcelona",
   "metadata": {},
   "source": [
    "#### Web Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "brief-electron",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|The success of al...|[[document, 0, 10...|[[document, 0, 10...|[[token, 0, 2, Th...|[[pos, 0, 2, DT, ...|[[named_entity, 0...|\n",
      "|The Source of the...|[[document, 0, 23...|[[document, 0, 23...|[[token, 0, 2, Th...|[[pos, 0, 2, DT, ...|[[named_entity, 0...|\n",
      "|The al - Jazeera ...|[[document, 0, 13...|[[document, 0, 13...|[[token, 0, 2, Th...|[[pos, 0, 2, DT, ...|[[named_entity, 0...|\n",
      "|In this film the ...|[[document, 0, 73...|[[document, 0, 73...|[[token, 0, 1, In...|[[pos, 0, 1, IN, ...|[[named_entity, 0...|\n",
      "|The Hebrew channe...|[[document, 0, 15...|[[document, 0, 15...|[[token, 0, 2, Th...|[[pos, 0, 2, DT, ...|[[named_entity, 0...|\n",
      "|This is a unique ...|[[document, 0, 45...|[[document, 0, 45...|[[token, 0, 3, Th...|[[pos, 0, 3, DT, ...|[[named_entity, 0...|\n",
      "|There are many qu...|[[document, 0, 20...|[[document, 0, 20...|[[token, 0, 4, Th...|[[pos, 0, 4, EX, ...|[[named_entity, 0...|\n",
      "|I leave you with ...|[[document, 0, 72...|[[document, 0, 72...|[[token, 0, 0, I,...|[[pos, 0, 0, PRP,...|[[named_entity, 0...|\n",
      "|the link to the f...|[[document, 0, 22...|[[document, 0, 22...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...|[[named_entity, 0...|\n",
      "|http://z08.zuploa...|[[document, 0, 52...|[[document, 0, 52...|[[token, 0, 52, h...|[[pos, 0, 52, ADD...|[[named_entity, 0...|\n",
      "|The Islam Diary :...|[[document, 0, 32...|[[document, 0, 32...|[[token, 0, 2, Th...|[[pos, 0, 2, DT, ...|[[named_entity, 0...|\n",
      "|Speaking at the s...|[[document, 0, 44...|[[document, 0, 44...|[[token, 0, 7, Sp...|[[pos, 0, 7, VBG,...|[[named_entity, 0...|\n",
      "|He also told the ...|[[document, 0, 34...|[[document, 0, 34...|[[token, 0, 1, He...|[[pos, 0, 1, PRP,...|[[named_entity, 0...|\n",
      "|The newspaper quo...|[[document, 0, 33...|[[document, 0, 33...|[[token, 0, 2, Th...|[[pos, 0, 2, DT, ...|[[named_entity, 0...|\n",
      "|The newspaper des...|[[document, 0, 14...|[[document, 0, 14...|[[token, 0, 2, Th...|[[pos, 0, 2, DT, ...|[[named_entity, 0...|\n",
      "|He speaks in the ...|[[document, 0, 93...|[[document, 0, 93...|[[token, 0, 1, He...|[[pos, 0, 1, PRP,...|[[named_entity, 0...|\n",
      "|He was quoted as ...|[[document, 0, 10...|[[document, 0, 10...|[[token, 0, 1, He...|[[pos, 0, 1, PRP,...|[[named_entity, 0...|\n",
      "|It pointed out th...|[[document, 0, 12...|[[document, 0, 12...|[[token, 0, 1, It...|[[pos, 0, 1, PRP,...|[[named_entity, 0...|\n",
      "|This means that t...|[[document, 0, 14...|[[document, 0, 14...|[[token, 0, 3, Th...|[[pos, 0, 3, DT, ...|[[named_entity, 0...|\n",
      "|\" Fadil \" was quo...|[[document, 0, 22...|[[document, 0, 22...|[[token, 0, 0, \",...|[[pos, 0, 0, ``, ...|[[named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = CoNLL().readDataset(spark, '/Users/ramybal/Downloads/bio/test/sample_genre4.train')\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-soccer",
   "metadata": {},
   "source": [
    "### BERT embeddings and Test data transformed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "enhanced-dictionary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base_cased download started this may take some time.\n",
      "Approximate size to download 389.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# use bert embeddings\n",
    "bert = BertEmbeddings.pretrained('bert_base_cased', 'en').setInputCols([\"sentence\",'token']).setOutputCol(\"bert\").setCaseSensitive(True).setMaxSentenceLength(512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-coating",
   "metadata": {},
   "source": [
    "### Creating pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-compound",
   "metadata": {},
   "source": [
    "#### All the news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "whole-collection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onto_bert_base_cased download started this may take some time.\n",
      "Approximate size to download 15.5 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "ner_onto = NerDLModel.pretrained('onto_bert_base_cased', lang='en') \\\n",
    "        .setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
    "        .setOutputCol(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "registered-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pipeline = Pipeline(stages=[bert,ner_onto])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "classified-designer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 109 µs, sys: 21 µs, total: 130 µs\n",
      "Wall time: 124 µs\n"
     ]
    }
   ],
   "source": [
    "%time myNerModel = nlp_pipeline.fit(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "peaceful-general",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.33 s, sys: 85.4 ms, total: 1.41 s\n",
      "Wall time: 2min 40s\n"
     ]
    }
   ],
   "source": [
    "%time results = myNerModel.transform(test_data).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "legal-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "chronic-ceiling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "imperial-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "electoral-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "severe-niagara",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9793398952601335\n",
      "0.9047264607368813\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "print(accuracy_score(labels,ners))\n",
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "pleased-button",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL   0.860182  0.918831  0.888540       616\n",
      "        DATE   0.887460  0.880383  0.883907      1254\n",
      "       EVENT   0.611111  0.594595  0.602740        37\n",
      "         FAC   0.677966  0.606061  0.640000        66\n",
      "         GPE   0.973913  0.942308  0.957850      1664\n",
      "    LANGUAGE   0.833333  0.500000  0.625000        10\n",
      "         LAW   0.769231  0.555556  0.645161        36\n",
      "         LOC   0.818182  0.812500  0.815331       144\n",
      "       MONEY   0.886525  0.896057  0.891266       279\n",
      "        NORP   0.962069  0.963731  0.962899       579\n",
      "     ORDINAL   0.778571  0.923729  0.844961       118\n",
      "         ORG   0.887948  0.909880  0.898780      1498\n",
      "     PERCENT   0.924399  0.908784  0.916525       296\n",
      "      PERSON   0.943447  0.956324  0.949842      1099\n",
      "     PRODUCT   0.716216  0.746479  0.731034        71\n",
      "    QUANTITY   0.813559  0.813559  0.813559        59\n",
      "        TIME   0.647059  0.726415  0.684444       106\n",
      " WORK_OF_ART   0.708738  0.657658  0.682243       111\n",
      "\n",
      "   micro avg   0.902711  0.906751  0.904726      8043\n",
      "   macro avg   0.816662  0.795158  0.801893      8043\n",
      "weighted avg   0.903583  0.906751  0.904719      8043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1,digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-portland",
   "metadata": {},
   "source": [
    "#### News Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "spare-wheat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onto_bert_base_cased download started this may take some time.\n",
      "Approximate size to download 15.5 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "ner_onto = NerDLModel.pretrained('onto_bert_base_cased', lang='en') \\\n",
    "        .setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
    "        .setOutputCol(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "hungry-governor",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pipeline = Pipeline(stages=[bert,ner_onto])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "radical-consideration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 108 µs, sys: 19 µs, total: 127 µs\n",
      "Wall time: 120 µs\n"
     ]
    }
   ],
   "source": [
    "%time myNerModel = nlp_pipeline.fit(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bored-crossing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 518 ms, sys: 32 ms, total: 550 ms\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%time results = myNerModel.transform(test_data).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "careful-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "extreme-intro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "instructional-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "million-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "offensive-repository",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.984917508002955\n",
      "0.8759210138520483\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(labels,ners))\n",
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "sacred-medicaid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL   0.767677  0.835165  0.800000       182\n",
      "        DATE   0.819209  0.725000  0.769231       200\n",
      "       EVENT   0.416667  0.357143  0.384615        14\n",
      "         FAC   0.956522  0.916667  0.936170        48\n",
      "         GPE   0.971264  0.957507  0.964337       353\n",
      "         LAW   1.000000  0.333333  0.500000         3\n",
      "         LOC   0.760000  0.730769  0.745098        26\n",
      "       MONEY   0.750000  1.000000  0.857143         3\n",
      "        NORP   0.936620  0.963768  0.950000       138\n",
      "     ORDINAL   0.903846  0.940000  0.921569        50\n",
      "         ORG   0.817568  0.790850  0.803987       153\n",
      "     PERCENT   1.000000  1.000000  1.000000        14\n",
      "      PERSON   0.940874  0.958115  0.949416       382\n",
      "    QUANTITY   0.925000  0.925000  0.925000        40\n",
      "        TIME   0.737705  0.714286  0.725806        63\n",
      " WORK_OF_ART   0.410256  0.571429  0.477612        28\n",
      "\n",
      "   micro avg   0.876179  0.875663  0.875921      1697\n",
      "   macro avg   0.819575  0.794939  0.794374      1697\n",
      "weighted avg   0.878658  0.875663  0.876008      1697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1,digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-cross",
   "metadata": {},
   "source": [
    "#### Phone conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "outer-orchestra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onto_bert_base_cased download started this may take some time.\n",
      "Approximate size to download 15.5 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "ner_onto = NerDLModel.pretrained('onto_bert_base_cased', lang='en') \\\n",
    "        .setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
    "        .setOutputCol(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "universal-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pipeline = Pipeline(stages=[bert,ner_onto])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cross-junction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 349 µs, sys: 295 µs, total: 644 µs\n",
      "Wall time: 376 µs\n"
     ]
    }
   ],
   "source": [
    "%time myNerModel = nlp_pipeline.fit(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "missing-disaster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 135 ms, sys: 9.96 ms, total: 145 ms\n",
      "Wall time: 32.9 s\n"
     ]
    }
   ],
   "source": [
    "%time results = myNerModel.transform(test_data).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "awful-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "previous-receipt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "imposed-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "valued-hawaiian",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "destroyed-silver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9815962099125365\n",
      "0.7837837837837838\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(labels,ners))\n",
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "received-bulgaria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL   0.696429  0.750000  0.722222        52\n",
      "        DATE   0.836066  0.689189  0.755556        74\n",
      "       EVENT   0.000000  1.000000  0.000000         0\n",
      "         FAC   0.000000  0.000000  0.000000         3\n",
      "         GPE   0.940000  0.940000  0.940000        50\n",
      "    LANGUAGE   1.000000  0.500000  0.666667         8\n",
      "         LOC   0.000000  1.000000  0.000000         0\n",
      "       MONEY   0.600000  0.428571  0.500000         7\n",
      "        NORP   0.772727  1.000000  0.871795        17\n",
      "     ORDINAL   0.888889  0.888889  0.888889         9\n",
      "         ORG   0.764706  0.481481  0.590909        27\n",
      "     PERCENT   0.666667  0.666667  0.666667         6\n",
      "      PERSON   0.898148  0.970000  0.932692       100\n",
      "     PRODUCT   1.000000  0.250000  0.400000         4\n",
      "    QUANTITY   0.000000  1.000000  0.000000         0\n",
      "        TIME   0.666667  0.260870  0.375000        23\n",
      " WORK_OF_ART   0.000000  1.000000  0.000000         0\n",
      "\n",
      "   micro avg   0.805556  0.763158  0.783784       380\n",
      "   macro avg   0.572370  0.695627  0.488847       380\n",
      "weighted avg   0.821618  0.763158  0.777814       380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1,digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-relief",
   "metadata": {},
   "source": [
    "#### Web Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "worth-omega",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onto_bert_base_cased download started this may take some time.\n",
      "Approximate size to download 15.5 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "ner_onto = NerDLModel.pretrained('onto_bert_base_cased', lang='en') \\\n",
    "        .setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
    "        .setOutputCol(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "changed-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pipeline = Pipeline(stages=[bert,ner_onto])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "amateur-english",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 112 µs, sys: 20 µs, total: 132 µs\n",
      "Wall time: 125 µs\n"
     ]
    }
   ],
   "source": [
    "%time myNerModel = nlp_pipeline.fit(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "familiar-hypothesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 252 ms, sys: 10.7 ms, total: 263 ms\n",
      "Wall time: 36 s\n"
     ]
    }
   ],
   "source": [
    "%time results = myNerModel.transform(test_data).select(\"sentence\",\"token\",\"label\",\"ner\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "median-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find exceptions where no. of labels does not match no. of ners detected\n",
    "count = 0\n",
    "indices = []\n",
    "for i,row in enumerate(results):\n",
    "    if len(row['label']) != len(row['ner']):\n",
    "        count += 1\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dynamic-morocco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "attached-teacher",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list = [results[t] for t in indices]\n",
    "results = [results[i] for i in range(len(results)) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "legal-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "ners = []\n",
    "\n",
    "for row in results:\n",
    "    tokens.append([t['result'] for t in row['token']])\n",
    "    labels.append([t['result'] for t in row['label']])\n",
    "    ners.append([t['result'] for t in row['ner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "direct-price",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9716020058062813\n",
      "0.8010657193605683\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(labels,ners))\n",
    "print(f1_score(labels,ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "logical-parking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CARDINAL   0.866667  0.764706  0.812500        85\n",
      "        DATE   0.654762  0.743243  0.696203        74\n",
      "       EVENT   0.600000  0.500000  0.545455        12\n",
      "         FAC   0.705882  0.666667  0.685714        18\n",
      "         GPE   0.930233  0.924855  0.927536       173\n",
      "    LANGUAGE   1.000000  0.250000  0.400000         4\n",
      "         LAW   1.000000  1.000000  1.000000         1\n",
      "         LOC   0.625000  0.555556  0.588235         9\n",
      "       MONEY   0.689655  0.800000  0.740741        25\n",
      "        NORP   0.875000  0.850467  0.862559       107\n",
      "     ORDINAL   0.812500  0.722222  0.764706        18\n",
      "         ORG   0.673913  0.794872  0.729412       117\n",
      "     PERCENT   0.575758  0.575758  0.575758        33\n",
      "      PERSON   0.911602  0.810811  0.858257       407\n",
      "     PRODUCT   0.000000  0.000000  0.000000         1\n",
      "    QUANTITY   0.333333  0.333333  0.333333         6\n",
      "        TIME   0.560000  0.700000  0.622222        20\n",
      " WORK_OF_ART   0.454545  0.555556  0.500000        27\n",
      "\n",
      "   micro avg   0.808969  0.793316  0.801066      1137\n",
      "   macro avg   0.681603  0.641558  0.646813      1137\n",
      "weighted avg   0.820945  0.793316  0.803871      1137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels,ners, zero_division=1,digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-turner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
